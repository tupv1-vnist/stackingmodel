{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3d88d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "init_notebook_mode(connected=True)\n",
    "import cufflinks as cf\n",
    "cf.go_offline()\n",
    "cf.set_config_file(offline=False, world_readable=True)\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc36fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "items = pd.read_csv('E:/DOWLOAD/LightGBM_XGBoost/item1.csv')\n",
    "train = pd.read_csv('E:/DOWLOAD/LightGBM_XGBoost/order1.csv')\n",
    "test  = pd.read_csv('E:/DOWLOAD/LightGBM_XGBoost/data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5d3141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>orders</th>\n",
       "      <th>clicks</th>\n",
       "      <th>product_id</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>price</th>\n",
       "      <th>purchase_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>68202834</td>\n",
       "      <td>1</td>\n",
       "      <td>221204</td>\n",
       "      <td>171204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>176251118</td>\n",
       "      <td>1</td>\n",
       "      <td>293260</td>\n",
       "      <td>243260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>99325284</td>\n",
       "      <td>1</td>\n",
       "      <td>79385</td>\n",
       "      <td>74385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>99183754</td>\n",
       "      <td>1</td>\n",
       "      <td>165042</td>\n",
       "      <td>160042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>99156609</td>\n",
       "      <td>1</td>\n",
       "      <td>104577</td>\n",
       "      <td>99577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223145</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>273633499</td>\n",
       "      <td>48</td>\n",
       "      <td>4053000</td>\n",
       "      <td>4003000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223146</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>34573674</td>\n",
       "      <td>48</td>\n",
       "      <td>800000</td>\n",
       "      <td>750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223147</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>34573146</td>\n",
       "      <td>48</td>\n",
       "      <td>800000</td>\n",
       "      <td>750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223148</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>271392620</td>\n",
       "      <td>48</td>\n",
       "      <td>1199000</td>\n",
       "      <td>1149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223149</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>270126840</td>\n",
       "      <td>48</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1450000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5223150 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               date  orders  clicks  product_id  campaign_id    price   \n",
       "0        2021-01-01       0      11    68202834            1   221204  \\\n",
       "1        2021-01-01       0      20   176251118            1   293260   \n",
       "2        2021-01-01       0      12    99325284            1    79385   \n",
       "3        2021-01-01       0      19    99183754            1   165042   \n",
       "4        2021-01-01       1       4    99156609            1   104577   \n",
       "...             ...     ...     ...         ...          ...      ...   \n",
       "5223145  2023-12-31       0      14   273633499           48  4053000   \n",
       "5223146  2023-12-31       0      17    34573674           48   800000   \n",
       "5223147  2023-12-31       2      11    34573146           48   800000   \n",
       "5223148  2023-12-31       0      15   271392620           48  1199000   \n",
       "5223149  2023-12-31       2      11   270126840           48  1500000   \n",
       "\n",
       "         purchase_price  \n",
       "0                171204  \n",
       "1                243260  \n",
       "2                 74385  \n",
       "3                160042  \n",
       "4                 99577  \n",
       "...                 ...  \n",
       "5223145         4003000  \n",
       "5223146          750000  \n",
       "5223147          750000  \n",
       "5223148         1149000  \n",
       "5223149         1450000  \n",
       "\n",
       "[5223150 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb71e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68202834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>176251118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99325284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99183754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99156609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>273633499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>34573674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1351</th>\n",
       "      <td>34573146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1352</th>\n",
       "      <td>271392620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>270126840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1354 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id\n",
       "0       68202834\n",
       "1      176251118\n",
       "2       99325284\n",
       "3       99183754\n",
       "4       99156609\n",
       "...          ...\n",
       "1349   273633499\n",
       "1350    34573674\n",
       "1351    34573146\n",
       "1352   271392620\n",
       "1353   270126840\n",
       "\n",
       "[1354 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78395ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển đổi cột 'date' sang dạng datetime\n",
    "train['date'] = pd.to_datetime(train['date'])\n",
    "\n",
    "# Tạo cột 'date_block_num' dựa trên 'date'\n",
    "train['date_block_num'] = (train['date'].dt.year - 2021) * 12 + train['date'].dt.month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "852e5dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>orders</th>\n",
       "      <th>clicks</th>\n",
       "      <th>product_id</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>price</th>\n",
       "      <th>purchase_price</th>\n",
       "      <th>date_block_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>68202834</td>\n",
       "      <td>1</td>\n",
       "      <td>221204</td>\n",
       "      <td>171204</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>176251118</td>\n",
       "      <td>1</td>\n",
       "      <td>293260</td>\n",
       "      <td>243260</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>99325284</td>\n",
       "      <td>1</td>\n",
       "      <td>79385</td>\n",
       "      <td>74385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>99183754</td>\n",
       "      <td>1</td>\n",
       "      <td>165042</td>\n",
       "      <td>160042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>99156609</td>\n",
       "      <td>1</td>\n",
       "      <td>104577</td>\n",
       "      <td>99577</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223145</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>273633499</td>\n",
       "      <td>48</td>\n",
       "      <td>4053000</td>\n",
       "      <td>4003000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223146</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>34573674</td>\n",
       "      <td>48</td>\n",
       "      <td>800000</td>\n",
       "      <td>750000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223147</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>34573146</td>\n",
       "      <td>48</td>\n",
       "      <td>800000</td>\n",
       "      <td>750000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223148</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>271392620</td>\n",
       "      <td>48</td>\n",
       "      <td>1199000</td>\n",
       "      <td>1149000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223149</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>270126840</td>\n",
       "      <td>48</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1450000</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5223150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  orders  clicks  product_id  campaign_id    price   \n",
       "0       2021-01-01       0      11    68202834            1   221204  \\\n",
       "1       2021-01-01       0      20   176251118            1   293260   \n",
       "2       2021-01-01       0      12    99325284            1    79385   \n",
       "3       2021-01-01       0      19    99183754            1   165042   \n",
       "4       2021-01-01       1       4    99156609            1   104577   \n",
       "...            ...     ...     ...         ...          ...      ...   \n",
       "5223145 2023-12-31       0      14   273633499           48  4053000   \n",
       "5223146 2023-12-31       0      17    34573674           48   800000   \n",
       "5223147 2023-12-31       2      11    34573146           48   800000   \n",
       "5223148 2023-12-31       0      15   271392620           48  1199000   \n",
       "5223149 2023-12-31       2      11   270126840           48  1500000   \n",
       "\n",
       "         purchase_price  date_block_num  \n",
       "0                171204               1  \n",
       "1                243260               1  \n",
       "2                 74385               1  \n",
       "3                160042               1  \n",
       "4                 99577               1  \n",
       "...                 ...             ...  \n",
       "5223145         4003000              36  \n",
       "5223146          750000              36  \n",
       "5223147          750000              36  \n",
       "5223148         1149000              36  \n",
       "5223149         1450000              36  \n",
       "\n",
       "[5223150 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305de79d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nhóm dữ liệu theo 'date_block_num' và tính tổng doanh số của mỗi tháng\n",
    "sale_by_month = train.groupby('date_block_num')['orders'].sum()\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sale_by_month.index, sale_by_month.values, marker='o', linestyle='-')\n",
    "plt.title('Tổng doanh số theo tháng')\n",
    "plt.xlabel('Tháng')\n",
    "plt.ylabel('Doanh số')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd2b149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "xaxis": "x",
         "y": [
          274058,
          294152,
          166959,
          161975,
          209686,
          162067,
          166912,
          358007,
          161858,
          166697,
          180310,
          349339,
          437341,
          288417,
          167027,
          161229,
          166897,
          202940,
          246566,
          247199,
          161246,
          166539,
          273603,
          168085,
          443759,
          151962,
          200255,
          161483,
          166318,
          161478,
          209543,
          166913,
          238718,
          241139,
          276990,
          250918
         ],
         "yaxis": "y"
        },
        {
         "mode": "lines",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "xaxis": "x2",
         "y": [
          null,
          null,
          null,
          205115.57142857142,
          217108.28571428568,
          198209.14285714284,
          198171.71428571423,
          200790.99999999994,
          220741.42857142852,
          260066.2857142857,
          277424.14285714284,
          250141.2857142857,
          250051.42857142858,
          250080.00000000003,
          253312.85714285716,
          238631.00000000003,
          211467.85714285716,
          193300.57142857142,
          193230.85714285713,
          209284.28571428568,
          209454,
          243856.71428571426,
          230341.85714285713,
          223635.5714285714,
          223669.42857142852,
          223637.85714285713,
          207620,
          213542.57142857145,
          173993.14285714287,
          186386.85714285713,
          192227.42857142858,
          208728.42857142858,
          220814.14285714287,
          null,
          null,
          null
         ],
         "yaxis": "y2"
        },
        {
         "mode": "lines",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "xaxis": "x3",
         "y": [
          1.0355398863332212,
          0.8787011469974226,
          0.8301945759591653,
          1.0703876355291253,
          1.095929739965842,
          1.1956903473277543,
          0.8935566678874688,
          1.0355398863332212,
          0.8787011469974226,
          0.8301945759591653,
          1.0703876355291253,
          1.095929739965842,
          1.1956903473277543,
          0.8935566678874688,
          1.0355398863332212,
          0.8787011469974226,
          0.8301945759591653,
          1.0703876355291253,
          1.095929739965842,
          1.1956903473277543,
          0.8935566678874688,
          1.0355398863332212,
          0.8787011469974226,
          0.8301945759591653,
          1.0703876355291253,
          1.095929739965842,
          1.1956903473277543,
          0.8935566678874688,
          1.0355398863332212,
          0.8787011469974226,
          0.8301945759591653,
          1.0703876355291253,
          1.095929739965842,
          1.1956903473277543,
          0.8935566678874688,
          1.0355398863332212
         ],
         "yaxis": "y3"
        },
        {
         "mode": "lines",
         "type": "scatter",
         "x": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36
         ],
         "xaxis": "x4",
         "y": [
          null,
          null,
          null,
          0.7377483954560732,
          0.8812727156660493,
          0.6838363561392201,
          0.9425920984222377,
          1.7217910430836199,
          0.8344669249134209,
          0.7720827283803721,
          0.6072037943331167,
          1.274321418018763,
          1.4627568155569155,
          1.2906836083092763,
          0.6367407078581051,
          0.768909286017668,
          0.95065792754501,
          0.9808293079221887,
          1.1643243951075386,
          0.9878508232419119,
          0.8615454464930377,
          0.659499397717894,
          1.3517824310931479,
          0.9053326505991877,
          1.8535293282878291,
          0.6200218092226436,
          0.8066691690074325,
          0.8462920107209282,
          0.9230820152858601,
          0.985954521921242,
          1.3130398574209092,
          0.7470806289326413,
          0.9864511169168538,
          null,
          null,
          null
         ],
         "yaxis": "y4"
        }
       ],
       "layout": {
        "annotations": [
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Observed",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 1,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Trend",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.71875,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Seasonal",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.4375,
          "yanchor": "bottom",
          "yref": "paper"
         },
         {
          "font": {
           "size": 16
          },
          "showarrow": false,
          "text": "Residuals",
          "x": 0.5,
          "xanchor": "center",
          "xref": "paper",
          "y": 0.15625,
          "yanchor": "bottom",
          "yref": "paper"
         }
        ],
        "height": 900,
        "margin": {
         "t": 100
        },
        "showlegend": false,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Annual Seasonal Decomposition of Sales",
         "x": 0.5
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "xaxis2": {
         "anchor": "y2",
         "domain": [
          0,
          1
         ]
        },
        "xaxis3": {
         "anchor": "y3",
         "domain": [
          0,
          1
         ]
        },
        "xaxis4": {
         "anchor": "y4",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0.84375,
          1
         ]
        },
        "yaxis2": {
         "anchor": "x2",
         "domain": [
          0.5625,
          0.71875
         ]
        },
        "yaxis3": {
         "anchor": "x3",
         "domain": [
          0.28125,
          0.4375
         ]
        },
        "yaxis4": {
         "anchor": "x4",
         "domain": [
          0,
          0.15625
         ]
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"3b9abd22-e013-4b18-b6d2-977d9f482f57\" class=\"plotly-graph-div\" style=\"height:900px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3b9abd22-e013-4b18-b6d2-977d9f482f57\")) {                    Plotly.newPlot(                        \"3b9abd22-e013-4b18-b6d2-977d9f482f57\",                        [{\"mode\":\"lines\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36],\"y\":[274058.0,294152.0,166959.0,161975.0,209686.0,162067.0,166912.0,358007.0,161858.0,166697.0,180310.0,349339.0,437341.0,288417.0,167027.0,161229.0,166897.0,202940.0,246566.0,247199.0,161246.0,166539.0,273603.0,168085.0,443759.0,151962.0,200255.0,161483.0,166318.0,161478.0,209543.0,166913.0,238718.0,241139.0,276990.0,250918.0],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"lines\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36],\"y\":[null,null,null,205115.57142857142,217108.28571428568,198209.14285714284,198171.71428571423,200790.99999999994,220741.42857142852,260066.2857142857,277424.14285714284,250141.2857142857,250051.42857142858,250080.00000000003,253312.85714285716,238631.00000000003,211467.85714285716,193300.57142857142,193230.85714285713,209284.28571428568,209454.0,243856.71428571426,230341.85714285713,223635.5714285714,223669.42857142852,223637.85714285713,207620.0,213542.57142857145,173993.14285714287,186386.85714285713,192227.42857142858,208728.42857142858,220814.14285714287,null,null,null],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"},{\"mode\":\"lines\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36],\"y\":[1.0355398863332212,0.8787011469974226,0.8301945759591653,1.0703876355291253,1.095929739965842,1.1956903473277543,0.8935566678874688,1.0355398863332212,0.8787011469974226,0.8301945759591653,1.0703876355291253,1.095929739965842,1.1956903473277543,0.8935566678874688,1.0355398863332212,0.8787011469974226,0.8301945759591653,1.0703876355291253,1.095929739965842,1.1956903473277543,0.8935566678874688,1.0355398863332212,0.8787011469974226,0.8301945759591653,1.0703876355291253,1.095929739965842,1.1956903473277543,0.8935566678874688,1.0355398863332212,0.8787011469974226,0.8301945759591653,1.0703876355291253,1.095929739965842,1.1956903473277543,0.8935566678874688,1.0355398863332212],\"type\":\"scatter\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"mode\":\"lines\",\"x\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36],\"y\":[null,null,null,0.7377483954560732,0.8812727156660493,0.6838363561392201,0.9425920984222377,1.7217910430836199,0.8344669249134209,0.7720827283803721,0.6072037943331167,1.274321418018763,1.4627568155569155,1.2906836083092763,0.6367407078581051,0.768909286017668,0.95065792754501,0.9808293079221887,1.1643243951075386,0.9878508232419119,0.8615454464930377,0.659499397717894,1.3517824310931479,0.9053326505991877,1.8535293282878291,0.6200218092226436,0.8066691690074325,0.8462920107209282,0.9230820152858601,0.985954521921242,1.3130398574209092,0.7470806289326413,0.9864511169168538,null,null,null],\"type\":\"scatter\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.84375,1.0]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.0,1.0]},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.5625,0.71875]},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,1.0]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.28125,0.4375]},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.0,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.15625]},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Observed\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Trend\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.71875,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Seasonal\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.4375,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Residuals\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.15625,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"margin\":{\"t\":100},\"title\":{\"text\":\"Annual Seasonal Decomposition of Sales\",\"x\":0.5},\"height\":900,\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('3b9abd22-e013-4b18-b6d2-977d9f482f57');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_seasonal_decompose(result, title=\"Seasonal Decomposition\"):\n",
    "    return (\n",
    "        make_subplots(\n",
    "            rows=4,\n",
    "            cols=1,\n",
    "            subplot_titles=[\"Observed\", \"Trend\", \"Seasonal\", \"Residuals\"],\n",
    "        )\n",
    "        .add_trace(\n",
    "            go.Scatter(x=result.seasonal.index, y=result.observed, mode=\"lines\"),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "        .add_trace(\n",
    "            go.Scatter(x=result.trend.index, y=result.trend, mode=\"lines\"),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "        .add_trace(\n",
    "            go.Scatter(x=result.seasonal.index, y=result.seasonal, mode=\"lines\"),\n",
    "            row=3,\n",
    "            col=1,\n",
    "        )\n",
    "        .add_trace(\n",
    "            go.Scatter(x=result.resid.index, y=result.resid, mode=\"lines\"),\n",
    "            row=4,\n",
    "            col=1,\n",
    "        )\n",
    "        .update_layout(\n",
    "            height=900,\n",
    "            title=title,\n",
    "            margin=dict(t=100),\n",
    "            title_x=0.5,\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "seasonnal = seasonal_decompose(sale_by_month, model='multiplicative', period=7)\n",
    "plot_seasonal_decompose(seasonnal, title=\"Annual Seasonal Decomposition of Sales\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dc24703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAGdCAYAAAA7VYb2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA140lEQVR4nO3df1RVdb7/8ddB5IfmOfhjAM8VlTuZSjo6/ggptfHKEq+ME2l3RC2tGJ1moFT8nUrWOKPhaGqZjHeasDV6M9dNxrAwQpMmCRU1w6toM+aP9IBzEU5QKMr5/tGXfT0D+YM2Ho48H2vttTr78z6f/d5nrxUv99l7H4vL5XIJAAAA34uPpxsAAAC4ExCqAAAATECoAgAAMAGhCgAAwASEKgAAABMQqgAAAExAqAIAADABoQoAAMAEvp5uoDmpqanRuXPn1KZNG1ksFk+3AwAAboLL5dJXX30lu90uH5/vPh9FqLqNzp07p7CwME+3AQAAGuDMmTPq1KnTd44Tqm6jNm3aSPr2oFitVg93AwAAbobT6VRYWJjxd/y7EKpuo9qv/KxWK6EKAAAvc6NLd7hQHQAAwASEKgAAABMQqgAAAExAqAIAADABoQoAAMAEhCoAAAATEKoAAABMQKgCAAAwAaEKAADABIQqAAAAExCqAAAATECoAgAAMAGhCgAAwASEKgAAABP4eroBAADQvHSdt71R5v1iWWyjzHuzOFMFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAk8Gqpyc3M1evRo2e12WSwWZWRkfGftU089JYvFolWrVrmtLy0t1cSJE2W1WhUUFKSEhARVVFS41Rw+fFhDhgxRQECAwsLClJqaWmf+LVu2qEePHgoICFDv3r317rvvuo27XC6lpKSoY8eOCgwMVHR0tE6cONHgfQcAAHcWj4aqyspK9enTR2vXrr1u3datW/XJJ5/IbrfXGZs4caKOHDmi7OxsZWZmKjc3V1OnTjXGnU6nRowYoS5duqigoEDLly/X4sWLtX79eqNmz549Gj9+vBISEnTw4EHFxcUpLi5OhYWFRk1qaqrWrFmjtLQ05efnq3Xr1oqJiVFVVZUJnwQAAPB2FpfL5fJ0E5JksVi0detWxcXFua3/8ssvFRkZqR07dig2NlbTp0/X9OnTJUlHjx5VRESE9u3bpwEDBkiSsrKyNGrUKJ09e1Z2u13r1q3TggUL5HA45OfnJ0maN2+eMjIydOzYMUnSuHHjVFlZqczMTGO7gwYNUt++fZWWliaXyyW73a6ZM2dq1qxZkqTy8nKFhIQoPT1d8fHxN7WPTqdTNptN5eXlslqt3+fjAgDAa3Wdt71R5v1iWWyjzHuzf7+b9DVVNTU1euyxxzR79mzde++9dcbz8vIUFBRkBCpJio6Olo+Pj/Lz842aoUOHGoFKkmJiYlRUVKSLFy8aNdHR0W5zx8TEKC8vT5J08uRJORwOtxqbzabIyEijpj6XLl2S0+l0WwAAwJ2pSYeqF198Ub6+vnrmmWfqHXc4HAoODnZb5+vrq3bt2snhcBg1ISEhbjW1r29Uc+34te+rr6Y+S5culc1mM5awsLDr7i8AAPBeTTZUFRQUaPXq1UpPT5fFYvF0Ow0yf/58lZeXG8uZM2c83RIAAGgkTTZUffTRRyopKVHnzp3l6+srX19fnTp1SjNnzlTXrl0lSaGhoSopKXF735UrV1RaWqrQ0FCjpri42K2m9vWNaq4dv/Z99dXUx9/fX1ar1W0BAAB3piYbqh577DEdPnxYhw4dMha73a7Zs2drx44dkqSoqCiVlZWpoKDAeN/OnTtVU1OjyMhIoyY3N1fV1dVGTXZ2trp37662bdsaNTk5OW7bz87OVlRUlCQpPDxcoaGhbjVOp1P5+flGDQAAaN58PbnxiooKff7558brkydP6tChQ2rXrp06d+6s9u3bu9W3bNlSoaGh6t69uySpZ8+eGjlypKZMmaK0tDRVV1crKSlJ8fHxxuMXJkyYoOeff14JCQmaO3euCgsLtXr1ar300kvGvNOmTdODDz6oFStWKDY2Vm+++ab2799vPHbBYrFo+vTpWrJkibp166bw8HAtWrRIdru9zt2KAACgefJoqNq/f7+GDRtmvE5OTpYkTZ48Wenp6Tc1x8aNG5WUlKThw4fLx8dHY8eO1Zo1a4xxm82m999/X4mJierfv786dOiglJQUt2dZ3X///dq0aZMWLlyoZ599Vt26dVNGRoZ69epl1MyZM0eVlZWaOnWqysrKNHjwYGVlZSkgIOB7fgoAAOBO0GSeU9Uc8JwqAAB4ThUAAACug1AFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAm8Gioys3N1ejRo2W322WxWJSRkWGMVVdXa+7cuerdu7dat24tu92uSZMm6dy5c25zlJaWauLEibJarQoKClJCQoIqKircag4fPqwhQ4YoICBAYWFhSk1NrdPLli1b1KNHDwUEBKh3795699133cZdLpdSUlLUsWNHBQYGKjo6WidOnDDvwwAAAF7No6GqsrJSffr00dq1a+uMff311zpw4IAWLVqkAwcO6O2331ZRUZF+9rOfudVNnDhRR44cUXZ2tjIzM5Wbm6upU6ca406nUyNGjFCXLl1UUFCg5cuXa/HixVq/fr1Rs2fPHo0fP14JCQk6ePCg4uLiFBcXp8LCQqMmNTVVa9asUVpamvLz89W6dWvFxMSoqqqqET4ZAADgbSwul8vl6SYkyWKxaOvWrYqLi/vOmn379um+++7TqVOn1LlzZx09elQRERHat2+fBgwYIEnKysrSqFGjdPbsWdntdq1bt04LFiyQw+GQn5+fJGnevHnKyMjQsWPHJEnjxo1TZWWlMjMzjW0NGjRIffv2VVpamlwul+x2u2bOnKlZs2ZJksrLyxUSEqL09HTFx8ff1D46nU7ZbDaVl5fLarU25GMCAMDrdZ23vVHm/WJZbKPMe7N/v73qmqry8nJZLBYFBQVJkvLy8hQUFGQEKkmKjo6Wj4+P8vPzjZqhQ4cagUqSYmJiVFRUpIsXLxo10dHRbtuKiYlRXl6eJOnkyZNyOBxuNTabTZGRkUZNfS5duiSn0+m2AACAO5PXhKqqqirNnTtX48ePN1Kiw+FQcHCwW52vr6/atWsnh8Nh1ISEhLjV1L6+Uc2149e+r76a+ixdulQ2m81YwsLCbmmfAQCA9/CKUFVdXa2f//zncrlcWrdunafbuWnz589XeXm5sZw5c8bTLQEAgEbi6+kGbqQ2UJ06dUo7d+50+y4zNDRUJSUlbvVXrlxRaWmpQkNDjZri4mK3mtrXN6q5drx2XceOHd1q+vbt+529+/v7y9/f/1Z2FwAAeKkmfaaqNlCdOHFCH3zwgdq3b+82HhUVpbKyMhUUFBjrdu7cqZqaGkVGRho1ubm5qq6uNmqys7PVvXt3tW3b1qjJyclxmzs7O1tRUVGSpPDwcIWGhrrVOJ1O5efnGzUAAKB582ioqqio0KFDh3To0CFJ314QfujQIZ0+fVrV1dV65JFHtH//fm3cuFFXr16Vw+GQw+HQ5cuXJUk9e/bUyJEjNWXKFO3du1cff/yxkpKSFB8fL7vdLkmaMGGC/Pz8lJCQoCNHjmjz5s1avXq1kpOTjT6mTZumrKwsrVixQseOHdPixYu1f/9+JSUlSfr2zsTp06dryZIl2rZtmz777DNNmjRJdrv9uncrAgCA5sOjj1T48MMPNWzYsDrrJ0+erMWLFys8PLze9+3atUs/+clPJH378M+kpCS988478vHx0dixY7VmzRrdddddRv3hw4eVmJioffv2qUOHDnr66ac1d+5ctzm3bNmihQsX6osvvlC3bt2UmpqqUaNGGeMul0vPPfec1q9fr7KyMg0ePFivvvqq7rnnnpveXx6pAADAnftIhSbznKrmgFAFAMCdG6qa9DVVAAAA3oJQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJvBoqMrNzdXo0aNlt9tlsViUkZHhNu5yuZSSkqKOHTsqMDBQ0dHROnHihFtNaWmpJk6cKKvVqqCgICUkJKiiosKt5vDhwxoyZIgCAgIUFham1NTUOr1s2bJFPXr0UEBAgHr37q133333lnsBAADNl0dDVWVlpfr06aO1a9fWO56amqo1a9YoLS1N+fn5at26tWJiYlRVVWXUTJw4UUeOHFF2drYyMzOVm5urqVOnGuNOp1MjRoxQly5dVFBQoOXLl2vx4sVav369UbNnzx6NHz9eCQkJOnjwoOLi4hQXF6fCwsJb6gUAADRfFpfL5fJ0E5JksVi0detWxcXFSfr2zJDdbtfMmTM1a9YsSVJ5eblCQkKUnp6u+Ph4HT16VBEREdq3b58GDBggScrKytKoUaN09uxZ2e12rVu3TgsWLJDD4ZCfn58kad68ecrIyNCxY8ckSePGjVNlZaUyMzONfgYNGqS+ffsqLS3tpnq5GU6nUzabTeXl5bJaraZ8bgAAeJuu87Y3yrxfLIttlHlv9u93k72m6uTJk3I4HIqOjjbW2Ww2RUZGKi8vT5KUl5enoKAgI1BJUnR0tHx8fJSfn2/UDB061AhUkhQTE6OioiJdvHjRqLl2O7U1tdu5mV7qc+nSJTmdTrcFAADcmZpsqHI4HJKkkJAQt/UhISHGmMPhUHBwsNu4r6+v2rVr51ZT3xzXbuO7aq4dv1Ev9Vm6dKlsNpuxhIWF3WCvAQCAt2qyoepOMH/+fJWXlxvLmTNnPN0SAABoJE02VIWGhkqSiouL3dYXFxcbY6GhoSopKXEbv3LlikpLS91q6pvj2m18V8214zfqpT7+/v6yWq1uCwAAuDM12VAVHh6u0NBQ5eTkGOucTqfy8/MVFRUlSYqKilJZWZkKCgqMmp07d6qmpkaRkZFGTW5urqqrq42a7Oxsde/eXW3btjVqrt1ObU3tdm6mFwAA0Lx5NFRVVFTo0KFDOnTokKRvLwg/dOiQTp8+LYvFounTp2vJkiXatm2bPvvsM02aNEl2u924Q7Bnz54aOXKkpkyZor179+rjjz9WUlKS4uPjZbfbJUkTJkyQn5+fEhISdOTIEW3evFmrV69WcnKy0ce0adOUlZWlFStW6NixY1q8eLH279+vpKQkSbqpXgAAQPPm68mN79+/X8OGDTNe1wadyZMnKz09XXPmzFFlZaWmTp2qsrIyDR48WFlZWQoICDDes3HjRiUlJWn48OHy8fHR2LFjtWbNGmPcZrPp/fffV2Jiovr3768OHTooJSXF7VlW999/vzZt2qSFCxfq2WefVbdu3ZSRkaFevXoZNTfTCwAAaL6azHOqmgOeUwUAAM+pAgAAwHUQqgAAAExAqAIAADABoQoAAMAEhCoAAAATEKoAAABMQKgCAAAwAaEKAADABIQqAAAAExCqAAAATECoAgAAMEGDQtXf//53s/sAAADwag0KVXfffbeGDRumP//5z6qqqjK7JwAAAK/ToFB14MAB/ehHP1JycrJCQ0P1y1/+Unv37jW7NwAAAK/RoFDVt29frV69WufOndOf/vQnnT9/XoMHD1avXr20cuVKXbhwwew+AQAAmrTvdaG6r6+vxowZoy1btujFF1/U559/rlmzZiksLEyTJk3S+fPnzeoTAACgSfteoWr//v369a9/rY4dO2rlypWaNWuW/va3vyk7O1vnzp3TQw89ZFafAAAATZpvQ960cuVKvf766yoqKtKoUaP0xhtvaNSoUfLx+TajhYeHKz09XV27djWzVwAAgCarQaFq3bp1evLJJ/X444+rY8eO9dYEBwfrtdde+17NAQAAeIsGhaoTJ07csMbPz0+TJ09uyPQAAABep0HXVL3++uvasmVLnfVbtmzRhg0bvndTAAAA3qZBoWrp0qXq0KFDnfXBwcH63e9+972bAgAA8DYNClWnT59WeHh4nfVdunTR6dOnv3dTAAAA3qZBoSo4OFiHDx+us/7TTz9V+/btv3dTAAAA3qZBoWr8+PF65plntGvXLl29elVXr17Vzp07NW3aNMXHx5vdIwAAQJPXoLv/fvOb3+iLL77Q8OHD5ev77RQ1NTWaNGkS11QBAIBmqUGhys/PT5s3b9ZvfvMbffrppwoMDFTv3r3VpUsXs/sDAADwCg0KVbXuuece3XPPPWb1AgAA4LUaFKquXr2q9PR05eTkqKSkRDU1NW7jO3fuNKU5AAAAb9GgUDVt2jSlp6crNjZWvXr1ksViMbsvAAAAr9KgUPXmm2/qrbfe0qhRo8zuBwAAwCs16JEKfn5+uvvuu83uBQAAwGs1KFTNnDlTq1evlsvlMrsfAAAAr9Sgr//++te/ateuXXrvvfd07733qmXLlm7jb7/9tinNAQAAeIsGhaqgoCA9/PDDZvcCAADgtRoUql5//XWz+wAAAPBqDbqmSpKuXLmiDz74QH/4wx/01VdfSZLOnTuniooK05q7evWqFi1apPDwcAUGBuqHP/yhfvOb37hdy+VyuZSSkqKOHTsqMDBQ0dHROnHihNs8paWlmjhxoqxWq4KCgpSQkFCnz8OHD2vIkCEKCAhQWFiYUlNT6/SzZcsW9ejRQwEBAerdu7feffdd0/YVAAB4twaFqlOnTql379566KGHlJiYqAsXLkiSXnzxRc2aNcu05l588UWtW7dOr7zyio4ePaoXX3xRqampevnll42a1NRUrVmzRmlpacrPz1fr1q0VExOjqqoqo2bixIk6cuSIsrOzlZmZqdzcXE2dOtUYdzqdGjFihLp06aKCggItX75cixcv1vr1642aPXv2aPz48UpISNDBgwcVFxenuLg4FRYWmra/AADAe1lcDbiFLy4uTm3atNFrr72m9u3b69NPP9W//uu/6sMPP9SUKVPqnClqqJ/+9KcKCQnRa6+9ZqwbO3asAgMD9ec//1kul0t2u10zZ840wlx5eblCQkKUnp6u+Ph4HT16VBEREdq3b58GDBggScrKytKoUaN09uxZ2e12rVu3TgsWLJDD4ZCfn58kad68ecrIyNCxY8ckSePGjVNlZaUyMzONXgYNGqS+ffsqLS3tpvbH6XTKZrOpvLxcVqvVlM8IAABv03Xe9kaZ94tlsY0y783+/W7QmaqPPvpICxcuNAJIra5du+rLL79syJT1uv/++5WTk6Pjx49Lkj799FP99a9/1b//+79Lkk6ePCmHw6Ho6GjjPTabTZGRkcrLy5Mk5eXlKSgoyAhUkhQdHS0fHx/l5+cbNUOHDnXbn5iYGBUVFenixYtGzbXbqa2p3U59Ll26JKfT6bYAAIA7U4MuVK+pqdHVq1frrD979qzatGnzvZuqNW/ePDmdTvXo0UMtWrTQ1atX9dvf/lYTJ06UJDkcDklSSEiI2/tCQkKMMYfDoeDgYLdxX19ftWvXzq0mPDy8zhy1Y23btpXD4bjuduqzdOlSPf/887e62wAAwAs16EzViBEjtGrVKuO1xWJRRUWFnnvuOVN/uuatt97Sxo0btWnTJh04cEAbNmzQ73//e23YsMG0bTSm+fPnq7y83FjOnDnj6ZYAAEAjadCZqhUrVigmJkYRERGqqqrShAkTdOLECXXo0EH/9V//ZVpzs2fP1rx58xQfHy9J6t27t06dOqWlS5dq8uTJCg0NlSQVFxerY8eOxvuKi4vVt29fSVJoaKhKSkrc5r1y5YpKS0uN94eGhqq4uNitpvb1jWpqx+vj7+8vf3//W91tAADghRp0pqpTp0769NNP9eyzz2rGjBn68Y9/rGXLlungwYN1vmr7Pr7++mv5+Li32KJFC9XU1EiSwsPDFRoaqpycHGPc6XQqPz9fUVFRkqSoqCiVlZWpoKDAqNm5c6dqamoUGRlp1OTm5qq6utqoyc7OVvfu3dW2bVuj5trt1NbUbgcAADRvDTpTJX17XdKjjz5qZi91jB49Wr/97W/VuXNn3XvvvTp48KBWrlypJ598UtK3XztOnz5dS5YsUbdu3RQeHq5FixbJbrcrLi5OktSzZ0+NHDlSU6ZMUVpamqqrq5WUlKT4+HjZ7XZJ0oQJE/T8888rISFBc+fOVWFhoVavXq2XXnrJ6GXatGl68MEHtWLFCsXGxurNN9/U/v373R67AAAAmq8Ghao33njjuuOTJk1qUDP/7OWXX9aiRYv061//WiUlJbLb7frlL3+plJQUo2bOnDmqrKzU1KlTVVZWpsGDBysrK0sBAQFGzcaNG5WUlKThw4fLx8dHY8eO1Zo1a4xxm82m999/X4mJierfv786dOiglJQUt2dZ3X///dq0aZMWLlyoZ599Vt26dVNGRoZ69eplyr4CAADv1qDnVNV+JVarurpaX3/9tfz8/NSqVSuVlpaa1uCdhOdUAQDAc6rcXLx40W2pqKhQUVGRBg8ebOqF6gAAAN6iwb/998+6deumZcuWadq0aWZNCQAA4DVMC1XStxevnzt3zswpAQAAvEKDLlTftm2b22uXy6Xz58/rlVde0QMPPGBKYwAAAN6kQaGq9nEFtSwWi37wgx/o3/7t37RixQoz+gIAAPAqDf7tPwAAAPwfU6+pAgAAaK4adKYqOTn5pmtXrlzZkE0AAAB4lQaFqoMHD+rgwYOqrq5W9+7dJUnHjx9XixYt1K9fP6POYrGY0yUAAEAT16BQNXr0aLVp00YbNmwwnq5+8eJFPfHEExoyZIhmzpxpapMAAABNXYOuqVqxYoWWLl3q9nM1bdu21ZIlS7j7DwAANEsNClVOp1MXLlyos/7ChQv66quvvndTAAAA3qZBoerhhx/WE088obfffltnz57V2bNn9d///d9KSEjQmDFjzO4RAACgyWvQNVVpaWmaNWuWJkyYoOrq6m8n8vVVQkKCli9fbmqDAAAA3qBBoapVq1Z69dVXtXz5cv3tb3+TJP3whz9U69atTW0OAADAW3yvh3+eP39e58+fV7du3dS6dWu5XC6z+gIAAPAqDQpV//u//6vhw4frnnvu0ahRo3T+/HlJUkJCAo9TAAAAzVKDQtWMGTPUsmVLnT59Wq1atTLWjxs3TllZWaY1BwAA4C0adE3V+++/rx07dqhTp05u67t166ZTp06Z0hgAAIA3adCZqsrKSrczVLVKS0vl7+//vZsCAADwNg0KVUOGDNEbb7xhvLZYLKqpqVFqaqqGDRtmWnMAAADeokFf/6Wmpmr48OHav3+/Ll++rDlz5ujIkSMqLS3Vxx9/bHaPAAAATV6DzlT16tVLx48f1+DBg/XQQw+psrJSY8aM0cGDB/XDH/7Q7B4BAACavFs+U1VdXa2RI0cqLS1NCxYsaIyeAAAAvM4tn6lq2bKlDh8+3Bi9AAAAeK0Gff336KOP6rXXXjO7FwAAAK/VoAvVr1y5oj/96U/64IMP1L9//zq/+bdy5UpTmgMAAPAWtxSq/v73v6tr164qLCxUv379JEnHjx93q7FYLOZ1BwAA4CVuKVR169ZN58+f165duyR9+7M0a9asUUhISKM0BwAA4C1u6Zoql8vl9vq9995TZWWlqQ0BAAB4owZdqF7rn0MWAABAc3VLocpisdS5ZoprqAAAAG7xmiqXy6XHH3/c+NHkqqoqPfXUU3Xu/nv77bfN6xAAAMAL3FKomjx5stvrRx991NRmAAAAvNUtharXX3+9sfoAAADwat/rQnUAAAB8q8mHqi+//FKPPvqo2rdvr8DAQPXu3Vv79+83xl0ul1JSUtSxY0cFBgYqOjpaJ06ccJujtLRUEydOlNVqVVBQkBISElRRUeFWc/jwYQ0ZMkQBAQEKCwtTampqnV62bNmiHj16KCAgQL1799a7777bODsNAAC8TpMOVRcvXtQDDzygli1b6r333tP//M//aMWKFWrbtq1Rk5qaqjVr1igtLU35+flq3bq1YmJiVFVVZdRMnDhRR44cUXZ2tjIzM5Wbm6upU6ca406nUyNGjFCXLl1UUFCg5cuXa/HixVq/fr1Rs2fPHo0fP14JCQk6ePCg4uLiFBcXp8LCwtvzYQAAgCbN4mrCD5uaN2+ePv74Y3300Uf1jrtcLtntds2cOVOzZs2SJJWXlyskJETp6emKj4/X0aNHFRERoX379mnAgAGSpKysLI0aNUpnz56V3W7XunXrtGDBAjkcDvn5+RnbzsjI0LFjxyR9+/T4yspKZWZmGtsfNGiQ+vbtq7S0tJvaH6fTKZvNpvLyclmt1gZ/LgAAeLOu87Y3yrxfLIttlHlv9u93kz5TtW3bNg0YMED/8R//oeDgYP34xz/Wf/7nfxrjJ0+elMPhUHR0tLHOZrMpMjJSeXl5kqS8vDwFBQUZgUqSoqOj5ePjo/z8fKNm6NChRqCSpJiYGBUVFenixYtGzbXbqa2p3U59Ll26JKfT6bYAAIA7U5MOVX//+9+1bt06devWTTt27NCvfvUrPfPMM9qwYYMkyeFwSFKd3x4MCQkxxhwOh4KDg93GfX191a5dO7ea+ua4dhvfVVM7Xp+lS5fKZrMZS1hY2C3tPwAA8B5NOlTV1NSoX79++t3vfqcf//jHmjp1qqZMmXLTX7d52vz581VeXm4sZ86c8XRLAACgkTTpUNWxY0dFRES4revZs6dOnz4tSQoNDZUkFRcXu9UUFxcbY6GhoSopKXEbv3LlikpLS91q6pvj2m18V03teH38/f1ltVrdFgAAcGdq0qHqgQceUFFRkdu648ePq0uXLpKk8PBwhYaGKicnxxh3Op3Kz89XVFSUJCkqKkplZWUqKCgwanbu3KmamhpFRkYaNbm5uaqurjZqsrOz1b17d+NOw6ioKLft1NbUbgcAADRvTTpUzZgxQ5988ol+97vf6fPPP9emTZu0fv16JSYmSvr2x5ynT5+uJUuWaNu2bfrss880adIk2e12xcXFSfr2zNbIkSM1ZcoU7d27Vx9//LGSkpIUHx8vu90uSZowYYL8/PyUkJCgI0eOaPPmzVq9erWSk5ONXqZNm6asrCytWLFCx44d0+LFi7V//34lJSXd9s8FAAA0Pbf0MzW328CBA7V161bNnz9fL7zwgsLDw7Vq1SpNnDjRqJkzZ44qKys1depUlZWVafDgwcrKylJAQIBRs3HjRiUlJWn48OHy8fHR2LFjtWbNGmPcZrPp/fffV2Jiovr3768OHTooJSXF7VlW999/vzZt2qSFCxfq2WefVbdu3ZSRkaFevXrdng8DAAA0aU36OVV3Gp5TBQAAz6kCAADAdRCqAAAATECoAgAAMAGhCgAAwASEKgAAABMQqgAAAExAqAIAADABoQoAAMAEhCoAAAATEKoAAABMQKgCAAAwAaEKAADABIQqAAAAExCqAAAATECoAgAAMAGhCgAAwASEKgAAABMQqgAAAExAqAIAADABoQoAAMAEhCoAAAATEKoAAABMQKgCAAAwAaEKAADABIQqAAAAExCqAAAATECoAgAAMAGhCgAAwASEKgAAABMQqgAAAExAqAIAADABoQoAAMAEhCoAAAATEKoAAABMQKgCAAAwAaEKAADABF4VqpYtWyaLxaLp06cb66qqqpSYmKj27dvrrrvu0tixY1VcXOz2vtOnTys2NlatWrVScHCwZs+erStXrrjVfPjhh+rXr5/8/f119913Kz09vc72165dq65duyogIECRkZHau3dvY+wmAADwQl4Tqvbt26c//OEP+tGPfuS2fsaMGXrnnXe0ZcsW7d69W+fOndOYMWOM8atXryo2NlaXL1/Wnj17tGHDBqWnpyslJcWoOXnypGJjYzVs2DAdOnRI06dP1y9+8Qvt2LHDqNm8ebOSk5P13HPP6cCBA+rTp49iYmJUUlLS+DsPAACaPIvL5XJ5uokbqaioUL9+/fTqq69qyZIl6tu3r1atWqXy8nL94Ac/0KZNm/TII49Iko4dO6aePXsqLy9PgwYN0nvvvaef/vSnOnfunEJCQiRJaWlpmjt3ri5cuCA/Pz/NnTtX27dvV2FhobHN+Ph4lZWVKSsrS5IUGRmpgQMH6pVXXpEk1dTUKCwsTE8//bTmzZt3U/vhdDpls9lUXl4uq9Vq5kcEAIDX6Dpve6PM+8Wy2EaZ92b/fnvFmarExETFxsYqOjrabX1BQYGqq6vd1vfo0UOdO3dWXl6eJCkvL0+9e/c2ApUkxcTEyOl06siRI0bNP88dExNjzHH58mUVFBS41fj4+Cg6Otqoqc+lS5fkdDrdFgAAcGfy9XQDN/Lmm2/qwIED2rdvX50xh8MhPz8/BQUFua0PCQmRw+Ewaq4NVLXjtWPXq3E6nfrmm2908eJFXb16td6aY8eOfWfvS5cu1fPPP39zOwoAALxakz5TdebMGU2bNk0bN25UQECAp9u5ZfPnz1d5ebmxnDlzxtMtAQCARtKkQ1VBQYFKSkrUr18/+fr6ytfXV7t379aaNWvk6+urkJAQXb58WWVlZW7vKy4uVmhoqCQpNDS0zt2Ata9vVGO1WhUYGKgOHTqoRYsW9dbUzlEff39/Wa1WtwUAANyZmnSoGj58uD777DMdOnTIWAYMGKCJEyca/92yZUvl5OQY7ykqKtLp06cVFRUlSYqKitJnn33mdpdedna2rFarIiIijJpr56itqZ3Dz89P/fv3d6upqalRTk6OUQMAAJq3Jn1NVZs2bdSrVy+3da1bt1b79u2N9QkJCUpOTla7du1ktVr19NNPKyoqSoMGDZIkjRgxQhEREXrssceUmpoqh8OhhQsXKjExUf7+/pKkp556Sq+88ormzJmjJ598Ujt37tRbb72l7dv/7+6E5ORkTZ48WQMGDNB9992nVatWqbKyUk888cRt+jQAAEBT1qRD1c146aWX5OPjo7Fjx+rSpUuKiYnRq6++aoy3aNFCmZmZ+tWvfqWoqCi1bt1akydP1gsvvGDUhIeHa/v27ZoxY4ZWr16tTp066Y9//KNiYmKMmnHjxunChQtKSUmRw+FQ3759lZWVVefidQAA0Dx5xXOq7hQ8pwoAAJ5TBQAAgOsgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAmadKhaunSpBg4cqDZt2ig4OFhxcXEqKipyq6mqqlJiYqLat2+vu+66S2PHjlVxcbFbzenTpxUbG6tWrVopODhYs2fP1pUrV9xqPvzwQ/Xr10/+/v66++67lZ6eXqeftWvXqmvXrgoICFBkZKT27t1r+j4DAADv1KRD1e7du5WYmKhPPvlE2dnZqq6u1ogRI1RZWWnUzJgxQ++88462bNmi3bt369y5cxozZowxfvXqVcXGxury5cvas2ePNmzYoPT0dKWkpBg1J0+eVGxsrIYNG6ZDhw5p+vTp+sUvfqEdO3YYNZs3b1ZycrKee+45HThwQH369FFMTIxKSkpuz4cBAACaNIvL5XJ5uombdeHCBQUHB2v37t0aOnSoysvL9YMf/ECbNm3SI488Ikk6duyYevbsqby8PA0aNEjvvfeefvrTn+rcuXMKCQmRJKWlpWnu3Lm6cOGC/Pz8NHfuXG3fvl2FhYXGtuLj41VWVqasrCxJUmRkpAYOHKhXXnlFklRTU6OwsDA9/fTTmjdv3k3173Q6ZbPZVF5eLqvVauZHAwCA1+g6b3ujzPvFsthGmfdm/3436TNV/6y8vFyS1K5dO0lSQUGBqqurFR0dbdT06NFDnTt3Vl5eniQpLy9PvXv3NgKVJMXExMjpdOrIkSNGzbVz1NbUznH58mUVFBS41fj4+Cg6Otqoqc+lS5fkdDrdFgAAcGfymlBVU1Oj6dOn64EHHlCvXr0kSQ6HQ35+fgoKCnKrDQkJkcPhMGquDVS147Vj16txOp365ptv9I9//ENXr16tt6Z2jvosXbpUNpvNWMLCwm59xwEAgFfwmlCVmJiowsJCvfnmm55u5abNnz9f5eXlxnLmzBlPtwQAABqJr6cbuBlJSUnKzMxUbm6uOnXqZKwPDQ3V5cuXVVZW5na2qri4WKGhoUbNP9+lV3t34LU1/3zHYHFxsaxWqwIDA9WiRQu1aNGi3praOerj7+8vf3//W99hAADgdZr0mSqXy6WkpCRt3bpVO3fuVHh4uNt4//791bJlS+Xk5BjrioqKdPr0aUVFRUmSoqKi9Nlnn7ndpZednS2r1aqIiAij5to5amtq5/Dz81P//v3dampqapSTk2PUAACA5q1Jn6lKTEzUpk2b9Je//EVt2rQxrl+y2WwKDAyUzWZTQkKCkpOT1a5dO1mtVj399NOKiorSoEGDJEkjRoxQRESEHnvsMaWmpsrhcGjhwoVKTEw0ziI99dRTeuWVVzRnzhw9+eST2rlzp9566y1t3/5/dyckJydr8uTJGjBggO677z6tWrVKlZWVeuKJJ27/BwMAAJqcJh2q1q1bJ0n6yU9+4rb+9ddf1+OPPy5Jeumll+Tj46OxY8fq0qVLiomJ0auvvmrUtmjRQpmZmfrVr36lqKgotW7dWpMnT9YLL7xg1ISHh2v79u2aMWOGVq9erU6dOumPf/yjYmJijJpx48bpwoULSklJkcPhUN++fZWVlVXn4nUAANA8edVzqrwdz6kCAIDnVAEAAOA6CFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQhUAAIAJCFUAAAAmIFQBAACYgFAFAABgAkIVAACACQhVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVQAAACYgVAEAAJiAUAUAAGACQtUtWrt2rbp27aqAgABFRkZq7969nm4JAAA0AYSqW7B582YlJyfrueee04EDB9SnTx/FxMSopKTE060BAAAPI1TdgpUrV2rKlCl64oknFBERobS0NLVq1Up/+tOfPN0aAADwMF9PN+AtLl++rIKCAs2fP99Y5+Pjo+joaOXl5dX7nkuXLunSpUvG6/LyckmS0+ls3GYBAM1Gr+d2NMq8hc/HNMq8klRz6etGmbex/r7Wzutyua5bR6i6Sf/4xz909epVhYSEuK0PCQnRsWPH6n3P0qVL9fzzz9dZHxYW1ig9AgBgFtsqT3dw6xq756+++ko2m+07xwlVjWj+/PlKTk42XtfU1Ki0tFTt27eXxWLxYGdNl9PpVFhYmM6cOSOr1erpdpo9jkfTwvFoWjgeTUtjHg+Xy6WvvvpKdrv9unWEqpvUoUMHtWjRQsXFxW7ri4uLFRoaWu97/P395e/v77YuKCiosVq8o1itVv4n1YRwPJoWjkfTwvFoWhrreFzvDFUtLlS/SX5+furfv79ycnKMdTU1NcrJyVFUVJQHOwMAAE0BZ6puQXJysiZPnqwBAwbovvvu06pVq1RZWaknnnjC060BAAAPI1TdgnHjxunChQtKSUmRw+FQ3759lZWVVefidTScv7+/nnvuuTpfm8IzOB5NC8ejaeF4NC1N4XhYXDe6PxAAAAA3xDVVAAAAJiBUAQAAmIBQBQAAYAJCFQAAgAkIVbjtcnNzNXr0aNntdlksFmVkZBhj1dXVmjt3rnr37q3WrVvLbrdr0qRJOnfunOcabgaud0z+2VNPPSWLxaJVq1bdtv6am5s5HkePHtXPfvYz2Ww2tW7dWgMHDtTp06dvf7PNwI2OR0VFhZKSktSpUycFBgYqIiJCaWlpnmm2GVi6dKkGDhyoNm3aKDg4WHFxcSoqKnKrqaqqUmJiotq3b6+77rpLY8eOrfPw7sZAqMJtV1lZqT59+mjt2rV1xr7++msdOHBAixYt0oEDB/T222+rqKhIP/vZzzzQafNxvWNyra1bt+qTTz654U814Pu50fH429/+psGDB6tHjx768MMPdfjwYS1atEgBAQG3udPm4UbHIzk5WVlZWfrzn/+so0ePavr06UpKStK2bdtuc6fNw+7du5WYmKhPPvlE2dnZqq6u1ogRI1RZWWnUzJgxQ++88462bNmi3bt369y5cxozZkzjN+cCPEiSa+vWrdet2bt3r0uS69SpU7enqWbuu47J2bNnXf/yL//iKiwsdHXp0sX10ksv3fbemqP6jse4ceNcjz76qGcaaubqOx733nuv64UXXnBb169fP9eCBQtuY2fNV0lJiUuSa/fu3S6Xy+UqKytztWzZ0rVlyxaj5ujRoy5Jrry8vEbthTNVaPLKy8tlsVj43UQPqqmp0WOPPabZs2fr3nvv9XQ7zVpNTY22b9+ue+65RzExMQoODlZkZOR1v7JF47r//vu1bds2ffnll3K5XNq1a5eOHz+uESNGeLq1ZqG8vFyS1K5dO0lSQUGBqqurFR0dbdT06NFDnTt3Vl5eXqP2QqhCk1ZVVaW5c+dq/Pjx/GCpB7344ovy9fXVM8884+lWmr2SkhJVVFRo2bJlGjlypN5//309/PDDGjNmjHbv3u3p9pqll19+WREREerUqZP8/Pw0cuRIrV27VkOHDvV0a3e8mpoaTZ8+XQ888IB69eolSXI4HPLz86vzD/GQkBA5HI5G7YefqUGTVV1drZ///OdyuVxat26dp9tptgoKCrR69WodOHBAFovF0+00ezU1NZKkhx56SDNmzJAk9e3bV3v27FFaWpoefPBBT7bXLL388sv65JNPtG3bNnXp0kW5ublKTEyU3W53O1sC8yUmJqqwsFB//etfPd2KJM5UoYmqDVSnTp1SdnY2Z6k86KOPPlJJSYk6d+4sX19f+fr66tSpU5o5c6a6du3q6faanQ4dOsjX11cRERFu63v27Mndfx7wzTff6Nlnn9XKlSs1evRo/ehHP1JSUpLGjRun3//+955u746WlJSkzMxM7dq1S506dTLWh4aG6vLlyyorK3OrLy4uVmhoaKP2RKhCk1MbqE6cOKEPPvhA7du393RLzdpjjz2mw4cP69ChQ8Zit9s1e/Zs7dixw9PtNTt+fn4aOHBgnVvIjx8/ri5dunioq+arurpa1dXV8vFx/3PaokUL46wizOVyuZSUlKStW7dq586dCg8Pdxvv37+/WrZsqZycHGNdUVGRTp8+raioqEbtja//cNtVVFTo888/N16fPHlShw4dUrt27dSxY0c98sgjOnDggDIzM3X16lXjO/B27drJz8/PU23f0a53TDp37lwn2LZs2VKhoaHq3r377W61WbjR8Zg9e7bGjRunoUOHatiwYcrKytI777yjDz/80HNN38FudDwefPBBzZ49W4GBgerSpYt2796tN954QytXrvRg13euxMREbdq0SX/5y1/Upk0b42+EzWZTYGCgbDabEhISlJycrHbt2slqterpp59WVFSUBg0a1LjNNeq9hUA9du3a5ZJUZ5k8ebLr5MmT9Y5Jcu3atcvTrd+xrndM6sMjFRrXzRyP1157zXX33Xe7AgICXH369HFlZGR4ruE73I2Ox/nz512PP/64y263uwICAlzdu3d3rVixwlVTU+PZxu9Q3/U34vXXXzdqvvnmG9evf/1rV9u2bV2tWrVyPfzww67z5883em+W/98gAAAAvgeuqQIAADABoQoAAMAEhCoAAAATEKoAAABMQKgCAAAwAaEKAADABIQqAAAAExCqAAAATECoAgAAMAGhCgAAwASEKgAAABMQqgAAAEzw/wA1OYVmNCaJ2AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "block_item_sale = train.groupby(['date_block_num','product_id','campaign_id'])['orders'].sum()\n",
    "block_item_sale.clip(0,20).plot.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86974466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a3c568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>orders</th>\n",
       "      <th>clicks</th>\n",
       "      <th>product_id</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>price</th>\n",
       "      <th>purchase_price</th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>68202834</td>\n",
       "      <td>1</td>\n",
       "      <td>221204</td>\n",
       "      <td>171204</td>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>176251118</td>\n",
       "      <td>1</td>\n",
       "      <td>293260</td>\n",
       "      <td>243260</td>\n",
       "      <td>1</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>99325284</td>\n",
       "      <td>1</td>\n",
       "      <td>79385</td>\n",
       "      <td>74385</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>99183754</td>\n",
       "      <td>1</td>\n",
       "      <td>165042</td>\n",
       "      <td>160042</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>99156609</td>\n",
       "      <td>1</td>\n",
       "      <td>104577</td>\n",
       "      <td>99577</td>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223145</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>273633499</td>\n",
       "      <td>48</td>\n",
       "      <td>4053000</td>\n",
       "      <td>4003000</td>\n",
       "      <td>36</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223146</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>34573674</td>\n",
       "      <td>48</td>\n",
       "      <td>800000</td>\n",
       "      <td>750000</td>\n",
       "      <td>36</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223147</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>34573146</td>\n",
       "      <td>48</td>\n",
       "      <td>800000</td>\n",
       "      <td>750000</td>\n",
       "      <td>36</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223148</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>271392620</td>\n",
       "      <td>48</td>\n",
       "      <td>1199000</td>\n",
       "      <td>1149000</td>\n",
       "      <td>36</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223149</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>270126840</td>\n",
       "      <td>48</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1450000</td>\n",
       "      <td>36</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5223150 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  orders  clicks  product_id  campaign_id    price   \n",
       "0       2021-01-01       0      11    68202834            1   221204  \\\n",
       "1       2021-01-01       0      20   176251118            1   293260   \n",
       "2       2021-01-01       0      12    99325284            1    79385   \n",
       "3       2021-01-01       0      19    99183754            1   165042   \n",
       "4       2021-01-01       1       4    99156609            1   104577   \n",
       "...            ...     ...     ...         ...          ...      ...   \n",
       "5223145 2023-12-31       0      14   273633499           48  4053000   \n",
       "5223146 2023-12-31       0      17    34573674           48   800000   \n",
       "5223147 2023-12-31       2      11    34573146           48   800000   \n",
       "5223148 2023-12-31       0      15   271392620           48  1199000   \n",
       "5223149 2023-12-31       2      11   270126840           48  1500000   \n",
       "\n",
       "         purchase_price  date_block_num  revenue  \n",
       "0                171204               1    50000  \n",
       "1                243260               1    50000  \n",
       "2                 74385               1     5000  \n",
       "3                160042               1     5000  \n",
       "4                 99577               1     5000  \n",
       "...                 ...             ...      ...  \n",
       "5223145         4003000              36    50000  \n",
       "5223146          750000              36    50000  \n",
       "5223147          750000              36    50000  \n",
       "5223148         1149000              36    50000  \n",
       "5223149         1450000              36    50000  \n",
       "\n",
       "[5223150 rows x 9 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['revenue'] = train['price'] -train['purchase_price']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a8ee722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10415172576904297"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['profit'] = train['revenue']*train['orders']\n",
    "\n",
    "\n",
    "group = train.groupby(['date_block_num','campaign_id','product_id']).agg({'orders': ['sum'], 'profit': ['sum']})\n",
    "\n",
    "# Đặt lại tên cột kết quả\n",
    "group.columns = ['item_cnt_month', 'profit_month']\n",
    "\n",
    "group\n",
    "\n",
    "# # Kết hợp DataFrame 'matrix' với DataFrame 'group' dựa trên các cột trong 'cols' và dùng phương pháp 'left join'\n",
    "# matrix = matrix.merge(group, on=['date_block_num', 'campaign_id', 'product_id'], how='left')\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "# Đặt lại index của group\n",
    "group = group.reset_index()\n",
    "matrix=[]\n",
    "# Tạo matrix từ group\n",
    "matrix = group.copy()\n",
    "\n",
    "# Chuyển đổi các cột về dạng số nguyên\n",
    "matrix['date_block_num'] = matrix['date_block_num'].astype(int)\n",
    "matrix['campaign_id'] = matrix['campaign_id'].astype(int)\n",
    "matrix['product_id'] = matrix['product_id'].astype(int)\n",
    "\n",
    "# Sắp xếp matrix theo các cột 'date_block_num', 'campaign_id', và 'product_id'\n",
    "matrix.sort_values(['date_block_num', 'campaign_id', 'product_id'], inplace=True)\n",
    "\n",
    "time.time() - ts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec953d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>profit_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>85</td>\n",
       "      <td>425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>27</td>\n",
       "      <td>135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>97</td>\n",
       "      <td>4850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>28</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>30</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150439</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>33</td>\n",
       "      <td>1650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150440</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>29</td>\n",
       "      <td>1450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150441</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>38</td>\n",
       "      <td>1900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150442</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>28</td>\n",
       "      <td>1400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150443</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>27</td>\n",
       "      <td>1350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150444 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  item_cnt_month  profit_month\n",
       "0                    1            1    42899542              85        425000\n",
       "1                    1            1    61393910              27        135000\n",
       "2                    1            1    68202834              97       4850000\n",
       "3                    1            1    73552136              28        140000\n",
       "4                    1            1    74421745              30        150000\n",
       "...                ...          ...         ...             ...           ...\n",
       "150439              36           48   273386993              33       1650000\n",
       "150440              36           48   273633499              29       1450000\n",
       "150441              36           48   273847739              38       1900000\n",
       "150442              36           48   274069973              28       1400000\n",
       "150443              36           48   274070205              27       1350000\n",
       "\n",
       "[150444 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf1e43a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f7e069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>profit_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>85</td>\n",
       "      <td>425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>27</td>\n",
       "      <td>135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>97</td>\n",
       "      <td>4850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>28</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>30</td>\n",
       "      <td>150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150439</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>33</td>\n",
       "      <td>1650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150440</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>29</td>\n",
       "      <td>1450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150441</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>38</td>\n",
       "      <td>1900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150442</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>28</td>\n",
       "      <td>1400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150443</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>27</td>\n",
       "      <td>1350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150444 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  item_cnt_month  profit_month\n",
       "0                    1            1    42899542              85        425000\n",
       "1                    1            1    61393910              27        135000\n",
       "2                    1            1    68202834              97       4850000\n",
       "3                    1            1    73552136              28        140000\n",
       "4                    1            1    74421745              30        150000\n",
       "...                ...          ...         ...             ...           ...\n",
       "150439              36           48   273386993              33       1650000\n",
       "150440              36           48   273633499              29       1450000\n",
       "150441              36           48   273847739              38       1900000\n",
       "150442              36           48   274069973              28       1400000\n",
       "150443              36           48   274070205              27       1350000\n",
       "\n",
       "[150444 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f678c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giá trị của cột 'date_block_num':\n",
      "date_block_num\n",
      "1     4179\n",
      "2     4179\n",
      "21    4179\n",
      "22    4179\n",
      "23    4179\n",
      "24    4179\n",
      "25    4179\n",
      "26    4179\n",
      "27    4179\n",
      "28    4179\n",
      "29    4179\n",
      "30    4179\n",
      "31    4179\n",
      "32    4179\n",
      "33    4179\n",
      "34    4179\n",
      "35    4179\n",
      "20    4179\n",
      "19    4179\n",
      "18    4179\n",
      "9     4179\n",
      "3     4179\n",
      "4     4179\n",
      "5     4179\n",
      "6     4179\n",
      "7     4179\n",
      "8     4179\n",
      "10    4179\n",
      "17    4179\n",
      "11    4179\n",
      "12    4179\n",
      "13    4179\n",
      "14    4179\n",
      "15    4179\n",
      "16    4179\n",
      "36    4179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'campaign_id':\n",
      "campaign_id\n",
      "22    11556\n",
      "24    11556\n",
      "23    11556\n",
      "1      4104\n",
      "2      4104\n",
      "3      4104\n",
      "45     3672\n",
      "43     3672\n",
      "44     3672\n",
      "8      3564\n",
      "9      3564\n",
      "7      3564\n",
      "4      3420\n",
      "5      3420\n",
      "6      3420\n",
      "41     3312\n",
      "40     3312\n",
      "12     3312\n",
      "11     3312\n",
      "10     3312\n",
      "42     3312\n",
      "33     3096\n",
      "32     3096\n",
      "31     3096\n",
      "27     3024\n",
      "25     3024\n",
      "26     3024\n",
      "37     2952\n",
      "39     2952\n",
      "38     2952\n",
      "34     2772\n",
      "35     2772\n",
      "36     2772\n",
      "48     2592\n",
      "47     2592\n",
      "46     2592\n",
      "17     1404\n",
      "16     1404\n",
      "18     1404\n",
      "29     1008\n",
      "28     1008\n",
      "30     1008\n",
      "20      216\n",
      "19      216\n",
      "21      216\n",
      "15      144\n",
      "14      144\n",
      "13      144\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'product_id':\n",
      "product_id\n",
      "42899542     108\n",
      "155701266    108\n",
      "167012865    108\n",
      "167002852    108\n",
      "164483934    108\n",
      "            ... \n",
      "7823134      108\n",
      "7599073      108\n",
      "7189521      108\n",
      "7170169      108\n",
      "274070205    108\n",
      "Name: count, Length: 1393, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_cnt_month':\n",
      "item_cnt_month\n",
      "31     8817\n",
      "30     8694\n",
      "32     8267\n",
      "29     8195\n",
      "28     7635\n",
      "33     7454\n",
      "27     6845\n",
      "34     6586\n",
      "35     5581\n",
      "26     5484\n",
      "25     4443\n",
      "36     4179\n",
      "37     3376\n",
      "24     3316\n",
      "38     2413\n",
      "23     2349\n",
      "39     1737\n",
      "22     1654\n",
      "40     1242\n",
      "60     1173\n",
      "63     1150\n",
      "58     1144\n",
      "61     1142\n",
      "59     1121\n",
      "62     1118\n",
      "21     1096\n",
      "57     1045\n",
      "64     1029\n",
      "56      995\n",
      "65      982\n",
      "55      930\n",
      "66      892\n",
      "54      873\n",
      "67      871\n",
      "41      770\n",
      "68      764\n",
      "53      761\n",
      "52      685\n",
      "69      666\n",
      "20      661\n",
      "51      628\n",
      "50      613\n",
      "42      559\n",
      "70      529\n",
      "71      512\n",
      "49      497\n",
      "48      486\n",
      "134     462\n",
      "43      453\n",
      "138     428\n",
      "139     413\n",
      "127     410\n",
      "128     410\n",
      "132     409\n",
      "137     408\n",
      "136     406\n",
      "141     404\n",
      "133     401\n",
      "130     400\n",
      "46      399\n",
      "131     397\n",
      "47      396\n",
      "142     395\n",
      "129     394\n",
      "44      392\n",
      "140     387\n",
      "135     386\n",
      "19      378\n",
      "126     373\n",
      "72      371\n",
      "45      358\n",
      "125     350\n",
      "124     347\n",
      "144     341\n",
      "145     334\n",
      "143     333\n",
      "123     326\n",
      "148     323\n",
      "146     323\n",
      "121     316\n",
      "119     313\n",
      "73      309\n",
      "122     306\n",
      "120     306\n",
      "147     304\n",
      "74      285\n",
      "150     265\n",
      "118     265\n",
      "151     264\n",
      "149     256\n",
      "154     253\n",
      "153     242\n",
      "117     237\n",
      "116     235\n",
      "94      225\n",
      "91      219\n",
      "152     218\n",
      "75      216\n",
      "155     215\n",
      "93      211\n",
      "87      208\n",
      "18      208\n",
      "113     208\n",
      "115     207\n",
      "114     206\n",
      "96      204\n",
      "90      193\n",
      "76      191\n",
      "89      188\n",
      "156     186\n",
      "92      185\n",
      "105     182\n",
      "85      182\n",
      "88      181\n",
      "112     181\n",
      "95      178\n",
      "158     174\n",
      "111     173\n",
      "97      170\n",
      "99      169\n",
      "159     165\n",
      "160     164\n",
      "77      164\n",
      "108     164\n",
      "98      164\n",
      "100     160\n",
      "86      159\n",
      "110     151\n",
      "157     150\n",
      "101     150\n",
      "109     148\n",
      "107     141\n",
      "84      140\n",
      "161     140\n",
      "103     138\n",
      "102     137\n",
      "78      132\n",
      "82      130\n",
      "79      128\n",
      "80      127\n",
      "81      126\n",
      "104     126\n",
      "106     125\n",
      "162     120\n",
      "17      114\n",
      "83      105\n",
      "165     100\n",
      "166      92\n",
      "163      90\n",
      "164      81\n",
      "167      77\n",
      "168      72\n",
      "169      65\n",
      "170      63\n",
      "16       56\n",
      "171      48\n",
      "174      46\n",
      "173      41\n",
      "172      41\n",
      "177      33\n",
      "176      32\n",
      "175      31\n",
      "181      29\n",
      "178      28\n",
      "180      28\n",
      "262      27\n",
      "275      25\n",
      "179      24\n",
      "15       22\n",
      "296      22\n",
      "266      22\n",
      "269      21\n",
      "274      21\n",
      "260      21\n",
      "281      21\n",
      "289      21\n",
      "277      20\n",
      "278      19\n",
      "264      19\n",
      "268      19\n",
      "188      19\n",
      "283      18\n",
      "185      18\n",
      "272      18\n",
      "187      18\n",
      "254      18\n",
      "276      17\n",
      "271      17\n",
      "265      17\n",
      "256      17\n",
      "261      17\n",
      "273      17\n",
      "267      17\n",
      "184      16\n",
      "182      16\n",
      "183      16\n",
      "259      16\n",
      "186      16\n",
      "290      16\n",
      "405      16\n",
      "253      16\n",
      "251      16\n",
      "263      15\n",
      "300      15\n",
      "191      15\n",
      "196      15\n",
      "258      15\n",
      "294      15\n",
      "14       14\n",
      "285      14\n",
      "435      14\n",
      "295      14\n",
      "413      13\n",
      "255      13\n",
      "282      13\n",
      "298      13\n",
      "395      13\n",
      "396      13\n",
      "280      13\n",
      "189      13\n",
      "392      13\n",
      "305      13\n",
      "194      13\n",
      "279      13\n",
      "387      13\n",
      "297      13\n",
      "257      12\n",
      "286      12\n",
      "420      12\n",
      "193      12\n",
      "417      11\n",
      "406      11\n",
      "426      11\n",
      "403      11\n",
      "246      11\n",
      "415      11\n",
      "288      11\n",
      "301      11\n",
      "284      11\n",
      "287      11\n",
      "421      11\n",
      "241      11\n",
      "250      10\n",
      "432      10\n",
      "303      10\n",
      "414      10\n",
      "252      10\n",
      "408      10\n",
      "419      10\n",
      "394      10\n",
      "292      10\n",
      "434      10\n",
      "249      10\n",
      "411      10\n",
      "248       9\n",
      "429       9\n",
      "299       9\n",
      "430       9\n",
      "291       9\n",
      "399       9\n",
      "433       9\n",
      "424       9\n",
      "192       9\n",
      "428       9\n",
      "397       9\n",
      "436       9\n",
      "400       9\n",
      "427       9\n",
      "195       9\n",
      "307       9\n",
      "418       9\n",
      "190       9\n",
      "409       9\n",
      "308       9\n",
      "380       9\n",
      "197       9\n",
      "237       8\n",
      "447       8\n",
      "312       8\n",
      "239       8\n",
      "443       8\n",
      "416       8\n",
      "247       8\n",
      "404       8\n",
      "398       8\n",
      "306       8\n",
      "309       8\n",
      "441       8\n",
      "270       8\n",
      "242       8\n",
      "243       8\n",
      "236       8\n",
      "234       7\n",
      "389       7\n",
      "425       7\n",
      "388       7\n",
      "384       7\n",
      "229       7\n",
      "376       7\n",
      "293       7\n",
      "369       7\n",
      "438       7\n",
      "381       7\n",
      "198       7\n",
      "302       7\n",
      "374       7\n",
      "200       6\n",
      "407       6\n",
      "445       6\n",
      "423       6\n",
      "385       6\n",
      "383       6\n",
      "386       6\n",
      "238       6\n",
      "203       6\n",
      "375       6\n",
      "313       6\n",
      "245       6\n",
      "401       6\n",
      "378       6\n",
      "321       6\n",
      "221       6\n",
      "317       6\n",
      "454       5\n",
      "464       5\n",
      "207       5\n",
      "199       5\n",
      "440       5\n",
      "201       5\n",
      "448       5\n",
      "304       5\n",
      "451       5\n",
      "240       5\n",
      "439       5\n",
      "371       5\n",
      "230       5\n",
      "222       5\n",
      "13        5\n",
      "226       5\n",
      "402       5\n",
      "412       5\n",
      "205       5\n",
      "422       5\n",
      "449       5\n",
      "372       5\n",
      "379       5\n",
      "458       4\n",
      "393       4\n",
      "206       4\n",
      "235       4\n",
      "211       4\n",
      "214       4\n",
      "355       4\n",
      "437       4\n",
      "431       4\n",
      "459       4\n",
      "341       4\n",
      "244       4\n",
      "202       4\n",
      "310       4\n",
      "366       4\n",
      "391       4\n",
      "233       4\n",
      "455       4\n",
      "358       4\n",
      "350       3\n",
      "349       3\n",
      "377       3\n",
      "356       3\n",
      "328       3\n",
      "314       3\n",
      "216       3\n",
      "410       3\n",
      "311       3\n",
      "227       3\n",
      "204       3\n",
      "318       3\n",
      "470       3\n",
      "478       3\n",
      "220       3\n",
      "461       3\n",
      "363       3\n",
      "354       3\n",
      "322       3\n",
      "351       3\n",
      "346       3\n",
      "456       3\n",
      "232       3\n",
      "209       3\n",
      "450       3\n",
      "367       3\n",
      "323       2\n",
      "210       2\n",
      "223       2\n",
      "217       2\n",
      "472       2\n",
      "326       2\n",
      "315       2\n",
      "390       2\n",
      "316       2\n",
      "353       2\n",
      "489       2\n",
      "338       2\n",
      "231       2\n",
      "343       2\n",
      "357       2\n",
      "365       2\n",
      "335       2\n",
      "452       2\n",
      "352       2\n",
      "477       2\n",
      "359       2\n",
      "465       2\n",
      "337       2\n",
      "469       2\n",
      "460       2\n",
      "442       2\n",
      "218       2\n",
      "347       2\n",
      "444       2\n",
      "463       2\n",
      "225       2\n",
      "476       2\n",
      "208       2\n",
      "319       2\n",
      "482       2\n",
      "462       2\n",
      "324       1\n",
      "342       1\n",
      "360       1\n",
      "320       1\n",
      "368       1\n",
      "325       1\n",
      "336       1\n",
      "492       1\n",
      "494       1\n",
      "466       1\n",
      "468       1\n",
      "467       1\n",
      "471       1\n",
      "344       1\n",
      "361       1\n",
      "491       1\n",
      "12        1\n",
      "370       1\n",
      "228       1\n",
      "329       1\n",
      "340       1\n",
      "330       1\n",
      "474       1\n",
      "364       1\n",
      "11        1\n",
      "327       1\n",
      "332       1\n",
      "457       1\n",
      "362       1\n",
      "224       1\n",
      "212       1\n",
      "483       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'profit_month':\n",
      "profit_month\n",
      "1550000     5995\n",
      "1500000     5866\n",
      "1600000     5634\n",
      "1450000     5599\n",
      "1400000     5174\n",
      "            ... \n",
      "23000000       1\n",
      "4945000        1\n",
      "2820000        1\n",
      "4545000        1\n",
      "10850000       1\n",
      "Name: count, Length: 1317, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in matrix.columns:\n",
    "    print(f\"Số lượng giá trị của cột '{column}':\")\n",
    "    print(matrix[column].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b867b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thực hiện phép nối giữa DataFrame 'matrix' và DataFrame 'items' dựa trên cột 'product_id' của 'matrix' và cột 'id' của 'items'\n",
    "matrix = pd.merge(matrix, items, left_on=['product_id'], right_on=['id'], how='left')\n",
    "\n",
    "# Xóa cột 'id' trong DataFrame 'matrix' nếu bạn không cần nó sau khi đã thực hiện phép nối\n",
    "matrix.drop('id', axis=1, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd4a2350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>85</td>\n",
       "      <td>425000</td>\n",
       "      <td>Áo ba lỗ nam/ 3 lỗ nam cao cấp, sợi cotton mềm...</td>\n",
       "      <td>CHI TIẾT SẢN PHẨM. Áo Ba lỗ Tanktop MRM kiểu ...</td>\n",
       "      <td>10378</td>\n",
       "      <td>Áo thun nam ba lỗ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>27</td>\n",
       "      <td>135000</td>\n",
       "      <td>Siêu giảm giá. Áo thun wash Nhật Bản thương hi...</td>\n",
       "      <td>Thông số áo thun (áo phông) wash GOKINGForm áo...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>97</td>\n",
       "      <td>4850000</td>\n",
       "      <td>Combo 2 áo ba lỗ nam, áo sát nách nam,sợi cott...</td>\n",
       "      <td>KIỂU DÁNG: SLIM FITCHI TIẾT:- Áo Tanktop sá...</td>\n",
       "      <td>10378</td>\n",
       "      <td>Áo thun nam ba lỗ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>28</td>\n",
       "      <td>140000</td>\n",
       "      <td>Áo thun nam tay lỡ form rộng Stay Real 69</td>\n",
       "      <td>Áo thun nam tay lỡ form rộng Stay Real 69Colo...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>30</td>\n",
       "      <td>150000</td>\n",
       "      <td>Áo One Piece Zoro ATT-02, Áo thun Anime Manga ...</td>\n",
       "      <td>Áo One Piece Zoro ATT-02, áo thun One Piece Ro...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150439</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>33</td>\n",
       "      <td>1650000</td>\n",
       "      <td>Giày Boots Nam chính hãng BXXY (879)</td>\n",
       "      <td>Hàng chính hãng Thương hiệu: BXXY Nhà sản xuấ...</td>\n",
       "      <td>49642</td>\n",
       "      <td>Giày boots nam cổ thấp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150440</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>29</td>\n",
       "      <td>1450000</td>\n",
       "      <td>GIÀY BOOT ECCO NAM GRAINER M  21470401001 222</td>\n",
       "      <td>Mang dáng vẻ mới mẻ, bộ sưu tập giày Boot ECCO...</td>\n",
       "      <td>49642</td>\n",
       "      <td>Giày boots nam cổ thấp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150441</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>38</td>\n",
       "      <td>1900000</td>\n",
       "      <td>Timberland Giày Nam Boot Cổ Trung Basic Chukka...</td>\n",
       "      <td>Thông tin thương hiệu: Timberland là một nhà s...</td>\n",
       "      <td>49642</td>\n",
       "      <td>Giày boots nam cổ thấp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150442</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>28</td>\n",
       "      <td>1400000</td>\n",
       "      <td>Giày Boots Nam GEOX U Spherica Ec1 C</td>\n",
       "      <td>• Đôi boot cổ thấp hiện đại dà...</td>\n",
       "      <td>49642</td>\n",
       "      <td>Giày boots nam cổ thấp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150443</th>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>27</td>\n",
       "      <td>1350000</td>\n",
       "      <td>Giày Boots Nam GEOX U Portello A</td>\n",
       "      <td>• Giày boots Portello với thiết kế siêu nhẹ, đ...</td>\n",
       "      <td>49642</td>\n",
       "      <td>Giày boots nam cổ thấp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150444 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  item_cnt_month  profit_month   \n",
       "0                    1            1    42899542              85        425000  \\\n",
       "1                    1            1    61393910              27        135000   \n",
       "2                    1            1    68202834              97       4850000   \n",
       "3                    1            1    73552136              28        140000   \n",
       "4                    1            1    74421745              30        150000   \n",
       "...                ...          ...         ...             ...           ...   \n",
       "150439              36           48   273386993              33       1650000   \n",
       "150440              36           48   273633499              29       1450000   \n",
       "150441              36           48   273847739              38       1900000   \n",
       "150442              36           48   274069973              28       1400000   \n",
       "150443              36           48   274070205              27       1350000   \n",
       "\n",
       "                                                     name   \n",
       "0       Áo ba lỗ nam/ 3 lỗ nam cao cấp, sợi cotton mềm...  \\\n",
       "1       Siêu giảm giá. Áo thun wash Nhật Bản thương hi...   \n",
       "2       Combo 2 áo ba lỗ nam, áo sát nách nam,sợi cott...   \n",
       "3               Áo thun nam tay lỡ form rộng Stay Real 69   \n",
       "4       Áo One Piece Zoro ATT-02, Áo thun Anime Manga ...   \n",
       "...                                                   ...   \n",
       "150439               Giày Boots Nam chính hãng BXXY (879)   \n",
       "150440      GIÀY BOOT ECCO NAM GRAINER M  21470401001 222   \n",
       "150441  Timberland Giày Nam Boot Cổ Trung Basic Chukka...   \n",
       "150442               Giày Boots Nam GEOX U Spherica Ec1 C   \n",
       "150443                   Giày Boots Nam GEOX U Portello A   \n",
       "\n",
       "                                        short_description  categories_id   \n",
       "0        CHI TIẾT SẢN PHẨM. Áo Ba lỗ Tanktop MRM kiểu ...          10378  \\\n",
       "1       Thông số áo thun (áo phông) wash GOKINGForm áo...           1685   \n",
       "2          KIỂU DÁNG: SLIM FITCHI TIẾT:- Áo Tanktop sá...          10378   \n",
       "3        Áo thun nam tay lỡ form rộng Stay Real 69Colo...           1685   \n",
       "4       Áo One Piece Zoro ATT-02, áo thun One Piece Ro...           1685   \n",
       "...                                                   ...            ...   \n",
       "150439   Hàng chính hãng Thương hiệu: BXXY Nhà sản xuấ...          49642   \n",
       "150440  Mang dáng vẻ mới mẻ, bộ sưu tập giày Boot ECCO...          49642   \n",
       "150441  Thông tin thương hiệu: Timberland là một nhà s...          49642   \n",
       "150442                  • Đôi boot cổ thấp hiện đại dà...          49642   \n",
       "150443  • Giày boots Portello với thiết kế siêu nhẹ, đ...          49642   \n",
       "\n",
       "                      categories_name  \n",
       "0                   Áo thun nam ba lỗ  \n",
       "1       Áo thun nam ngắn tay không cổ  \n",
       "2                   Áo thun nam ba lỗ  \n",
       "3       Áo thun nam ngắn tay không cổ  \n",
       "4       Áo thun nam ngắn tay không cổ  \n",
       "...                               ...  \n",
       "150439         Giày boots nam cổ thấp  \n",
       "150440         Giày boots nam cổ thấp  \n",
       "150441         Giày boots nam cổ thấp  \n",
       "150442         Giày boots nam cổ thấp  \n",
       "150443         Giày boots nam cổ thấp  \n",
       "\n",
       "[150444 rows x 9 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a194f3",
   "metadata": {},
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76452f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lag_feature(df, lags, col):\n",
    "    # Tạo một bản sao của DataFrame chỉ chứa các cột 'date_block_num', 'shop_id', 'item_id' và cột 'col'\n",
    "    tmp = df[['date_block_num','campaign_id','product_id',col]]\n",
    "    \n",
    "    # Vòng lặp qua tất cả các độ trễ trong danh sách 'lags'\n",
    "    for i in lags:\n",
    "        # Tạo một bản sao của DataFrame 'tmp' để thêm cột độ trễ\n",
    "        shifted = tmp.copy()\n",
    "        \n",
    "        # Đặt tên cho cột mới với độ trễ tương ứng\n",
    "        shifted.columns = ['date_block_num','campaign_id','product_id', col+'_lag_'+str(i)]\n",
    "        \n",
    "        # Tăng giá trị 'date_block_num' của DataFrame 'shifted' lên 'i'\n",
    "        shifted['date_block_num'] += i\n",
    "        \n",
    "        # Kết hợp DataFrame gốc 'df' với DataFrame 'shifted' dựa trên các cột 'date_block_num', 'shop_id', 'item_id' và phương thức 'left join'\n",
    "        df = pd.merge(df, shifted, on=['date_block_num','campaign_id','product_id'], how='left')\n",
    "    \n",
    "    # Trả về DataFrame đã được mở rộng với các cột độ trễ\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d828815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8441fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_group_stats(matrix_, groupby_feats, target, enc_feat, last_periods):\n",
    "    # Kiểm tra nếu 'date_block_num' không nằm trong các cột nhóm\n",
    "    if not 'date_block_num' in groupby_feats:\n",
    "        print ('date_block_num must in groupby_feats') # In thông báo lỗi\n",
    "        return matrix_ # Trả về DataFrame ban đầu nếu có lỗi\n",
    "    \n",
    "    # Nhóm dữ liệu theo các cột nhóm 'groupby_feats' và tính tổng 'target' cho mỗi nhóm\n",
    "    group = matrix_.groupby(groupby_feats)[target].sum().reset_index()\n",
    "    \n",
    "    # Xác định số lượng lớn nhất của độ trễ trong 'last_periods'\n",
    "    max_lags = np.max(last_periods)\n",
    "    \n",
    "    # Vòng lặp qua tất cả các độ trễ từ 1 đến 'max_lags'\n",
    "    for i in range(1,max_lags+1):\n",
    "        # Tạo bản sao của nhóm dữ liệu và tăng giá trị 'date_block_num' lên 'i'\n",
    "        shifted = group[groupby_feats+[target]].copy(deep=True)\n",
    "        shifted['date_block_num'] += i\n",
    "        \n",
    "        # Đổi tên cột 'target' thành 'target+'_lag_'+str(i)'\n",
    "        shifted.rename({target:target+'_lag_'+str(i)},axis=1,inplace=True)\n",
    "        \n",
    "        # Kết hợp DataFrame 'group' với DataFrame 'shifted' dựa trên các cột nhóm và phương thức 'left join'\n",
    "        group = group.merge(shifted, on=groupby_feats, how='left')\n",
    "    \n",
    "    # Thay thế giá trị NaN bằng 0\n",
    "    group.fillna(0,inplace=True)\n",
    "    \n",
    "    # Vòng lặp qua các khoảng thời gian 'last_periods'\n",
    "    for period in last_periods:\n",
    "        lag_feats = [target+'_lag_'+str(lag) for lag in np.arange(1,period+1)]\n",
    "        \n",
    "        # Tính trung bình và bình phương trung bình của 'lag_feats'\n",
    "        mean = group[lag_feats].sum(axis=1)/float(period)\n",
    "        mean2 = (group[lag_feats]**2).sum(axis=1)/float(period)\n",
    "        \n",
    "        # Tạo các cột mới chứa giá trị trung bình và độ lệch chuẩn\n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] = mean\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] = (mean2 - mean**2).apply(np.sqrt)\n",
    "        \n",
    "        # Thay thế giá trị vô hướng bằng 0\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)].replace(np.inf,0,inplace=True)\n",
    "        \n",
    "        # Chia cho giá trị trung bình của mỗi cột, điều này làm cho các đặc trưng có tỷ lệ giống nhau cho mạng nơ-ron\n",
    "        group[enc_feat+'_avg_sale_last_'+str(period)] /= group[enc_feat+'_avg_sale_last_'+str(period)].mean()\n",
    "        group[enc_feat+'_std_sale_last_'+str(period)] /= group[enc_feat+'_std_sale_last_'+str(period)].mean()\n",
    "    \n",
    "    # Tạo danh sách cột mới cần thêm vào 'matrix_'\n",
    "    cols = groupby_feats + [f_ for f_ in group.columns.values if f_.find('_sale_last_')>=0]\n",
    "    \n",
    "    # Kết hợp DataFrame 'matrix_' với các cột mới từ DataFrame 'group' dựa trên các cột nhóm và phương thức 'left join'\n",
    "    matrix = matrix_.merge(group[cols], on=groupby_feats, how='left')\n",
    "    \n",
    "    # Trả về DataFrame đã được mở rộng với các cột mới\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db2a07e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3395824432373047"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'product_id'], 'item_cnt_month', 'product', [6,12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'campaign_id'], 'item_cnt_month', 'campaign', [6,12])\n",
    "matrix = add_group_stats(matrix, ['date_block_num', 'categories_id'], 'item_cnt_month', 'category', [12])\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c31faa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(matrix_, groupby_feats, target, enc_feat, lags):\n",
    "    # In ra thông báo về việc sử dụng target encoding cho các nhóm được chỉ định\n",
    "    print ('target encoding for',groupby_feats)\n",
    "    \n",
    "    # Nhóm dữ liệu theo các cột nhóm và tính trung bình của 'target' cho mỗi nhóm\n",
    "    group = matrix_.groupby(groupby_feats).agg({target:'mean'})\n",
    "    \n",
    "    # Đặt tên cho cột kết quả là 'enc_feat'\n",
    "    group.columns = [enc_feat]\n",
    "    \n",
    "    # Đặt lại chỉ số của DataFrame 'group' và kết hợp nó với DataFrame gốc 'matrix_'\n",
    "    group.reset_index(inplace=True)\n",
    "    matrix = matrix_.merge(group, on=groupby_feats, how='left')\n",
    "    \n",
    "    # Chuyển đổi kiểu dữ liệu của cột 'enc_feat' thành np.float16\n",
    "    matrix[enc_feat] = matrix[enc_feat].astype(np.float16)\n",
    "    \n",
    "    # Tạo các đặc trưng độ trễ từ đặc trưng đã được mã hóa mục tiêu\n",
    "    matrix = lag_feature(matrix, lags, enc_feat)\n",
    "    \n",
    "    # Loại bỏ cột mã hóa mục tiêu đã được tạo ra từ 'matrix_'\n",
    "    matrix.drop(enc_feat, axis=1, inplace=True)\n",
    "    \n",
    "    # Trả về DataFrame 'matrix_' đã được mở rộng với các đặc trưng độ trễ từ mã hóa mục tiêu\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31f493c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target encoding for ['date_block_num']\n",
      "target encoding for ['date_block_num', 'campaign_id']\n",
      "target encoding for ['date_block_num', 'product_id']\n",
      "target encoding for ['date_block_num', 'campaign_id', 'categories_id']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.516470193862915"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts = time.time()\n",
    "matrix = target_encoding(matrix, ['date_block_num'], 'item_cnt_month', 'date_avg_item_cnt', [1])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'campaign_id'], 'item_cnt_month', 'date_campaign_avg_item_cnt', [1,2,3,6,12])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'product_id'], 'item_cnt_month', 'date_item_avg_item_cnt', [1,2,3,6,12])\n",
    "matrix = target_encoding(matrix, ['date_block_num', 'campaign_id', 'categories_id'], 'item_cnt_month', 'date_campaign_cat_avg_item_cnt', [1])\n",
    "\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "714b0f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.446274280548096"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Lấy thời gian bắt đầu tính toán\n",
    "ts = time.time()\n",
    "\n",
    "# Tính giá trung bình của mỗi mặt hàng và gán vào cột mới 'item_avg_item_price'\n",
    "group = train.groupby(['product_id']).agg({'price': ['mean']})\n",
    "group.columns = ['item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "# Kết hợp DataFrame 'matrix' với DataFrame 'group' dựa trên cột 'item_id'\n",
    "matrix = pd.merge(matrix, group, on=['product_id'], how='left')\n",
    "matrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "# Tính giá trung bình của mỗi mặt hàng theo tháng và gán vào cột mới 'date_item_avg_item_price'\n",
    "group = train.groupby(['date_block_num','product_id']).agg({'price': ['mean']})\n",
    "group.columns = ['date_item_avg_item_price']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "# Kết hợp DataFrame 'matrix' với DataFrame 'group' dựa trên các cột 'date_block_num' và 'item_id'\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','product_id'], how='left')\n",
    "matrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n",
    "\n",
    "# Xác định các độ trễ cần sử dụng\n",
    "lags = [1,2,3,4,5,6]\n",
    "\n",
    "# Tạo các đặc trưng độ trễ từ cột 'date_item_avg_item_price' sử dụng các độ trễ trong 'lags'\n",
    "for lag in lags:\n",
    "    matrix['date_item_avg_item_price_lag_'+str(lag)] = matrix.groupby(['product_id'])['date_item_avg_item_price'].shift(lag)\n",
    "\n",
    "# Tính toán biến thể giá so với giá trung bình của mỗi mặt hàng và gán vào các cột 'delta_price_lag_i'\n",
    "for lag in lags:\n",
    "    matrix['delta_price_lag_'+str(lag)] = \\\n",
    "        (matrix['date_item_avg_item_price_lag_'+str(lag)] - matrix['item_avg_item_price']) / matrix['item_avg_item_price']\n",
    "\n",
    "# Lựa chọn xu hướng giá mới nhất từ các cột 'delta_price_lag_i'\n",
    "def select_trend(row):\n",
    "    for lag in lags:\n",
    "        if pd.notna(row['delta_price_lag_'+str(lag)]):\n",
    "            return row['delta_price_lag_'+str(lag)]\n",
    "    return 0\n",
    "\n",
    "matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\n",
    "matrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\n",
    "matrix['delta_price_lag'].fillna(0, inplace=True)\n",
    "\n",
    "# Loại bỏ các cột không cần thiết đã tạo ra\n",
    "features_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\n",
    "for lag in lags:\n",
    "    features_to_drop += ['date_item_avg_item_price_lag_'+str(lag)]\n",
    "    features_to_drop += ['delta_price_lag_'+str(lag)]\n",
    "\n",
    "matrix.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Tính thời gian tính toán\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8deca213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1299242973327637"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lấy thời gian bắt đầu tính toán\n",
    "ts = time.time()\n",
    "\n",
    "# Tính tổng doanh thu của mỗi cửa hàng theo tháng và gán vào cột mới 'date_shop_revenue'\n",
    "group = train.groupby(['date_block_num','campaign_id']).agg({'revenue': ['sum']})\n",
    "group.columns = ['date_campaign_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "# Kết hợp DataFrame 'matrix' với DataFrame 'group' dựa trên các cột 'date_block_num' và 'shop_id'\n",
    "matrix = pd.merge(matrix, group, on=['date_block_num','campaign_id'], how='left')\n",
    "matrix['date_campaign_revenue'] = matrix['date_campaign_revenue'].astype(np.float32)\n",
    "\n",
    "# Tính giá trị trung bình của doanh thu của mỗi cửa hàng và gán vào cột mới 'shop_avg_revenue'\n",
    "group = group.groupby(['campaign_id']).agg({'date_campaign_revenue': ['mean']})\n",
    "group.columns = ['campaign_avg_revenue']\n",
    "group.reset_index(inplace=True)\n",
    "\n",
    "# Kết hợp DataFrame 'matrix' với DataFrame 'group' dựa trên cột 'shop_id'\n",
    "matrix = pd.merge(matrix, group, on=['campaign_id'], how='left')\n",
    "matrix['campaign_avg_revenue'] = matrix['campaign_avg_revenue'].astype(np.float32)\n",
    "\n",
    "# Tính toán biến thể của doanh thu so với giá trị trung bình của mỗi cửa hàng và gán vào cột mới 'delta_revenue'\n",
    "matrix['delta_revenue'] = (matrix['date_campaign_revenue'] - matrix['campaign_avg_revenue']) / matrix['campaign_avg_revenue']\n",
    "matrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n",
    "\n",
    "# Tạo đặc trưng độ trễ từ cột 'delta_revenue' sử dụng độ trễ là 1\n",
    "matrix = lag_feature(matrix, [1], 'delta_revenue')\n",
    "\n",
    "# Loại bỏ các cột không cần thiết đã tạo ra\n",
    "matrix.drop(['date_campaign_revenue','campaign_avg_revenue','delta_revenue'], axis=1, inplace=True)\n",
    "\n",
    "# Tính thời gian tính toán\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16a9cf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix['month'] = matrix['date_block_num'] % 12\n",
    "matrix['year'] = (matrix['date_block_num'] / 12).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7328e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo một dataframe rỗng để lưu trữ thông tin về lần bán hàng cuối cùng của mỗi cặp cửa hàng / sản phẩm.\n",
    "last_sale = pd.DataFrame()\n",
    "\n",
    "# Duyệt qua các tháng từ 1 đến 36.\n",
    "for month in range(1,37):\n",
    "    # Tạo một dataframe last_month bằng cách lấy ra tháng lớn nhất mà sản phẩm đã được bán trong tháng đó cho mỗi cặp cửa hàng / sản phẩm trước tháng hiện tại.\n",
    "    last_month = matrix.loc[(matrix['date_block_num']<month)&(matrix['item_cnt_month']>0)].groupby(['product_id','campaign_id'])['date_block_num'].max()\n",
    "    \n",
    "    # Tạo một dataframe df từ thông tin về tháng hiện tại, id sản phẩm, id cửa hàng và số tháng kể từ lần bán hàng cuối cùng.\n",
    "    df = pd.DataFrame({\n",
    "        'date_block_num':np.ones([last_month.shape[0],])*month,\n",
    "        'product_id': last_month.index.get_level_values(0).values,\n",
    "        'campaign_id': last_month.index.get_level_values(1).values,\n",
    "        'item_campaign_last_sale': last_month.values\n",
    "    })\n",
    "    \n",
    "    # Gộp dataframe df vào dataframe last_sale.\n",
    "    last_sale = pd.concat([last_sale, df], ignore_index=True)\n",
    "\n",
    "last_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int32)\n",
    "\n",
    "# Ghép dataframe last_sale vào dataframe matrix dựa trên các cột 'date_block_num', 'item_id', và 'shop_id' với phương pháp gộp 'left'.\n",
    "matrix = matrix.merge(last_sale, on=['date_block_num','product_id','campaign_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a5073ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3615472316741943"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tính số tháng kể từ lần bán hàng cuối cùng cho mỗi sản phẩm.\n",
    "ts = time.time()\n",
    "\n",
    "# Khởi tạo một dataframe rỗng để lưu trữ thông tin về lần bán hàng cuối cùng của mỗi sản phẩm.\n",
    "last_sale = pd.DataFrame()\n",
    "\n",
    "# Duyệt qua các tháng từ 1 đến 36.\n",
    "for month in range(1, 37):\n",
    "    # Tạo một dataframe last_month bằng cách lấy ra tháng lớn nhất mà sản phẩm đã được bán trong tháng đó cho mỗi sản phẩm trước tháng hiện tại.\n",
    "    last_month = matrix.loc[(matrix['date_block_num'] < month) & (matrix['item_cnt_month'] > 0)].groupby('product_id')['date_block_num'].max()\n",
    "    \n",
    "    # Tạo một dataframe df từ thông tin về tháng hiện tại, id sản phẩm và số tháng kể từ lần bán hàng cuối cùng của sản phẩm.\n",
    "    df = pd.DataFrame({\n",
    "        'date_block_num': np.ones([last_month.shape[0],])*month,\n",
    "        'product_id': last_month.index.values,\n",
    "        'item_last_sale': last_month.values\n",
    "    })\n",
    "    \n",
    "    # Gộp dataframe df vào dataframe last_sale.\n",
    "    last_sale = pd.concat([last_sale, df], ignore_index=True)\n",
    "\n",
    "# Chuyển đổi kiểu dữ liệu cột date_block_num của dataframe last_sale sang kiểu dữ liệu số nguyên nhỏ để tiết kiệm bộ nhớ.\n",
    "last_sale['date_block_num'] = last_sale['date_block_num'].astype(np.int32)\n",
    "\n",
    "# Ghép dataframe last_sale vào dataframe matrix dựa trên các cột 'date_block_num' và 'item_id' với phương pháp gộp 'left'.\n",
    "matrix = matrix.merge(last_sale, on=['date_block_num', 'product_id'], how='left')\n",
    "\n",
    "# Tính thời gian thực thi của đoạn mã.\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c5d97c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05155348777770996"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Months since the first sale for each shop/item pair and for item only.\n",
    "ts = time.time()\n",
    "matrix['item_campaign_first_sale'] = matrix['date_block_num'] - matrix.groupby(['product_id','campaign_id'])['date_block_num'].transform('min')\n",
    "matrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('product_id')['date_block_num'].transform('min')\n",
    "time.time() - ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b623ec18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_block_num', 'campaign_id', 'product_id', 'item_cnt_month',\n",
       "       'profit_month', 'name', 'short_description', 'categories_id',\n",
       "       'categories_name', 'item_cnt_month_lag_1', 'item_cnt_month_lag_2',\n",
       "       'item_cnt_month_lag_3', 'item_cnt_month_lag_6', 'item_cnt_month_lag_12',\n",
       "       'product_avg_sale_last_6', 'product_std_sale_last_6',\n",
       "       'product_avg_sale_last_12', 'product_std_sale_last_12',\n",
       "       'campaign_avg_sale_last_6', 'campaign_std_sale_last_6',\n",
       "       'campaign_avg_sale_last_12', 'campaign_std_sale_last_12',\n",
       "       'category_avg_sale_last_12', 'category_std_sale_last_12',\n",
       "       'date_avg_item_cnt_lag_1', 'date_campaign_avg_item_cnt_lag_1',\n",
       "       'date_campaign_avg_item_cnt_lag_2', 'date_campaign_avg_item_cnt_lag_3',\n",
       "       'date_campaign_avg_item_cnt_lag_6', 'date_campaign_avg_item_cnt_lag_12',\n",
       "       'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2',\n",
       "       'date_item_avg_item_cnt_lag_3', 'date_item_avg_item_cnt_lag_6',\n",
       "       'date_item_avg_item_cnt_lag_12', 'date_campaign_cat_avg_item_cnt_lag_1',\n",
       "       'delta_price_lag', 'delta_revenue_lag_1', 'month', 'year',\n",
       "       'item_campaign_last_sale', 'item_last_sale', 'item_campaign_first_sale',\n",
       "       'item_first_sale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix = matrix[matrix.date_block_num > 11]\n",
    "matrix.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "285ac50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.to_pickle('data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61e7c2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>item_cnt_month_lag_12</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_12</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_12</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_campaign_last_sale</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>85</td>\n",
       "      <td>425000</td>\n",
       "      <td>Áo ba lỗ nam/ 3 lỗ nam cao cấp, sợi cotton mềm...</td>\n",
       "      <td>CHI TIẾT SẢN PHẨM. Áo Ba lỗ Tanktop MRM kiểu ...</td>\n",
       "      <td>10378</td>\n",
       "      <td>Áo thun nam ba lỗ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>27</td>\n",
       "      <td>135000</td>\n",
       "      <td>Siêu giảm giá. Áo thun wash Nhật Bản thương hi...</td>\n",
       "      <td>Thông số áo thun (áo phông) wash GOKINGForm áo...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>97</td>\n",
       "      <td>4850000</td>\n",
       "      <td>Combo 2 áo ba lỗ nam, áo sát nách nam,sợi cott...</td>\n",
       "      <td>KIỂU DÁNG: SLIM FITCHI TIẾT:- Áo Tanktop sá...</td>\n",
       "      <td>10378</td>\n",
       "      <td>Áo thun nam ba lỗ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>28</td>\n",
       "      <td>140000</td>\n",
       "      <td>Áo thun nam tay lỡ form rộng Stay Real 69</td>\n",
       "      <td>Áo thun nam tay lỡ form rộng Stay Real 69Colo...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>30</td>\n",
       "      <td>150000</td>\n",
       "      <td>Áo One Piece Zoro ATT-02, Áo thun Anime Manga ...</td>\n",
       "      <td>Áo One Piece Zoro ATT-02, áo thun One Piece Ro...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num  campaign_id  product_id  item_cnt_month  profit_month   \n",
       "0               1            1    42899542              85        425000  \\\n",
       "1               1            1    61393910              27        135000   \n",
       "2               1            1    68202834              97       4850000   \n",
       "3               1            1    73552136              28        140000   \n",
       "4               1            1    74421745              30        150000   \n",
       "\n",
       "                                                name   \n",
       "0  Áo ba lỗ nam/ 3 lỗ nam cao cấp, sợi cotton mềm...  \\\n",
       "1  Siêu giảm giá. Áo thun wash Nhật Bản thương hi...   \n",
       "2  Combo 2 áo ba lỗ nam, áo sát nách nam,sợi cott...   \n",
       "3          Áo thun nam tay lỡ form rộng Stay Real 69   \n",
       "4  Áo One Piece Zoro ATT-02, Áo thun Anime Manga ...   \n",
       "\n",
       "                                   short_description  categories_id   \n",
       "0   CHI TIẾT SẢN PHẨM. Áo Ba lỗ Tanktop MRM kiểu ...          10378  \\\n",
       "1  Thông số áo thun (áo phông) wash GOKINGForm áo...           1685   \n",
       "2     KIỂU DÁNG: SLIM FITCHI TIẾT:- Áo Tanktop sá...          10378   \n",
       "3   Áo thun nam tay lỡ form rộng Stay Real 69Colo...           1685   \n",
       "4  Áo One Piece Zoro ATT-02, áo thun One Piece Ro...           1685   \n",
       "\n",
       "                 categories_name  item_cnt_month_lag_1  item_cnt_month_lag_2   \n",
       "0              Áo thun nam ba lỗ                   NaN                   NaN  \\\n",
       "1  Áo thun nam ngắn tay không cổ                   NaN                   NaN   \n",
       "2              Áo thun nam ba lỗ                   NaN                   NaN   \n",
       "3  Áo thun nam ngắn tay không cổ                   NaN                   NaN   \n",
       "4  Áo thun nam ngắn tay không cổ                   NaN                   NaN   \n",
       "\n",
       "   item_cnt_month_lag_3  item_cnt_month_lag_6  item_cnt_month_lag_12   \n",
       "0                   NaN                   NaN                    NaN  \\\n",
       "1                   NaN                   NaN                    NaN   \n",
       "2                   NaN                   NaN                    NaN   \n",
       "3                   NaN                   NaN                    NaN   \n",
       "4                   NaN                   NaN                    NaN   \n",
       "\n",
       "   product_avg_sale_last_6  product_std_sale_last_6  product_avg_sale_last_12   \n",
       "0                      0.0                      0.0                       0.0  \\\n",
       "1                      0.0                      0.0                       0.0   \n",
       "2                      0.0                      0.0                       0.0   \n",
       "3                      0.0                      0.0                       0.0   \n",
       "4                      0.0                      0.0                       0.0   \n",
       "\n",
       "   product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "0                       0.0                       0.0  \\\n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "0                       0.0                        0.0  \\\n",
       "1                       0.0                        0.0   \n",
       "2                       0.0                        0.0   \n",
       "3                       0.0                        0.0   \n",
       "4                       0.0                        0.0   \n",
       "\n",
       "   campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "0                        0.0                        0.0  \\\n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "0                        0.0                      NaN  \\\n",
       "1                        0.0                      NaN   \n",
       "2                        0.0                      NaN   \n",
       "3                        0.0                      NaN   \n",
       "4                        0.0                      NaN   \n",
       "\n",
       "   date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "0                               NaN                               NaN  \\\n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "3                               NaN                               NaN   \n",
       "4                               NaN                               NaN   \n",
       "\n",
       "   date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "0                               NaN                               NaN  \\\n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "3                               NaN                               NaN   \n",
       "4                               NaN                               NaN   \n",
       "\n",
       "   date_campaign_avg_item_cnt_lag_12  date_item_avg_item_cnt_lag_1   \n",
       "0                                NaN                           NaN  \\\n",
       "1                                NaN                           NaN   \n",
       "2                                NaN                           NaN   \n",
       "3                                NaN                           NaN   \n",
       "4                                NaN                           NaN   \n",
       "\n",
       "   date_item_avg_item_cnt_lag_2  date_item_avg_item_cnt_lag_3   \n",
       "0                           NaN                           NaN  \\\n",
       "1                           NaN                           NaN   \n",
       "2                           NaN                           NaN   \n",
       "3                           NaN                           NaN   \n",
       "4                           NaN                           NaN   \n",
       "\n",
       "   date_item_avg_item_cnt_lag_6  date_item_avg_item_cnt_lag_12   \n",
       "0                           NaN                            NaN  \\\n",
       "1                           NaN                            NaN   \n",
       "2                           NaN                            NaN   \n",
       "3                           NaN                            NaN   \n",
       "4                           NaN                            NaN   \n",
       "\n",
       "   date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag  delta_revenue_lag_1   \n",
       "0                                   NaN              0.0                  NaN  \\\n",
       "1                                   NaN              0.0                  NaN   \n",
       "2                                   NaN              0.0                  NaN   \n",
       "3                                   NaN              0.0                  NaN   \n",
       "4                                   NaN              0.0                  NaN   \n",
       "\n",
       "   month  year  item_campaign_last_sale  item_last_sale   \n",
       "0      1     0                      NaN             NaN  \\\n",
       "1      1     0                      NaN             NaN   \n",
       "2      1     0                      NaN             NaN   \n",
       "3      1     0                      NaN             NaN   \n",
       "4      1     0                      NaN             NaN   \n",
       "\n",
       "   item_campaign_first_sale  item_first_sale  \n",
       "0                         0                0  \n",
       "1                         0                0  \n",
       "2                         0                0  \n",
       "3                         0                0  \n",
       "4                         0                0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('./data.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2256b318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>85</td>\n",
       "      <td>425000</td>\n",
       "      <td>Áo ba lỗ nam/ 3 lỗ nam cao cấp, sợi cotton mềm...</td>\n",
       "      <td>CHI TIẾT SẢN PHẨM. Áo Ba lỗ Tanktop MRM kiểu ...</td>\n",
       "      <td>10378</td>\n",
       "      <td>Áo thun nam ba lỗ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>27</td>\n",
       "      <td>135000</td>\n",
       "      <td>Siêu giảm giá. Áo thun wash Nhật Bản thương hi...</td>\n",
       "      <td>Thông số áo thun (áo phông) wash GOKINGForm áo...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>97</td>\n",
       "      <td>4850000</td>\n",
       "      <td>Combo 2 áo ba lỗ nam, áo sát nách nam,sợi cott...</td>\n",
       "      <td>KIỂU DÁNG: SLIM FITCHI TIẾT:- Áo Tanktop sá...</td>\n",
       "      <td>10378</td>\n",
       "      <td>Áo thun nam ba lỗ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>28</td>\n",
       "      <td>140000</td>\n",
       "      <td>Áo thun nam tay lỡ form rộng Stay Real 69</td>\n",
       "      <td>Áo thun nam tay lỡ form rộng Stay Real 69Colo...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>30</td>\n",
       "      <td>150000</td>\n",
       "      <td>Áo One Piece Zoro ATT-02, Áo thun Anime Manga ...</td>\n",
       "      <td>Áo One Piece Zoro ATT-02, áo thun One Piece Ro...</td>\n",
       "      <td>1685</td>\n",
       "      <td>Áo thun nam ngắn tay không cổ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_block_num  campaign_id  product_id  item_cnt_month  profit_month   \n",
       "0               1            1    42899542              85        425000  \\\n",
       "1               1            1    61393910              27        135000   \n",
       "2               1            1    68202834              97       4850000   \n",
       "3               1            1    73552136              28        140000   \n",
       "4               1            1    74421745              30        150000   \n",
       "\n",
       "                                                name   \n",
       "0  Áo ba lỗ nam/ 3 lỗ nam cao cấp, sợi cotton mềm...  \\\n",
       "1  Siêu giảm giá. Áo thun wash Nhật Bản thương hi...   \n",
       "2  Combo 2 áo ba lỗ nam, áo sát nách nam,sợi cott...   \n",
       "3          Áo thun nam tay lỡ form rộng Stay Real 69   \n",
       "4  Áo One Piece Zoro ATT-02, Áo thun Anime Manga ...   \n",
       "\n",
       "                                   short_description  categories_id   \n",
       "0   CHI TIẾT SẢN PHẨM. Áo Ba lỗ Tanktop MRM kiểu ...          10378  \\\n",
       "1  Thông số áo thun (áo phông) wash GOKINGForm áo...           1685   \n",
       "2     KIỂU DÁNG: SLIM FITCHI TIẾT:- Áo Tanktop sá...          10378   \n",
       "3   Áo thun nam tay lỡ form rộng Stay Real 69Colo...           1685   \n",
       "4  Áo One Piece Zoro ATT-02, áo thun One Piece Ro...           1685   \n",
       "\n",
       "                 categories_name  item_cnt_month_lag_1  item_cnt_month_lag_2   \n",
       "0              Áo thun nam ba lỗ                   NaN                   NaN  \\\n",
       "1  Áo thun nam ngắn tay không cổ                   NaN                   NaN   \n",
       "2              Áo thun nam ba lỗ                   NaN                   NaN   \n",
       "3  Áo thun nam ngắn tay không cổ                   NaN                   NaN   \n",
       "4  Áo thun nam ngắn tay không cổ                   NaN                   NaN   \n",
       "\n",
       "   item_cnt_month_lag_3  item_cnt_month_lag_6  product_avg_sale_last_6   \n",
       "0                   NaN                   NaN                      0.0  \\\n",
       "1                   NaN                   NaN                      0.0   \n",
       "2                   NaN                   NaN                      0.0   \n",
       "3                   NaN                   NaN                      0.0   \n",
       "4                   NaN                   NaN                      0.0   \n",
       "\n",
       "   product_std_sale_last_6  product_avg_sale_last_12   \n",
       "0                      0.0                       0.0  \\\n",
       "1                      0.0                       0.0   \n",
       "2                      0.0                       0.0   \n",
       "3                      0.0                       0.0   \n",
       "4                      0.0                       0.0   \n",
       "\n",
       "   product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "0                       0.0                       0.0  \\\n",
       "1                       0.0                       0.0   \n",
       "2                       0.0                       0.0   \n",
       "3                       0.0                       0.0   \n",
       "4                       0.0                       0.0   \n",
       "\n",
       "   campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "0                       0.0                        0.0  \\\n",
       "1                       0.0                        0.0   \n",
       "2                       0.0                        0.0   \n",
       "3                       0.0                        0.0   \n",
       "4                       0.0                        0.0   \n",
       "\n",
       "   campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "0                        0.0                        0.0  \\\n",
       "1                        0.0                        0.0   \n",
       "2                        0.0                        0.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "0                        0.0                      NaN  \\\n",
       "1                        0.0                      NaN   \n",
       "2                        0.0                      NaN   \n",
       "3                        0.0                      NaN   \n",
       "4                        0.0                      NaN   \n",
       "\n",
       "   date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "0                               NaN                               NaN  \\\n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "3                               NaN                               NaN   \n",
       "4                               NaN                               NaN   \n",
       "\n",
       "   date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "0                               NaN                               NaN  \\\n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "3                               NaN                               NaN   \n",
       "4                               NaN                               NaN   \n",
       "\n",
       "   date_item_avg_item_cnt_lag_1  date_item_avg_item_cnt_lag_2   \n",
       "0                           NaN                           NaN  \\\n",
       "1                           NaN                           NaN   \n",
       "2                           NaN                           NaN   \n",
       "3                           NaN                           NaN   \n",
       "4                           NaN                           NaN   \n",
       "\n",
       "   date_item_avg_item_cnt_lag_3  date_item_avg_item_cnt_lag_6   \n",
       "0                           NaN                           NaN  \\\n",
       "1                           NaN                           NaN   \n",
       "2                           NaN                           NaN   \n",
       "3                           NaN                           NaN   \n",
       "4                           NaN                           NaN   \n",
       "\n",
       "   date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag  delta_revenue_lag_1   \n",
       "0                                   NaN              0.0                  NaN  \\\n",
       "1                                   NaN              0.0                  NaN   \n",
       "2                                   NaN              0.0                  NaN   \n",
       "3                                   NaN              0.0                  NaN   \n",
       "4                                   NaN              0.0                  NaN   \n",
       "\n",
       "   month  year  item_last_sale  item_campaign_first_sale  item_first_sale  \n",
       "0      1     0             NaN                         0                0  \n",
       "1      1     0             NaN                         0                0  \n",
       "2      1     0             NaN                         0                0  \n",
       "3      1     0             NaN                         0                0  \n",
       "4      1     0             NaN                         0                0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xóa hai cột 'date_shop_avg_item_cnt_lag_12' và 'item_campaign_last_sale' từ DataFrame\n",
    "data.drop(['item_campaign_last_sale','item_cnt_month_lag_12','date_campaign_avg_item_cnt_lag_12','date_item_avg_item_cnt_lag_12'], axis=1, inplace=True)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e175a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng giá trị của cột 'date_block_num':\n",
      "date_block_num\n",
      "1     4179\n",
      "2     4179\n",
      "21    4179\n",
      "22    4179\n",
      "23    4179\n",
      "24    4179\n",
      "25    4179\n",
      "26    4179\n",
      "27    4179\n",
      "28    4179\n",
      "29    4179\n",
      "30    4179\n",
      "31    4179\n",
      "32    4179\n",
      "33    4179\n",
      "34    4179\n",
      "35    4179\n",
      "20    4179\n",
      "19    4179\n",
      "18    4179\n",
      "9     4179\n",
      "3     4179\n",
      "4     4179\n",
      "5     4179\n",
      "6     4179\n",
      "7     4179\n",
      "8     4179\n",
      "10    4179\n",
      "17    4179\n",
      "11    4179\n",
      "12    4179\n",
      "13    4179\n",
      "14    4179\n",
      "15    4179\n",
      "16    4179\n",
      "36    4179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'campaign_id':\n",
      "campaign_id\n",
      "22    11556\n",
      "24    11556\n",
      "23    11556\n",
      "1      4104\n",
      "2      4104\n",
      "3      4104\n",
      "45     3672\n",
      "43     3672\n",
      "44     3672\n",
      "8      3564\n",
      "9      3564\n",
      "7      3564\n",
      "4      3420\n",
      "5      3420\n",
      "6      3420\n",
      "41     3312\n",
      "40     3312\n",
      "12     3312\n",
      "11     3312\n",
      "10     3312\n",
      "42     3312\n",
      "33     3096\n",
      "32     3096\n",
      "31     3096\n",
      "27     3024\n",
      "25     3024\n",
      "26     3024\n",
      "37     2952\n",
      "39     2952\n",
      "38     2952\n",
      "34     2772\n",
      "35     2772\n",
      "36     2772\n",
      "48     2592\n",
      "47     2592\n",
      "46     2592\n",
      "17     1404\n",
      "16     1404\n",
      "18     1404\n",
      "29     1008\n",
      "28     1008\n",
      "30     1008\n",
      "20      216\n",
      "19      216\n",
      "21      216\n",
      "15      144\n",
      "14      144\n",
      "13      144\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'product_id':\n",
      "product_id\n",
      "42899542     108\n",
      "155701266    108\n",
      "167012865    108\n",
      "167002852    108\n",
      "164483934    108\n",
      "            ... \n",
      "7823134      108\n",
      "7599073      108\n",
      "7189521      108\n",
      "7170169      108\n",
      "274070205    108\n",
      "Name: count, Length: 1393, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_cnt_month':\n",
      "item_cnt_month\n",
      "31     8817\n",
      "30     8694\n",
      "32     8267\n",
      "29     8195\n",
      "28     7635\n",
      "33     7454\n",
      "27     6845\n",
      "34     6586\n",
      "35     5581\n",
      "26     5484\n",
      "25     4443\n",
      "36     4179\n",
      "37     3376\n",
      "24     3316\n",
      "38     2413\n",
      "23     2349\n",
      "39     1737\n",
      "22     1654\n",
      "40     1242\n",
      "60     1173\n",
      "63     1150\n",
      "58     1144\n",
      "61     1142\n",
      "59     1121\n",
      "62     1118\n",
      "21     1096\n",
      "57     1045\n",
      "64     1029\n",
      "56      995\n",
      "65      982\n",
      "55      930\n",
      "66      892\n",
      "54      873\n",
      "67      871\n",
      "41      770\n",
      "68      764\n",
      "53      761\n",
      "52      685\n",
      "69      666\n",
      "20      661\n",
      "51      628\n",
      "50      613\n",
      "42      559\n",
      "70      529\n",
      "71      512\n",
      "49      497\n",
      "48      486\n",
      "134     462\n",
      "43      453\n",
      "138     428\n",
      "139     413\n",
      "127     410\n",
      "128     410\n",
      "132     409\n",
      "137     408\n",
      "136     406\n",
      "141     404\n",
      "133     401\n",
      "130     400\n",
      "46      399\n",
      "131     397\n",
      "47      396\n",
      "142     395\n",
      "129     394\n",
      "44      392\n",
      "140     387\n",
      "135     386\n",
      "19      378\n",
      "126     373\n",
      "72      371\n",
      "45      358\n",
      "125     350\n",
      "124     347\n",
      "144     341\n",
      "145     334\n",
      "143     333\n",
      "123     326\n",
      "148     323\n",
      "146     323\n",
      "121     316\n",
      "119     313\n",
      "73      309\n",
      "122     306\n",
      "120     306\n",
      "147     304\n",
      "74      285\n",
      "150     265\n",
      "118     265\n",
      "151     264\n",
      "149     256\n",
      "154     253\n",
      "153     242\n",
      "117     237\n",
      "116     235\n",
      "94      225\n",
      "91      219\n",
      "152     218\n",
      "75      216\n",
      "155     215\n",
      "93      211\n",
      "87      208\n",
      "18      208\n",
      "113     208\n",
      "115     207\n",
      "114     206\n",
      "96      204\n",
      "90      193\n",
      "76      191\n",
      "89      188\n",
      "156     186\n",
      "92      185\n",
      "105     182\n",
      "85      182\n",
      "88      181\n",
      "112     181\n",
      "95      178\n",
      "158     174\n",
      "111     173\n",
      "97      170\n",
      "99      169\n",
      "159     165\n",
      "160     164\n",
      "77      164\n",
      "108     164\n",
      "98      164\n",
      "100     160\n",
      "86      159\n",
      "110     151\n",
      "157     150\n",
      "101     150\n",
      "109     148\n",
      "107     141\n",
      "84      140\n",
      "161     140\n",
      "103     138\n",
      "102     137\n",
      "78      132\n",
      "82      130\n",
      "79      128\n",
      "80      127\n",
      "81      126\n",
      "104     126\n",
      "106     125\n",
      "162     120\n",
      "17      114\n",
      "83      105\n",
      "165     100\n",
      "166      92\n",
      "163      90\n",
      "164      81\n",
      "167      77\n",
      "168      72\n",
      "169      65\n",
      "170      63\n",
      "16       56\n",
      "171      48\n",
      "174      46\n",
      "173      41\n",
      "172      41\n",
      "177      33\n",
      "176      32\n",
      "175      31\n",
      "181      29\n",
      "178      28\n",
      "180      28\n",
      "262      27\n",
      "275      25\n",
      "179      24\n",
      "15       22\n",
      "296      22\n",
      "266      22\n",
      "269      21\n",
      "274      21\n",
      "260      21\n",
      "281      21\n",
      "289      21\n",
      "277      20\n",
      "278      19\n",
      "264      19\n",
      "268      19\n",
      "188      19\n",
      "283      18\n",
      "185      18\n",
      "272      18\n",
      "187      18\n",
      "254      18\n",
      "276      17\n",
      "271      17\n",
      "265      17\n",
      "256      17\n",
      "261      17\n",
      "273      17\n",
      "267      17\n",
      "184      16\n",
      "182      16\n",
      "183      16\n",
      "259      16\n",
      "186      16\n",
      "290      16\n",
      "405      16\n",
      "253      16\n",
      "251      16\n",
      "263      15\n",
      "300      15\n",
      "191      15\n",
      "196      15\n",
      "258      15\n",
      "294      15\n",
      "14       14\n",
      "285      14\n",
      "435      14\n",
      "295      14\n",
      "413      13\n",
      "255      13\n",
      "282      13\n",
      "298      13\n",
      "395      13\n",
      "396      13\n",
      "280      13\n",
      "189      13\n",
      "392      13\n",
      "305      13\n",
      "194      13\n",
      "279      13\n",
      "387      13\n",
      "297      13\n",
      "257      12\n",
      "286      12\n",
      "420      12\n",
      "193      12\n",
      "417      11\n",
      "406      11\n",
      "426      11\n",
      "403      11\n",
      "246      11\n",
      "415      11\n",
      "288      11\n",
      "301      11\n",
      "284      11\n",
      "287      11\n",
      "421      11\n",
      "241      11\n",
      "250      10\n",
      "432      10\n",
      "303      10\n",
      "414      10\n",
      "252      10\n",
      "408      10\n",
      "419      10\n",
      "394      10\n",
      "292      10\n",
      "434      10\n",
      "249      10\n",
      "411      10\n",
      "248       9\n",
      "429       9\n",
      "299       9\n",
      "430       9\n",
      "291       9\n",
      "399       9\n",
      "433       9\n",
      "424       9\n",
      "192       9\n",
      "428       9\n",
      "397       9\n",
      "436       9\n",
      "400       9\n",
      "427       9\n",
      "195       9\n",
      "307       9\n",
      "418       9\n",
      "190       9\n",
      "409       9\n",
      "308       9\n",
      "380       9\n",
      "197       9\n",
      "237       8\n",
      "447       8\n",
      "312       8\n",
      "239       8\n",
      "443       8\n",
      "416       8\n",
      "247       8\n",
      "404       8\n",
      "398       8\n",
      "306       8\n",
      "309       8\n",
      "441       8\n",
      "270       8\n",
      "242       8\n",
      "243       8\n",
      "236       8\n",
      "234       7\n",
      "389       7\n",
      "425       7\n",
      "388       7\n",
      "384       7\n",
      "229       7\n",
      "376       7\n",
      "293       7\n",
      "369       7\n",
      "438       7\n",
      "381       7\n",
      "198       7\n",
      "302       7\n",
      "374       7\n",
      "200       6\n",
      "407       6\n",
      "445       6\n",
      "423       6\n",
      "385       6\n",
      "383       6\n",
      "386       6\n",
      "238       6\n",
      "203       6\n",
      "375       6\n",
      "313       6\n",
      "245       6\n",
      "401       6\n",
      "378       6\n",
      "321       6\n",
      "221       6\n",
      "317       6\n",
      "454       5\n",
      "464       5\n",
      "207       5\n",
      "199       5\n",
      "440       5\n",
      "201       5\n",
      "448       5\n",
      "304       5\n",
      "451       5\n",
      "240       5\n",
      "439       5\n",
      "371       5\n",
      "230       5\n",
      "222       5\n",
      "13        5\n",
      "226       5\n",
      "402       5\n",
      "412       5\n",
      "205       5\n",
      "422       5\n",
      "449       5\n",
      "372       5\n",
      "379       5\n",
      "458       4\n",
      "393       4\n",
      "206       4\n",
      "235       4\n",
      "211       4\n",
      "214       4\n",
      "355       4\n",
      "437       4\n",
      "431       4\n",
      "459       4\n",
      "341       4\n",
      "244       4\n",
      "202       4\n",
      "310       4\n",
      "366       4\n",
      "391       4\n",
      "233       4\n",
      "455       4\n",
      "358       4\n",
      "350       3\n",
      "349       3\n",
      "377       3\n",
      "356       3\n",
      "328       3\n",
      "314       3\n",
      "216       3\n",
      "410       3\n",
      "311       3\n",
      "227       3\n",
      "204       3\n",
      "318       3\n",
      "470       3\n",
      "478       3\n",
      "220       3\n",
      "461       3\n",
      "363       3\n",
      "354       3\n",
      "322       3\n",
      "351       3\n",
      "346       3\n",
      "456       3\n",
      "232       3\n",
      "209       3\n",
      "450       3\n",
      "367       3\n",
      "323       2\n",
      "210       2\n",
      "223       2\n",
      "217       2\n",
      "472       2\n",
      "326       2\n",
      "315       2\n",
      "390       2\n",
      "316       2\n",
      "353       2\n",
      "489       2\n",
      "338       2\n",
      "231       2\n",
      "343       2\n",
      "357       2\n",
      "365       2\n",
      "335       2\n",
      "452       2\n",
      "352       2\n",
      "477       2\n",
      "359       2\n",
      "465       2\n",
      "337       2\n",
      "469       2\n",
      "460       2\n",
      "442       2\n",
      "218       2\n",
      "347       2\n",
      "444       2\n",
      "463       2\n",
      "225       2\n",
      "476       2\n",
      "208       2\n",
      "319       2\n",
      "482       2\n",
      "462       2\n",
      "324       1\n",
      "342       1\n",
      "360       1\n",
      "320       1\n",
      "368       1\n",
      "325       1\n",
      "336       1\n",
      "492       1\n",
      "494       1\n",
      "466       1\n",
      "468       1\n",
      "467       1\n",
      "471       1\n",
      "344       1\n",
      "361       1\n",
      "491       1\n",
      "12        1\n",
      "370       1\n",
      "228       1\n",
      "329       1\n",
      "340       1\n",
      "330       1\n",
      "474       1\n",
      "364       1\n",
      "11        1\n",
      "327       1\n",
      "332       1\n",
      "457       1\n",
      "362       1\n",
      "224       1\n",
      "212       1\n",
      "483       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'profit_month':\n",
      "profit_month\n",
      "1550000     5995\n",
      "1500000     5866\n",
      "1600000     5634\n",
      "1450000     5599\n",
      "1400000     5174\n",
      "            ... \n",
      "23000000       1\n",
      "4945000        1\n",
      "2820000        1\n",
      "4545000        1\n",
      "10850000       1\n",
      "Name: count, Length: 1317, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'name':\n",
      "name\n",
      "Quần bò dài nam trung niên dáng thụng có đến size 36+ẢNH THẬT                                               216\n",
      "Áo thun cộc tay nam trung niên chất mát+ ẢNH THẬT                                                           216\n",
      "Giày Sandal Nam TiTi ĐÔ Da Bò Thật Cao Cấp DNA2137a                                                         216\n",
      "Áo thun nam tay dài mặc thu đông, giữ nhiệt, chất cotton co giãn, áo dài tay dáng ôm cao cấp                216\n",
      "Áo Len Nam nhiều  Màu cổ ba phân Trơn Phong Cách Trẻ Trung Lịch Lãm được nhập khẩu bởi công ty XUÂN ANH     216\n",
      "                                                                                                           ... \n",
      "Combo 3 Quần Sịp Đúc Cao Cấp Nhật Bản (Mầu ngẫu nhiên)                                                      108\n",
      "Quần kaki nam sống suông thời trang VICERO                                                                  108\n",
      "Combo 4 Quần Sịp Đùi Boxer  Thông Hơi Thoáng Mát - Quần Lót Nam (Giao Màu Ngẫu Nhiên)                       108\n",
      "Combo 10 Quần Lót Nam nhiều lưng vải cotton 2 chiều hiệu CITYMEN, màu ngẫu nhiên, sịp nam - LMTK-MULTI2C    108\n",
      "Giày Boots Nam GEOX U Portello A                                                                            108\n",
      "Name: count, Length: 1386, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'short_description':\n",
      "short_description\n",
      "...                                                                                                                                                                                                           2592\n",
      "NAM NỮ COUPLE ĐỀU MẶC ĐƯỢC, HÀNG BÁN SHOP STORE DÀY DẶN HÌNH THẬT (ĐỪNG SO SÁNH VỚI HÀNG GIÁ RẺ NHÉ KHÁCH ƠI!!!)* Chất liệu: Thun Nỉ cao cấp mềm mịn, co giản tốt, from chuẩn, không xù, thấm hút mồ hô...    1188\n",
      "CHẤT LIỆU SẢN XUẤT:- Thân giày: da bò Nappa/ da bò Top - grain/ da bò bề mặt độ dày tiêu chuẩn 1.8mm. - Đế giày: cao su TPR cao cấp xẻ rãnh chống trượt.- Kiểu dáng giày Dress Shoes cao cấp chuẩn châ...      972\n",
      "GIỚI THIỆU THƯƠNG HIỆULà thương hiệu thời trang thiết kế nam với những sản phẩm chất lượng cao, giá thành phù hợp, Aligro nâng tầm phong cách quý ông qua những thiết kế hiện đại lịch lãm, phong cách ...     864\n",
      "MÔ TẢ SẢN PHẨM:Biến hóa linh động với thiết kế mới nhất từ Sandals Pierre Cardin. Lấy cảm hứng từ mẫu Sandals quai đôi truyền thống, phối hợp quai giữa với họa tiết dập nổi giả khóa bền chắc và sành ...     540\n",
      "                                                                                                                                                                                                              ... \n",
      " CHI TIẾT SẢN PHẨM:–  Quần Boxer cotton cạp 2.5cm MR.M thiết kế hiện đại, thể thao hướng tới người đàn ông bất khả chiến bại.–  Logo dệt nổi bật tại chính giữa cạp quần tạo nét nam tính, khỏe khoắ...        108\n",
      "CHI TIẾT SẢN PHẨM:–  Quần Boxer Organic cạp 2.5cm MR.M thiết kế hiện đại, thể thao hướng tới người đàn ông bất khả chiến bại.–  Logo dệt nổi bật tại chính giữa cạp quần tạo nét nam tính, khỏe khoắn....      108\n",
      "THÔNG TIN SẢN PHẨM Chất liệu vải Jean co giãn nhẹ giúp thoải mái trong mọi hoạt động, vải mềm mịn, ít nhăn, ít xù lông, giữ dáng không bai. Quần Jean màu bạc, tạo cho chiếc quần jean một phom dáng đ...      108\n",
      "Quần bơi nam Với gam màu tối đen trầm và xanh nước tạo nên nét sexy riêng cho các bạn nam.Thiết kế vây cá mập đơn giản nhưng vẫn cá tính phù hợp với nhiều lứa tuổi dáng người. Chất liệu polyester k...       108\n",
      "• Giày boots Portello với thiết kế siêu nhẹ, đơn giản và cực kỳ thoải mái dành cho nam. Phần trên giày được thiết kế từ da lộn kết hợp với chất liệu vải co giãn trong gam màu xanh đậm trung tín...           108\n",
      "Name: count, Length: 1231, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'categories_id':\n",
      "categories_id\n",
      "49512    9072\n",
      "49634    8748\n",
      "49620    8208\n",
      "49540    7668\n",
      "4555     6372\n",
      "49626    6372\n",
      "49458    5616\n",
      "67324    5616\n",
      "10384    5184\n",
      "67326    5184\n",
      "1685     4428\n",
      "1686     4320\n",
      "49640    4320\n",
      "27554    4212\n",
      "49630    4212\n",
      "49632    3672\n",
      "49446    3672\n",
      "67252    3564\n",
      "67250    3456\n",
      "49642    3456\n",
      "5333     3348\n",
      "49496    3348\n",
      "67248    3132\n",
      "2269     2700\n",
      "49500    2592\n",
      "67247    2484\n",
      "49448    2376\n",
      "49530    1512\n",
      "922      1512\n",
      "49452    1404\n",
      "10383    1296\n",
      "49450    1296\n",
      "49622    1080\n",
      "49636    1080\n",
      "5409      972\n",
      "49624     972\n",
      "49628     972\n",
      "1581      972\n",
      "49444     864\n",
      "49514     864\n",
      "49464     756\n",
      "27566     648\n",
      "67249     648\n",
      "67251     648\n",
      "67244     540\n",
      "10378     540\n",
      "22900     432\n",
      "49528     432\n",
      "8336      432\n",
      "49456     432\n",
      "49506     324\n",
      "49460     324\n",
      "49520     324\n",
      "49508     324\n",
      "4556      216\n",
      "67327     216\n",
      "49638     216\n",
      "67325     216\n",
      "67298     108\n",
      "49504     108\n",
      "5341      108\n",
      "49498     108\n",
      "49536     108\n",
      "10381     108\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'categories_name':\n",
      "categories_name\n",
      "Áo hoodies nam vải nỉ             9072\n",
      "Giày sandals nam quai ngang       8748\n",
      "Giày thể thao nam cổ thấp         8208\n",
      "Quần bơi - đi biển nam            7668\n",
      "Đồ mặc nhà nam - Bộ ngắn          6372\n",
      "Giày lười da nam                  6372\n",
      "Áo khoác gió                      5616\n",
      "Quần lót boxer nam                5616\n",
      "Dép nam quai ngang                5184\n",
      "Quần lót kiểu brief nam           5184\n",
      "Áo thun nam ngắn tay không cổ     4428\n",
      "Giày - Dép nam                    4320\n",
      "Giày boots nam cổ cao             4320\n",
      "Áo len dệt kim                    4212\n",
      "Giày tây nam có dây               4212\n",
      "Giày tây nam không dây            3672\n",
      "Áo sơ mi nam tay dài trơn         3672\n",
      "Quần dài nam trung niên           3564\n",
      "Áo thun nam trung niên            3456\n",
      "Giày boots nam cổ thấp            3456\n",
      "Áo thun nam ngắn tay có cổ        3348\n",
      "Quần jeans nam                    3348\n",
      "Áo khoác nam trung niên           3132\n",
      "Đồ mặc nhà nam - Bộ dài           2700\n",
      "Quần kaki nam dài                 2592\n",
      "Bộ đồ gia đình cho nam            2484\n",
      "Áo sơ mi nam tay dài họa tiết     2376\n",
      "Bộ quần áo nam kích cỡ lớn        1512\n",
      "Quần tây nam                      1512\n",
      "Áo sơ mi nam tay ngắn họa tiết    1404\n",
      "Dép nam xỏ ngón                   1296\n",
      "Áo sơ mi nam tay ngắn trơn        1296\n",
      "Giày thể thao nam cổ cao          1080\n",
      "Giày sandals nam quai chéo        1080\n",
      "Quần jogger nam                    972\n",
      "Giày lười vải nam                  972\n",
      "Giày lười nhựa nam                 972\n",
      "Giày tây nam                       972\n",
      "Áo sơ mi nam cổ tàu                864\n",
      "Áo hoodies nam vải cotton          864\n",
      "Áo khoác bomber nam                756\n",
      "Áo vest và Blazer nam              648\n",
      "Áo sơ mi nam trung niên            648\n",
      "Quần short nam trung niên          648\n",
      "Bộ đồ đôi cho nam                  540\n",
      "Áo thun nam ba lỗ                  540\n",
      "Áo nỉ nam                          432\n",
      "Quần nam kích cỡ lớn               432\n",
      "Áo thun nam dài tay không cổ       432\n",
      "Áo khoác da nam                    432\n",
      "Quần thun nam                      324\n",
      "Áo khoác nỉ                        324\n",
      "Quần đôi cho nam                   324\n",
      "Áo khoác jeans nam                 324\n",
      "Quần lót nam                       216\n",
      "Quần lót nam khác                  216\n",
      "Dép nam đi trong nhà               216\n",
      "Quần lót dây nam                   216\n",
      "Áo thun nam dài tay có cổ          108\n",
      "Quần short thun nam                108\n",
      "Giày sandals nam                   108\n",
      "Quần short kaki nam                108\n",
      "Quần nam trung niên                108\n",
      "Áo khoác phao nam                  108\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_cnt_month_lag_1':\n",
      "item_cnt_month_lag_1\n",
      "31.0     8548\n",
      "30.0     8482\n",
      "32.0     8047\n",
      "29.0     8012\n",
      "28.0     7458\n",
      "33.0     7228\n",
      "27.0     6670\n",
      "34.0     6382\n",
      "35.0     5414\n",
      "26.0     5370\n",
      "25.0     4354\n",
      "36.0     4060\n",
      "37.0     3271\n",
      "24.0     3242\n",
      "38.0     2343\n",
      "23.0     2306\n",
      "39.0     1701\n",
      "22.0     1611\n",
      "40.0     1205\n",
      "60.0     1138\n",
      "63.0     1122\n",
      "61.0     1115\n",
      "58.0     1114\n",
      "62.0     1089\n",
      "59.0     1088\n",
      "21.0     1071\n",
      "57.0     1023\n",
      "64.0      999\n",
      "56.0      975\n",
      "65.0      948\n",
      "55.0      916\n",
      "66.0      865\n",
      "54.0      859\n",
      "67.0      847\n",
      "53.0      748\n",
      "68.0      743\n",
      "41.0      737\n",
      "52.0      665\n",
      "69.0      646\n",
      "20.0      646\n",
      "51.0      618\n",
      "50.0      596\n",
      "42.0      539\n",
      "70.0      504\n",
      "71.0      498\n",
      "49.0      482\n",
      "48.0      477\n",
      "134.0     442\n",
      "43.0      436\n",
      "138.0     411\n",
      "128.0     401\n",
      "132.0     397\n",
      "127.0     397\n",
      "137.0     395\n",
      "46.0      394\n",
      "139.0     390\n",
      "136.0     389\n",
      "141.0     388\n",
      "130.0     388\n",
      "131.0     386\n",
      "129.0     384\n",
      "47.0      383\n",
      "133.0     381\n",
      "44.0      380\n",
      "142.0     379\n",
      "19.0      375\n",
      "140.0     368\n",
      "135.0     366\n",
      "126.0     360\n",
      "72.0      358\n",
      "45.0      354\n",
      "124.0     343\n",
      "125.0     341\n",
      "144.0     327\n",
      "145.0     319\n",
      "123.0     318\n",
      "143.0     315\n",
      "146.0     313\n",
      "148.0     311\n",
      "121.0     308\n",
      "119.0     302\n",
      "73.0      300\n",
      "120.0     297\n",
      "122.0     295\n",
      "147.0     291\n",
      "74.0      274\n",
      "118.0     258\n",
      "150.0     256\n",
      "151.0     252\n",
      "154.0     243\n",
      "149.0     241\n",
      "117.0     229\n",
      "153.0     227\n",
      "116.0     226\n",
      "94.0      222\n",
      "91.0      213\n",
      "152.0     211\n",
      "75.0      210\n",
      "155.0     209\n",
      "18.0      206\n",
      "87.0      204\n",
      "93.0      203\n",
      "113.0     199\n",
      "114.0     199\n",
      "115.0     198\n",
      "96.0      194\n",
      "90.0      186\n",
      "89.0      184\n",
      "76.0      183\n",
      "92.0      179\n",
      "105.0     179\n",
      "88.0      177\n",
      "85.0      175\n",
      "95.0      173\n",
      "156.0     172\n",
      "112.0     172\n",
      "97.0      169\n",
      "158.0     169\n",
      "99.0      168\n",
      "111.0     168\n",
      "108.0     163\n",
      "159.0     161\n",
      "77.0      160\n",
      "98.0      156\n",
      "86.0      155\n",
      "160.0     155\n",
      "100.0     149\n",
      "110.0     147\n",
      "109.0     145\n",
      "101.0     145\n",
      "157.0     144\n",
      "84.0      137\n",
      "161.0     136\n",
      "107.0     134\n",
      "103.0     134\n",
      "102.0     134\n",
      "78.0      129\n",
      "81.0      125\n",
      "82.0      123\n",
      "79.0      123\n",
      "104.0     122\n",
      "106.0     121\n",
      "80.0      120\n",
      "162.0     115\n",
      "17.0      113\n",
      "83.0      104\n",
      "165.0      96\n",
      "163.0      85\n",
      "166.0      84\n",
      "164.0      80\n",
      "167.0      74\n",
      "168.0      71\n",
      "169.0      64\n",
      "170.0      57\n",
      "16.0       55\n",
      "171.0      48\n",
      "174.0      40\n",
      "172.0      39\n",
      "173.0      39\n",
      "177.0      31\n",
      "176.0      31\n",
      "175.0      30\n",
      "178.0      28\n",
      "180.0      27\n",
      "181.0      27\n",
      "275.0      25\n",
      "262.0      24\n",
      "179.0      23\n",
      "15.0       22\n",
      "281.0      21\n",
      "296.0      21\n",
      "277.0      20\n",
      "266.0      20\n",
      "289.0      20\n",
      "274.0      20\n",
      "269.0      19\n",
      "188.0      19\n",
      "260.0      19\n",
      "268.0      18\n",
      "278.0      18\n",
      "187.0      18\n",
      "272.0      18\n",
      "283.0      18\n",
      "185.0      17\n",
      "264.0      17\n",
      "186.0      16\n",
      "183.0      16\n",
      "182.0      16\n",
      "184.0      16\n",
      "273.0      16\n",
      "267.0      16\n",
      "251.0      16\n",
      "271.0      16\n",
      "405.0      16\n",
      "265.0      16\n",
      "294.0      15\n",
      "253.0      15\n",
      "290.0      15\n",
      "258.0      15\n",
      "191.0      15\n",
      "263.0      15\n",
      "276.0      15\n",
      "254.0      15\n",
      "256.0      15\n",
      "196.0      15\n",
      "435.0      14\n",
      "295.0      14\n",
      "14.0       14\n",
      "261.0      14\n",
      "300.0      14\n",
      "259.0      14\n",
      "305.0      13\n",
      "387.0      13\n",
      "285.0      13\n",
      "279.0      13\n",
      "396.0      13\n",
      "298.0      13\n",
      "282.0      13\n",
      "413.0      13\n",
      "193.0      12\n",
      "189.0      12\n",
      "255.0      12\n",
      "392.0      12\n",
      "280.0      12\n",
      "406.0      11\n",
      "287.0      11\n",
      "194.0      11\n",
      "246.0      11\n",
      "395.0      11\n",
      "286.0      11\n",
      "421.0      11\n",
      "426.0      11\n",
      "415.0      11\n",
      "420.0      11\n",
      "417.0      11\n",
      "403.0      11\n",
      "250.0      10\n",
      "414.0      10\n",
      "303.0      10\n",
      "394.0      10\n",
      "434.0      10\n",
      "249.0      10\n",
      "419.0      10\n",
      "241.0      10\n",
      "257.0      10\n",
      "288.0      10\n",
      "190.0       9\n",
      "197.0       9\n",
      "432.0       9\n",
      "297.0       9\n",
      "408.0       9\n",
      "397.0       9\n",
      "400.0       9\n",
      "248.0       9\n",
      "409.0       9\n",
      "380.0       9\n",
      "284.0       9\n",
      "252.0       9\n",
      "430.0       9\n",
      "427.0       9\n",
      "301.0       9\n",
      "308.0       9\n",
      "428.0       9\n",
      "424.0       9\n",
      "433.0       9\n",
      "429.0       9\n",
      "418.0       9\n",
      "399.0       9\n",
      "411.0       9\n",
      "441.0       8\n",
      "312.0       8\n",
      "270.0       8\n",
      "247.0       8\n",
      "447.0       8\n",
      "239.0       8\n",
      "237.0       8\n",
      "307.0       8\n",
      "404.0       8\n",
      "236.0       8\n",
      "436.0       8\n",
      "299.0       8\n",
      "443.0       8\n",
      "195.0       8\n",
      "243.0       8\n",
      "292.0       8\n",
      "398.0       8\n",
      "192.0       7\n",
      "293.0       7\n",
      "425.0       7\n",
      "291.0       7\n",
      "234.0       7\n",
      "389.0       7\n",
      "438.0       7\n",
      "306.0       7\n",
      "416.0       7\n",
      "302.0       7\n",
      "388.0       7\n",
      "376.0       7\n",
      "229.0       7\n",
      "369.0       7\n",
      "374.0       7\n",
      "198.0       7\n",
      "381.0       7\n",
      "401.0       6\n",
      "445.0       6\n",
      "238.0       6\n",
      "378.0       6\n",
      "203.0       6\n",
      "385.0       6\n",
      "317.0       6\n",
      "423.0       6\n",
      "309.0       6\n",
      "321.0       6\n",
      "384.0       6\n",
      "386.0       6\n",
      "242.0       6\n",
      "200.0       6\n",
      "375.0       6\n",
      "407.0       6\n",
      "221.0       6\n",
      "13.0        5\n",
      "402.0       5\n",
      "205.0       5\n",
      "440.0       5\n",
      "240.0       5\n",
      "454.0       5\n",
      "199.0       5\n",
      "313.0       5\n",
      "226.0       5\n",
      "383.0       5\n",
      "230.0       5\n",
      "201.0       5\n",
      "245.0       5\n",
      "412.0       5\n",
      "371.0       5\n",
      "304.0       5\n",
      "464.0       5\n",
      "448.0       5\n",
      "372.0       5\n",
      "451.0       5\n",
      "222.0       5\n",
      "449.0       5\n",
      "379.0       5\n",
      "439.0       5\n",
      "422.0       5\n",
      "233.0       4\n",
      "355.0       4\n",
      "358.0       4\n",
      "431.0       4\n",
      "244.0       4\n",
      "366.0       4\n",
      "437.0       4\n",
      "459.0       4\n",
      "206.0       4\n",
      "458.0       4\n",
      "235.0       4\n",
      "341.0       4\n",
      "391.0       4\n",
      "455.0       4\n",
      "207.0       4\n",
      "393.0       4\n",
      "214.0       4\n",
      "202.0       3\n",
      "227.0       3\n",
      "478.0       3\n",
      "470.0       3\n",
      "377.0       3\n",
      "216.0       3\n",
      "450.0       3\n",
      "349.0       3\n",
      "410.0       3\n",
      "350.0       3\n",
      "204.0       3\n",
      "461.0       3\n",
      "232.0       3\n",
      "354.0       3\n",
      "211.0       3\n",
      "367.0       3\n",
      "346.0       3\n",
      "351.0       3\n",
      "209.0       3\n",
      "322.0       3\n",
      "356.0       3\n",
      "363.0       3\n",
      "310.0       3\n",
      "220.0       3\n",
      "456.0       3\n",
      "231.0       2\n",
      "223.0       2\n",
      "465.0       2\n",
      "311.0       2\n",
      "390.0       2\n",
      "323.0       2\n",
      "326.0       2\n",
      "476.0       2\n",
      "463.0       2\n",
      "335.0       2\n",
      "338.0       2\n",
      "442.0       2\n",
      "225.0       2\n",
      "208.0       2\n",
      "452.0       2\n",
      "352.0       2\n",
      "343.0       2\n",
      "489.0       2\n",
      "444.0       2\n",
      "469.0       2\n",
      "460.0       2\n",
      "462.0       2\n",
      "210.0       2\n",
      "477.0       2\n",
      "337.0       2\n",
      "365.0       2\n",
      "347.0       2\n",
      "359.0       2\n",
      "482.0       2\n",
      "218.0       2\n",
      "316.0       2\n",
      "315.0       2\n",
      "353.0       2\n",
      "314.0       2\n",
      "319.0       2\n",
      "357.0       2\n",
      "360.0       1\n",
      "361.0       1\n",
      "468.0       1\n",
      "368.0       1\n",
      "325.0       1\n",
      "471.0       1\n",
      "492.0       1\n",
      "324.0       1\n",
      "320.0       1\n",
      "466.0       1\n",
      "467.0       1\n",
      "342.0       1\n",
      "336.0       1\n",
      "494.0       1\n",
      "327.0       1\n",
      "212.0       1\n",
      "364.0       1\n",
      "217.0       1\n",
      "340.0       1\n",
      "318.0       1\n",
      "370.0       1\n",
      "491.0       1\n",
      "330.0       1\n",
      "344.0       1\n",
      "474.0       1\n",
      "11.0        1\n",
      "224.0       1\n",
      "12.0        1\n",
      "472.0       1\n",
      "332.0       1\n",
      "457.0       1\n",
      "328.0       1\n",
      "329.0       1\n",
      "228.0       1\n",
      "362.0       1\n",
      "483.0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_cnt_month_lag_2':\n",
      "item_cnt_month_lag_2\n",
      "31.0     8343\n",
      "30.0     8292\n",
      "32.0     7862\n",
      "29.0     7817\n",
      "28.0     7281\n",
      "33.0     7059\n",
      "27.0     6488\n",
      "34.0     6233\n",
      "35.0     5291\n",
      "26.0     5216\n",
      "25.0     4261\n",
      "36.0     3968\n",
      "37.0     3191\n",
      "24.0     3163\n",
      "38.0     2278\n",
      "23.0     2257\n",
      "39.0     1673\n",
      "22.0     1570\n",
      "40.0     1176\n",
      "60.0     1100\n",
      "63.0     1093\n",
      "61.0     1090\n",
      "58.0     1078\n",
      "62.0     1061\n",
      "59.0     1046\n",
      "21.0     1038\n",
      "57.0     1001\n",
      "64.0      970\n",
      "56.0      940\n",
      "65.0      919\n",
      "55.0      886\n",
      "66.0      839\n",
      "67.0      836\n",
      "54.0      825\n",
      "41.0      725\n",
      "53.0      724\n",
      "68.0      721\n",
      "52.0      644\n",
      "69.0      637\n",
      "20.0      627\n",
      "51.0      600\n",
      "50.0      581\n",
      "42.0      529\n",
      "70.0      490\n",
      "71.0      479\n",
      "49.0      468\n",
      "48.0      468\n",
      "43.0      425\n",
      "134.0     418\n",
      "138.0     389\n",
      "46.0      388\n",
      "127.0     378\n",
      "128.0     377\n",
      "47.0      374\n",
      "44.0      373\n",
      "137.0     373\n",
      "130.0     372\n",
      "132.0     371\n",
      "136.0     370\n",
      "129.0     368\n",
      "19.0      368\n",
      "131.0     367\n",
      "141.0     365\n",
      "133.0     361\n",
      "139.0     360\n",
      "142.0     360\n",
      "72.0      352\n",
      "140.0     346\n",
      "135.0     344\n",
      "45.0      342\n",
      "126.0     339\n",
      "124.0     332\n",
      "125.0     331\n",
      "144.0     311\n",
      "145.0     305\n",
      "123.0     299\n",
      "146.0     298\n",
      "143.0     295\n",
      "148.0     293\n",
      "121.0     291\n",
      "73.0      288\n",
      "119.0     276\n",
      "122.0     274\n",
      "120.0     274\n",
      "147.0     273\n",
      "74.0      268\n",
      "118.0     245\n",
      "150.0     242\n",
      "151.0     236\n",
      "154.0     227\n",
      "149.0     225\n",
      "94.0      217\n",
      "153.0     215\n",
      "116.0     210\n",
      "117.0     210\n",
      "91.0      207\n",
      "75.0      206\n",
      "152.0     203\n",
      "93.0      198\n",
      "18.0      198\n",
      "87.0      197\n",
      "155.0     192\n",
      "96.0      190\n",
      "113.0     188\n",
      "115.0     188\n",
      "90.0      184\n",
      "114.0     180\n",
      "76.0      179\n",
      "105.0     177\n",
      "89.0      177\n",
      "92.0      174\n",
      "88.0      171\n",
      "85.0      170\n",
      "95.0      168\n",
      "158.0     167\n",
      "97.0      166\n",
      "111.0     164\n",
      "99.0      163\n",
      "156.0     162\n",
      "112.0     161\n",
      "108.0     157\n",
      "159.0     155\n",
      "77.0      154\n",
      "98.0      151\n",
      "86.0      148\n",
      "160.0     147\n",
      "100.0     147\n",
      "101.0     143\n",
      "109.0     138\n",
      "110.0     137\n",
      "84.0      134\n",
      "161.0     133\n",
      "157.0     132\n",
      "107.0     131\n",
      "103.0     130\n",
      "102.0     128\n",
      "78.0      123\n",
      "81.0      122\n",
      "79.0      121\n",
      "82.0      121\n",
      "80.0      119\n",
      "106.0     114\n",
      "104.0     111\n",
      "17.0      108\n",
      "162.0     106\n",
      "83.0      100\n",
      "165.0      90\n",
      "163.0      81\n",
      "164.0      77\n",
      "166.0      76\n",
      "168.0      69\n",
      "167.0      68\n",
      "169.0      64\n",
      "170.0      56\n",
      "16.0       52\n",
      "171.0      47\n",
      "174.0      39\n",
      "173.0      38\n",
      "172.0      38\n",
      "177.0      30\n",
      "176.0      30\n",
      "175.0      29\n",
      "178.0      27\n",
      "180.0      26\n",
      "181.0      25\n",
      "275.0      24\n",
      "179.0      23\n",
      "262.0      22\n",
      "15.0       22\n",
      "281.0      21\n",
      "266.0      20\n",
      "296.0      19\n",
      "274.0      19\n",
      "188.0      19\n",
      "277.0      19\n",
      "289.0      18\n",
      "260.0      18\n",
      "269.0      18\n",
      "268.0      17\n",
      "187.0      17\n",
      "272.0      17\n",
      "278.0      17\n",
      "283.0      16\n",
      "251.0      16\n",
      "405.0      16\n",
      "265.0      16\n",
      "264.0      16\n",
      "186.0      16\n",
      "183.0      16\n",
      "185.0      16\n",
      "267.0      15\n",
      "196.0      15\n",
      "182.0      15\n",
      "290.0      15\n",
      "254.0      15\n",
      "256.0      15\n",
      "294.0      15\n",
      "276.0      15\n",
      "263.0      14\n",
      "191.0      14\n",
      "300.0      14\n",
      "253.0      14\n",
      "273.0      14\n",
      "271.0      14\n",
      "258.0      14\n",
      "295.0      13\n",
      "396.0      13\n",
      "435.0      13\n",
      "305.0      13\n",
      "285.0      13\n",
      "282.0      13\n",
      "14.0       13\n",
      "259.0      13\n",
      "413.0      13\n",
      "184.0      13\n",
      "298.0      13\n",
      "261.0      13\n",
      "279.0      12\n",
      "189.0      12\n",
      "387.0      12\n",
      "255.0      12\n",
      "193.0      12\n",
      "420.0      11\n",
      "280.0      11\n",
      "395.0      11\n",
      "426.0      11\n",
      "246.0      11\n",
      "287.0      11\n",
      "415.0      11\n",
      "406.0      11\n",
      "403.0      11\n",
      "392.0      10\n",
      "241.0      10\n",
      "250.0      10\n",
      "194.0      10\n",
      "419.0      10\n",
      "286.0      10\n",
      "421.0      10\n",
      "394.0      10\n",
      "430.0       9\n",
      "411.0       9\n",
      "288.0       9\n",
      "303.0       9\n",
      "284.0       9\n",
      "297.0       9\n",
      "427.0       9\n",
      "197.0       9\n",
      "190.0       9\n",
      "428.0       9\n",
      "418.0       9\n",
      "248.0       9\n",
      "433.0       9\n",
      "308.0       9\n",
      "400.0       9\n",
      "414.0       9\n",
      "429.0       9\n",
      "301.0       9\n",
      "257.0       9\n",
      "249.0       9\n",
      "252.0       9\n",
      "404.0       8\n",
      "307.0       8\n",
      "239.0       8\n",
      "409.0       8\n",
      "237.0       8\n",
      "397.0       8\n",
      "270.0       8\n",
      "417.0       8\n",
      "408.0       8\n",
      "247.0       8\n",
      "432.0       8\n",
      "443.0       8\n",
      "195.0       8\n",
      "380.0       8\n",
      "243.0       8\n",
      "292.0       8\n",
      "312.0       8\n",
      "299.0       8\n",
      "236.0       7\n",
      "291.0       7\n",
      "306.0       7\n",
      "229.0       7\n",
      "425.0       7\n",
      "198.0       7\n",
      "447.0       7\n",
      "376.0       7\n",
      "434.0       7\n",
      "302.0       7\n",
      "436.0       7\n",
      "388.0       7\n",
      "389.0       7\n",
      "424.0       7\n",
      "374.0       7\n",
      "398.0       7\n",
      "438.0       7\n",
      "192.0       7\n",
      "399.0       7\n",
      "441.0       7\n",
      "234.0       6\n",
      "238.0       6\n",
      "378.0       6\n",
      "242.0       6\n",
      "416.0       6\n",
      "200.0       6\n",
      "384.0       6\n",
      "321.0       6\n",
      "369.0       6\n",
      "221.0       6\n",
      "203.0       6\n",
      "309.0       6\n",
      "445.0       6\n",
      "401.0       6\n",
      "386.0       6\n",
      "293.0       6\n",
      "454.0       5\n",
      "317.0       5\n",
      "313.0       5\n",
      "379.0       5\n",
      "440.0       5\n",
      "304.0       5\n",
      "199.0       5\n",
      "407.0       5\n",
      "451.0       5\n",
      "422.0       5\n",
      "381.0       5\n",
      "375.0       5\n",
      "371.0       5\n",
      "13.0        5\n",
      "385.0       5\n",
      "240.0       5\n",
      "383.0       5\n",
      "372.0       5\n",
      "245.0       5\n",
      "412.0       5\n",
      "423.0       5\n",
      "402.0       5\n",
      "230.0       5\n",
      "222.0       5\n",
      "226.0       5\n",
      "449.0       5\n",
      "439.0       5\n",
      "205.0       5\n",
      "214.0       4\n",
      "437.0       4\n",
      "206.0       4\n",
      "393.0       4\n",
      "391.0       4\n",
      "459.0       4\n",
      "458.0       4\n",
      "358.0       4\n",
      "455.0       4\n",
      "431.0       4\n",
      "244.0       4\n",
      "341.0       4\n",
      "201.0       4\n",
      "464.0       4\n",
      "448.0       4\n",
      "207.0       4\n",
      "366.0       4\n",
      "233.0       4\n",
      "235.0       4\n",
      "350.0       3\n",
      "363.0       3\n",
      "410.0       3\n",
      "204.0       3\n",
      "232.0       3\n",
      "470.0       3\n",
      "478.0       3\n",
      "310.0       3\n",
      "227.0       3\n",
      "349.0       3\n",
      "367.0       3\n",
      "351.0       3\n",
      "377.0       3\n",
      "450.0       3\n",
      "461.0       3\n",
      "322.0       3\n",
      "209.0       3\n",
      "354.0       3\n",
      "202.0       3\n",
      "356.0       3\n",
      "216.0       3\n",
      "355.0       3\n",
      "456.0       3\n",
      "346.0       3\n",
      "220.0       3\n",
      "211.0       3\n",
      "352.0       2\n",
      "489.0       2\n",
      "314.0       2\n",
      "326.0       2\n",
      "452.0       2\n",
      "343.0       2\n",
      "335.0       2\n",
      "231.0       2\n",
      "465.0       2\n",
      "311.0       2\n",
      "323.0       2\n",
      "319.0       2\n",
      "315.0       2\n",
      "357.0       2\n",
      "225.0       2\n",
      "462.0       2\n",
      "482.0       2\n",
      "218.0       2\n",
      "223.0       2\n",
      "347.0       2\n",
      "463.0       2\n",
      "444.0       2\n",
      "390.0       2\n",
      "469.0       2\n",
      "316.0       2\n",
      "208.0       2\n",
      "365.0       2\n",
      "337.0       2\n",
      "460.0       2\n",
      "359.0       2\n",
      "477.0       2\n",
      "325.0       1\n",
      "442.0       1\n",
      "360.0       1\n",
      "361.0       1\n",
      "476.0       1\n",
      "330.0       1\n",
      "492.0       1\n",
      "344.0       1\n",
      "466.0       1\n",
      "336.0       1\n",
      "471.0       1\n",
      "368.0       1\n",
      "342.0       1\n",
      "467.0       1\n",
      "320.0       1\n",
      "494.0       1\n",
      "468.0       1\n",
      "324.0       1\n",
      "329.0       1\n",
      "212.0       1\n",
      "224.0       1\n",
      "217.0       1\n",
      "210.0       1\n",
      "474.0       1\n",
      "364.0       1\n",
      "318.0       1\n",
      "491.0       1\n",
      "11.0        1\n",
      "340.0       1\n",
      "327.0       1\n",
      "472.0       1\n",
      "332.0       1\n",
      "457.0       1\n",
      "370.0       1\n",
      "338.0       1\n",
      "328.0       1\n",
      "228.0       1\n",
      "362.0       1\n",
      "12.0        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_cnt_month_lag_3':\n",
      "item_cnt_month_lag_3\n",
      "31.0     8102\n",
      "30.0     8074\n",
      "32.0     7622\n",
      "29.0     7615\n",
      "28.0     7075\n",
      "33.0     6845\n",
      "27.0     6324\n",
      "34.0     6024\n",
      "35.0     5132\n",
      "26.0     5083\n",
      "25.0     4160\n",
      "36.0     3847\n",
      "24.0     3092\n",
      "37.0     3090\n",
      "23.0     2210\n",
      "38.0     2201\n",
      "39.0     1627\n",
      "22.0     1534\n",
      "40.0     1135\n",
      "63.0     1068\n",
      "60.0     1057\n",
      "58.0     1055\n",
      "61.0     1049\n",
      "62.0     1041\n",
      "21.0     1018\n",
      "59.0     1017\n",
      "57.0      974\n",
      "64.0      933\n",
      "56.0      921\n",
      "65.0      890\n",
      "55.0      864\n",
      "54.0      807\n",
      "67.0      802\n",
      "66.0      802\n",
      "53.0      707\n",
      "41.0      701\n",
      "68.0      695\n",
      "52.0      631\n",
      "20.0      616\n",
      "69.0      611\n",
      "51.0      583\n",
      "50.0      561\n",
      "42.0      512\n",
      "70.0      474\n",
      "71.0      460\n",
      "48.0      458\n",
      "49.0      458\n",
      "43.0      413\n",
      "134.0     398\n",
      "46.0      375\n",
      "138.0     371\n",
      "127.0     367\n",
      "47.0      367\n",
      "132.0     364\n",
      "128.0     362\n",
      "19.0      362\n",
      "137.0     361\n",
      "130.0     359\n",
      "44.0      359\n",
      "129.0     354\n",
      "131.0     354\n",
      "136.0     350\n",
      "142.0     349\n",
      "141.0     349\n",
      "133.0     347\n",
      "72.0      345\n",
      "139.0     345\n",
      "45.0      336\n",
      "140.0     332\n",
      "126.0     328\n",
      "124.0     324\n",
      "135.0     319\n",
      "125.0     317\n",
      "144.0     296\n",
      "123.0     290\n",
      "145.0     289\n",
      "143.0     288\n",
      "146.0     286\n",
      "121.0     282\n",
      "148.0     281\n",
      "73.0      279\n",
      "119.0     267\n",
      "120.0     266\n",
      "122.0     265\n",
      "147.0     262\n",
      "74.0      259\n",
      "118.0     241\n",
      "150.0     235\n",
      "151.0     231\n",
      "149.0     212\n",
      "94.0      212\n",
      "117.0     208\n",
      "153.0     207\n",
      "154.0     206\n",
      "116.0     202\n",
      "75.0      202\n",
      "91.0      201\n",
      "152.0     193\n",
      "87.0      192\n",
      "18.0      192\n",
      "93.0      189\n",
      "96.0      185\n",
      "155.0     185\n",
      "113.0     183\n",
      "90.0      181\n",
      "115.0     179\n",
      "76.0      173\n",
      "114.0     172\n",
      "89.0      172\n",
      "105.0     171\n",
      "92.0      169\n",
      "88.0      167\n",
      "95.0      164\n",
      "85.0      162\n",
      "111.0     161\n",
      "97.0      161\n",
      "158.0     161\n",
      "112.0     159\n",
      "99.0      158\n",
      "156.0     156\n",
      "108.0     155\n",
      "77.0      151\n",
      "98.0      147\n",
      "159.0     146\n",
      "100.0     144\n",
      "86.0      143\n",
      "160.0     139\n",
      "101.0     137\n",
      "109.0     136\n",
      "110.0     130\n",
      "161.0     130\n",
      "84.0      129\n",
      "103.0     127\n",
      "157.0     127\n",
      "102.0     125\n",
      "107.0     125\n",
      "78.0      120\n",
      "82.0      119\n",
      "81.0      119\n",
      "79.0      118\n",
      "80.0      116\n",
      "106.0     113\n",
      "104.0     106\n",
      "17.0      106\n",
      "83.0       99\n",
      "162.0      96\n",
      "165.0      89\n",
      "163.0      78\n",
      "166.0      74\n",
      "164.0      74\n",
      "167.0      65\n",
      "168.0      65\n",
      "169.0      61\n",
      "170.0      52\n",
      "16.0       51\n",
      "171.0      45\n",
      "172.0      38\n",
      "174.0      37\n",
      "173.0      36\n",
      "177.0      30\n",
      "175.0      29\n",
      "176.0      27\n",
      "178.0      25\n",
      "181.0      24\n",
      "275.0      23\n",
      "180.0      23\n",
      "15.0       22\n",
      "262.0      22\n",
      "179.0      22\n",
      "296.0      19\n",
      "277.0      19\n",
      "266.0      19\n",
      "281.0      19\n",
      "188.0      18\n",
      "260.0      18\n",
      "268.0      17\n",
      "274.0      17\n",
      "272.0      17\n",
      "278.0      17\n",
      "289.0      17\n",
      "265.0      16\n",
      "269.0      16\n",
      "264.0      16\n",
      "405.0      16\n",
      "186.0      16\n",
      "251.0      16\n",
      "185.0      15\n",
      "283.0      15\n",
      "267.0      15\n",
      "183.0      15\n",
      "254.0      15\n",
      "276.0      15\n",
      "187.0      15\n",
      "294.0      14\n",
      "290.0      14\n",
      "256.0      14\n",
      "258.0      14\n",
      "196.0      14\n",
      "253.0      14\n",
      "273.0      14\n",
      "191.0      13\n",
      "261.0      13\n",
      "295.0      13\n",
      "259.0      13\n",
      "263.0      13\n",
      "285.0      13\n",
      "182.0      13\n",
      "14.0       13\n",
      "184.0      13\n",
      "271.0      13\n",
      "298.0      13\n",
      "413.0      13\n",
      "300.0      13\n",
      "435.0      13\n",
      "193.0      12\n",
      "279.0      12\n",
      "282.0      12\n",
      "255.0      12\n",
      "396.0      12\n",
      "305.0      12\n",
      "189.0      12\n",
      "406.0      11\n",
      "403.0      11\n",
      "280.0      11\n",
      "426.0      11\n",
      "387.0      11\n",
      "287.0      11\n",
      "246.0      11\n",
      "392.0      10\n",
      "395.0      10\n",
      "286.0      10\n",
      "420.0      10\n",
      "250.0      10\n",
      "194.0      10\n",
      "419.0      10\n",
      "429.0       9\n",
      "297.0       9\n",
      "288.0       9\n",
      "303.0       9\n",
      "197.0       9\n",
      "427.0       9\n",
      "248.0       9\n",
      "433.0       9\n",
      "430.0       9\n",
      "190.0       9\n",
      "414.0       9\n",
      "241.0       9\n",
      "400.0       9\n",
      "428.0       9\n",
      "252.0       9\n",
      "394.0       9\n",
      "249.0       9\n",
      "284.0       9\n",
      "421.0       9\n",
      "237.0       8\n",
      "308.0       8\n",
      "270.0       8\n",
      "243.0       8\n",
      "415.0       8\n",
      "404.0       8\n",
      "408.0       8\n",
      "312.0       8\n",
      "432.0       8\n",
      "292.0       8\n",
      "299.0       8\n",
      "411.0       8\n",
      "301.0       8\n",
      "380.0       8\n",
      "257.0       8\n",
      "417.0       8\n",
      "418.0       8\n",
      "443.0       8\n",
      "307.0       8\n",
      "397.0       8\n",
      "388.0       7\n",
      "229.0       7\n",
      "447.0       7\n",
      "247.0       7\n",
      "376.0       7\n",
      "192.0       7\n",
      "438.0       7\n",
      "195.0       7\n",
      "441.0       7\n",
      "306.0       7\n",
      "291.0       7\n",
      "409.0       7\n",
      "374.0       7\n",
      "436.0       7\n",
      "302.0       7\n",
      "399.0       7\n",
      "424.0       7\n",
      "389.0       7\n",
      "236.0       7\n",
      "198.0       7\n",
      "242.0       6\n",
      "434.0       6\n",
      "445.0       6\n",
      "234.0       6\n",
      "369.0       6\n",
      "401.0       6\n",
      "239.0       6\n",
      "221.0       6\n",
      "203.0       6\n",
      "416.0       6\n",
      "425.0       6\n",
      "378.0       6\n",
      "321.0       6\n",
      "384.0       6\n",
      "398.0       6\n",
      "293.0       6\n",
      "238.0       6\n",
      "440.0       5\n",
      "309.0       5\n",
      "379.0       5\n",
      "454.0       5\n",
      "304.0       5\n",
      "200.0       5\n",
      "317.0       5\n",
      "222.0       5\n",
      "383.0       5\n",
      "439.0       5\n",
      "386.0       5\n",
      "205.0       5\n",
      "385.0       5\n",
      "375.0       5\n",
      "13.0        5\n",
      "422.0       5\n",
      "245.0       5\n",
      "451.0       5\n",
      "423.0       5\n",
      "240.0       5\n",
      "372.0       5\n",
      "381.0       5\n",
      "371.0       5\n",
      "402.0       5\n",
      "226.0       5\n",
      "230.0       5\n",
      "201.0       4\n",
      "358.0       4\n",
      "431.0       4\n",
      "214.0       4\n",
      "448.0       4\n",
      "412.0       4\n",
      "437.0       4\n",
      "393.0       4\n",
      "313.0       4\n",
      "464.0       4\n",
      "407.0       4\n",
      "459.0       4\n",
      "207.0       4\n",
      "199.0       4\n",
      "206.0       4\n",
      "458.0       4\n",
      "391.0       4\n",
      "449.0       4\n",
      "366.0       4\n",
      "235.0       4\n",
      "202.0       3\n",
      "470.0       3\n",
      "450.0       3\n",
      "216.0       3\n",
      "349.0       3\n",
      "455.0       3\n",
      "310.0       3\n",
      "204.0       3\n",
      "227.0       3\n",
      "478.0       3\n",
      "350.0       3\n",
      "410.0       3\n",
      "244.0       3\n",
      "377.0       3\n",
      "232.0       3\n",
      "346.0       3\n",
      "209.0       3\n",
      "211.0       3\n",
      "456.0       3\n",
      "322.0       3\n",
      "341.0       3\n",
      "367.0       3\n",
      "363.0       3\n",
      "355.0       3\n",
      "233.0       3\n",
      "220.0       3\n",
      "356.0       3\n",
      "354.0       3\n",
      "352.0       2\n",
      "343.0       2\n",
      "231.0       2\n",
      "335.0       2\n",
      "225.0       2\n",
      "359.0       2\n",
      "465.0       2\n",
      "311.0       2\n",
      "469.0       2\n",
      "326.0       2\n",
      "208.0       2\n",
      "489.0       2\n",
      "452.0       2\n",
      "357.0       2\n",
      "463.0       2\n",
      "365.0       2\n",
      "323.0       2\n",
      "444.0       2\n",
      "347.0       2\n",
      "223.0       2\n",
      "390.0       2\n",
      "316.0       2\n",
      "482.0       2\n",
      "337.0       2\n",
      "462.0       2\n",
      "461.0       2\n",
      "460.0       2\n",
      "314.0       2\n",
      "477.0       2\n",
      "351.0       2\n",
      "218.0       2\n",
      "368.0       1\n",
      "492.0       1\n",
      "336.0       1\n",
      "362.0       1\n",
      "360.0       1\n",
      "467.0       1\n",
      "324.0       1\n",
      "442.0       1\n",
      "224.0       1\n",
      "476.0       1\n",
      "325.0       1\n",
      "361.0       1\n",
      "212.0       1\n",
      "342.0       1\n",
      "466.0       1\n",
      "471.0       1\n",
      "491.0       1\n",
      "328.0       1\n",
      "318.0       1\n",
      "217.0       1\n",
      "210.0       1\n",
      "329.0       1\n",
      "228.0       1\n",
      "12.0        1\n",
      "344.0       1\n",
      "370.0       1\n",
      "340.0       1\n",
      "330.0       1\n",
      "338.0       1\n",
      "319.0       1\n",
      "474.0       1\n",
      "364.0       1\n",
      "11.0        1\n",
      "327.0       1\n",
      "472.0       1\n",
      "332.0       1\n",
      "457.0       1\n",
      "320.0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_cnt_month_lag_6':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_cnt_month_lag_6\n",
      "30.0     7325\n",
      "31.0     7292\n",
      "29.0     6889\n",
      "32.0     6865\n",
      "28.0     6428\n",
      "33.0     6215\n",
      "27.0     5721\n",
      "34.0     5478\n",
      "26.0     4616\n",
      "35.0     4597\n",
      "25.0     3775\n",
      "36.0     3470\n",
      "24.0     2846\n",
      "37.0     2796\n",
      "23.0     2048\n",
      "38.0     1974\n",
      "39.0     1453\n",
      "22.0     1402\n",
      "40.0     1000\n",
      "63.0      983\n",
      "58.0      968\n",
      "60.0      963\n",
      "61.0      953\n",
      "62.0      951\n",
      "21.0      950\n",
      "59.0      914\n",
      "57.0      894\n",
      "64.0      845\n",
      "56.0      838\n",
      "65.0      811\n",
      "55.0      770\n",
      "54.0      735\n",
      "67.0      734\n",
      "66.0      726\n",
      "53.0      641\n",
      "41.0      634\n",
      "68.0      617\n",
      "52.0      576\n",
      "20.0      556\n",
      "69.0      546\n",
      "51.0      527\n",
      "50.0      512\n",
      "42.0      464\n",
      "70.0      425\n",
      "49.0      422\n",
      "48.0      419\n",
      "71.0      395\n",
      "134.0     374\n",
      "43.0      365\n",
      "138.0     343\n",
      "47.0      343\n",
      "127.0     340\n",
      "129.0     335\n",
      "46.0      335\n",
      "128.0     334\n",
      "19.0      332\n",
      "132.0     332\n",
      "130.0     331\n",
      "137.0     330\n",
      "131.0     330\n",
      "142.0     329\n",
      "136.0     328\n",
      "133.0     327\n",
      "44.0      327\n",
      "141.0     323\n",
      "139.0     321\n",
      "140.0     315\n",
      "72.0      309\n",
      "124.0     306\n",
      "126.0     305\n",
      "45.0      299\n",
      "135.0     296\n",
      "125.0     295\n",
      "144.0     277\n",
      "143.0     273\n",
      "145.0     267\n",
      "123.0     265\n",
      "148.0     264\n",
      "146.0     262\n",
      "121.0     261\n",
      "119.0     249\n",
      "120.0     249\n",
      "122.0     247\n",
      "73.0      246\n",
      "74.0      243\n",
      "147.0     239\n",
      "118.0     223\n",
      "151.0     220\n",
      "150.0     219\n",
      "149.0     202\n",
      "153.0     195\n",
      "94.0      193\n",
      "117.0     192\n",
      "154.0     189\n",
      "116.0     184\n",
      "75.0      183\n",
      "18.0      182\n",
      "152.0     181\n",
      "91.0      180\n",
      "87.0      175\n",
      "113.0     174\n",
      "96.0      173\n",
      "155.0     171\n",
      "93.0      168\n",
      "90.0      165\n",
      "89.0      161\n",
      "105.0     161\n",
      "114.0     161\n",
      "115.0     159\n",
      "92.0      157\n",
      "88.0      154\n",
      "97.0      151\n",
      "76.0      151\n",
      "156.0     149\n",
      "95.0      147\n",
      "112.0     146\n",
      "111.0     146\n",
      "158.0     146\n",
      "85.0      145\n",
      "108.0     145\n",
      "159.0     142\n",
      "99.0      142\n",
      "86.0      134\n",
      "98.0      134\n",
      "77.0      134\n",
      "160.0     133\n",
      "100.0     131\n",
      "109.0     128\n",
      "161.0     122\n",
      "110.0     120\n",
      "101.0     120\n",
      "157.0     118\n",
      "84.0      116\n",
      "103.0     113\n",
      "107.0     111\n",
      "102.0     111\n",
      "79.0      111\n",
      "78.0      109\n",
      "81.0      107\n",
      "82.0      106\n",
      "80.0      104\n",
      "106.0     102\n",
      "104.0     100\n",
      "17.0       97\n",
      "162.0      89\n",
      "83.0       87\n",
      "165.0      84\n",
      "163.0      74\n",
      "166.0      70\n",
      "164.0      70\n",
      "167.0      62\n",
      "168.0      59\n",
      "169.0      57\n",
      "170.0      51\n",
      "16.0       50\n",
      "171.0      41\n",
      "172.0      37\n",
      "173.0      35\n",
      "174.0      32\n",
      "175.0      27\n",
      "176.0      25\n",
      "177.0      25\n",
      "275.0      22\n",
      "178.0      21\n",
      "180.0      21\n",
      "181.0      21\n",
      "15.0       20\n",
      "179.0      20\n",
      "262.0      19\n",
      "296.0      19\n",
      "277.0      19\n",
      "281.0      18\n",
      "266.0      18\n",
      "268.0      17\n",
      "289.0      17\n",
      "274.0      16\n",
      "278.0      16\n",
      "272.0      16\n",
      "188.0      15\n",
      "264.0      15\n",
      "185.0      15\n",
      "186.0      15\n",
      "269.0      15\n",
      "254.0      15\n",
      "405.0      15\n",
      "260.0      15\n",
      "253.0      14\n",
      "273.0      14\n",
      "183.0      14\n",
      "283.0      14\n",
      "276.0      14\n",
      "265.0      14\n",
      "251.0      14\n",
      "258.0      14\n",
      "187.0      14\n",
      "290.0      14\n",
      "182.0      13\n",
      "261.0      13\n",
      "196.0      13\n",
      "267.0      13\n",
      "259.0      13\n",
      "256.0      13\n",
      "294.0      13\n",
      "298.0      13\n",
      "14.0       13\n",
      "300.0      13\n",
      "263.0      12\n",
      "282.0      12\n",
      "295.0      12\n",
      "305.0      12\n",
      "255.0      12\n",
      "191.0      12\n",
      "285.0      12\n",
      "406.0      11\n",
      "184.0      11\n",
      "193.0      11\n",
      "396.0      11\n",
      "403.0      11\n",
      "426.0      11\n",
      "279.0      11\n",
      "189.0      11\n",
      "280.0      11\n",
      "392.0      10\n",
      "286.0      10\n",
      "435.0      10\n",
      "287.0      10\n",
      "246.0      10\n",
      "420.0      10\n",
      "271.0      10\n",
      "190.0       9\n",
      "248.0       9\n",
      "414.0       9\n",
      "241.0       9\n",
      "197.0       9\n",
      "427.0       9\n",
      "303.0       9\n",
      "387.0       9\n",
      "252.0       9\n",
      "249.0       9\n",
      "250.0       9\n",
      "429.0       8\n",
      "430.0       8\n",
      "237.0       8\n",
      "243.0       8\n",
      "419.0       8\n",
      "299.0       8\n",
      "292.0       8\n",
      "395.0       8\n",
      "288.0       8\n",
      "312.0       8\n",
      "404.0       8\n",
      "411.0       8\n",
      "443.0       8\n",
      "307.0       8\n",
      "394.0       8\n",
      "308.0       8\n",
      "297.0       8\n",
      "417.0       8\n",
      "270.0       8\n",
      "257.0       8\n",
      "415.0       8\n",
      "433.0       7\n",
      "399.0       7\n",
      "388.0       7\n",
      "247.0       7\n",
      "376.0       7\n",
      "408.0       7\n",
      "194.0       7\n",
      "192.0       7\n",
      "424.0       7\n",
      "421.0       7\n",
      "374.0       7\n",
      "432.0       7\n",
      "400.0       7\n",
      "389.0       7\n",
      "397.0       7\n",
      "441.0       7\n",
      "301.0       7\n",
      "413.0       7\n",
      "306.0       7\n",
      "284.0       7\n",
      "198.0       7\n",
      "447.0       7\n",
      "409.0       7\n",
      "436.0       7\n",
      "242.0       6\n",
      "236.0       6\n",
      "234.0       6\n",
      "434.0       6\n",
      "221.0       6\n",
      "302.0       6\n",
      "384.0       6\n",
      "238.0       6\n",
      "398.0       6\n",
      "203.0       6\n",
      "425.0       6\n",
      "438.0       6\n",
      "291.0       6\n",
      "293.0       6\n",
      "229.0       6\n",
      "445.0       6\n",
      "380.0       6\n",
      "369.0       6\n",
      "378.0       6\n",
      "428.0       6\n",
      "401.0       6\n",
      "418.0       6\n",
      "226.0       5\n",
      "385.0       5\n",
      "321.0       5\n",
      "375.0       5\n",
      "439.0       5\n",
      "454.0       5\n",
      "304.0       5\n",
      "416.0       5\n",
      "440.0       5\n",
      "200.0       5\n",
      "372.0       5\n",
      "230.0       5\n",
      "317.0       5\n",
      "245.0       5\n",
      "205.0       5\n",
      "371.0       5\n",
      "222.0       5\n",
      "423.0       5\n",
      "386.0       5\n",
      "195.0       5\n",
      "240.0       5\n",
      "239.0       5\n",
      "309.0       5\n",
      "381.0       5\n",
      "366.0       4\n",
      "207.0       4\n",
      "199.0       4\n",
      "201.0       4\n",
      "391.0       4\n",
      "393.0       4\n",
      "402.0       4\n",
      "451.0       4\n",
      "383.0       4\n",
      "214.0       4\n",
      "464.0       4\n",
      "437.0       4\n",
      "459.0       4\n",
      "422.0       4\n",
      "458.0       4\n",
      "379.0       4\n",
      "449.0       4\n",
      "216.0       3\n",
      "407.0       3\n",
      "349.0       3\n",
      "450.0       3\n",
      "478.0       3\n",
      "202.0       3\n",
      "227.0       3\n",
      "204.0       3\n",
      "455.0       3\n",
      "448.0       3\n",
      "456.0       3\n",
      "412.0       3\n",
      "354.0       3\n",
      "431.0       3\n",
      "244.0       3\n",
      "358.0       3\n",
      "363.0       3\n",
      "220.0       3\n",
      "206.0       3\n",
      "232.0       3\n",
      "322.0       3\n",
      "233.0       3\n",
      "13.0        3\n",
      "209.0       3\n",
      "356.0       3\n",
      "313.0       3\n",
      "310.0       3\n",
      "341.0       3\n",
      "235.0       3\n",
      "482.0       2\n",
      "470.0       2\n",
      "314.0       2\n",
      "347.0       2\n",
      "465.0       2\n",
      "326.0       2\n",
      "311.0       2\n",
      "390.0       2\n",
      "323.0       2\n",
      "452.0       2\n",
      "335.0       2\n",
      "469.0       2\n",
      "461.0       2\n",
      "489.0       2\n",
      "377.0       2\n",
      "231.0       2\n",
      "351.0       2\n",
      "218.0       2\n",
      "343.0       2\n",
      "460.0       2\n",
      "410.0       2\n",
      "359.0       2\n",
      "316.0       2\n",
      "367.0       2\n",
      "337.0       2\n",
      "463.0       2\n",
      "444.0       2\n",
      "357.0       2\n",
      "350.0       2\n",
      "477.0       2\n",
      "225.0       2\n",
      "355.0       2\n",
      "462.0       2\n",
      "346.0       2\n",
      "211.0       2\n",
      "223.0       2\n",
      "362.0       1\n",
      "368.0       1\n",
      "224.0       1\n",
      "342.0       1\n",
      "471.0       1\n",
      "328.0       1\n",
      "212.0       1\n",
      "336.0       1\n",
      "476.0       1\n",
      "324.0       1\n",
      "361.0       1\n",
      "492.0       1\n",
      "325.0       1\n",
      "442.0       1\n",
      "360.0       1\n",
      "466.0       1\n",
      "474.0       1\n",
      "338.0       1\n",
      "330.0       1\n",
      "370.0       1\n",
      "365.0       1\n",
      "344.0       1\n",
      "12.0        1\n",
      "340.0       1\n",
      "318.0       1\n",
      "228.0       1\n",
      "208.0       1\n",
      "329.0       1\n",
      "319.0       1\n",
      "352.0       1\n",
      "364.0       1\n",
      "11.0        1\n",
      "491.0       1\n",
      "327.0       1\n",
      "210.0       1\n",
      "472.0       1\n",
      "332.0       1\n",
      "217.0       1\n",
      "457.0       1\n",
      "467.0       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'product_avg_sale_last_6':\n",
      "product_avg_sale_last_6\n",
      "0.000000    4179\n",
      "0.638866    1092\n",
      "0.636530    1065\n",
      "0.645873    1059\n",
      "0.648209    1053\n",
      "            ... \n",
      "2.725982       3\n",
      "2.821754       3\n",
      "5.029169       3\n",
      "3.353169       3\n",
      "3.757277       3\n",
      "Name: count, Length: 2722, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'product_std_sale_last_6':\n",
      "product_std_sale_last_6\n",
      "0.000000    4179\n",
      "0.484469     129\n",
      "0.463173     123\n",
      "0.516412     120\n",
      "0.500440     114\n",
      "            ... \n",
      "2.286294       3\n",
      "2.205110       3\n",
      "2.192189       3\n",
      "2.241955       3\n",
      "0.143012       3\n",
      "Name: count, Length: 31027, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'product_avg_sale_last_12':\n",
      "product_avg_sale_last_12\n",
      "0.000000    4179\n",
      "0.695438     396\n",
      "0.699887     339\n",
      "0.697345     336\n",
      "0.694166     327\n",
      "            ... \n",
      "3.477189       3\n",
      "3.342424       3\n",
      "1.188092       3\n",
      "3.382472       3\n",
      "2.022109       3\n",
      "Name: count, Length: 4234, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'product_std_sale_last_12':\n",
      "product_std_sale_last_12\n",
      "0.000000    4179\n",
      "0.278339     129\n",
      "0.266104     123\n",
      "0.296691     120\n",
      "0.287515     114\n",
      "            ... \n",
      "1.389848       3\n",
      "1.233094       3\n",
      "1.352586       3\n",
      "1.281535       3\n",
      "0.075847       3\n",
      "Name: count, Length: 43224, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'campaign_avg_sale_last_6':\n",
      "campaign_avg_sale_last_6\n",
      "0.000000    4179\n",
      "4.235025     642\n",
      "4.252411     321\n",
      "4.245570     321\n",
      "2.655850     321\n",
      "            ... \n",
      "0.065599       4\n",
      "0.064996       4\n",
      "0.039319       4\n",
      "0.040768       4\n",
      "0.041372       4\n",
      "Name: count, Length: 1566, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'campaign_std_sale_last_6':\n",
      "campaign_std_sale_last_6\n",
      "0.000000    4179\n",
      "7.503352     321\n",
      "9.484383     321\n",
      "9.543721     321\n",
      "9.489879     321\n",
      "            ... \n",
      "0.004390       4\n",
      "0.002565       4\n",
      "0.108294       4\n",
      "0.105625       4\n",
      "0.003945       4\n",
      "Name: count, Length: 1679, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'campaign_avg_sale_last_12':\n",
      "campaign_avg_sale_last_12\n",
      "0.000000    4179\n",
      "3.757807     642\n",
      "6.263361     321\n",
      "0.954527     321\n",
      "0.946226     321\n",
      "            ... \n",
      "0.030710       4\n",
      "0.030688       4\n",
      "0.051870       4\n",
      "0.051629       4\n",
      "0.050030       4\n",
      "Name: count, Length: 1599, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'campaign_std_sale_last_12':\n",
      "campaign_std_sale_last_12\n",
      "0.000000    4179\n",
      "4.289868     321\n",
      "4.245614     321\n",
      "7.024654     321\n",
      "7.017521     321\n",
      "            ... \n",
      "0.064850       4\n",
      "0.063305       4\n",
      "0.072806       4\n",
      "0.069079       4\n",
      "0.069560       4\n",
      "Name: count, Length: 1669, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'category_avg_sale_last_12':\n",
      "category_avg_sale_last_12\n",
      "0.000000    4179\n",
      "4.348400     354\n",
      "4.411105     312\n",
      "1.791136     288\n",
      "5.585589     252\n",
      "            ... \n",
      "0.141327       3\n",
      "0.048920       3\n",
      "0.071613       3\n",
      "0.133091       3\n",
      "0.117057       3\n",
      "Name: count, Length: 2143, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'category_std_sale_last_12':\n",
      "category_std_sale_last_12\n",
      "0.000000    4179\n",
      "1.221977     252\n",
      "5.883348     252\n",
      "6.744088     252\n",
      "5.733106     252\n",
      "            ... \n",
      "0.004540       3\n",
      "0.021837       3\n",
      "0.095531       3\n",
      "0.020296       3\n",
      "0.061896       3\n",
      "Name: count, Length: 2227, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_avg_item_cnt_lag_1':\n",
      "date_avg_item_cnt_lag_1\n",
      "39.93750     16716\n",
      "38.59375      8358\n",
      "65.56250      4179\n",
      "59.15625      4179\n",
      "57.68750      4179\n",
      "57.12500      4179\n",
      "50.15625      4179\n",
      "38.62500      4179\n",
      "39.81250      4179\n",
      "38.65625      4179\n",
      "47.90625      4179\n",
      "36.37500      4179\n",
      "106.18750     4179\n",
      "40.21875      4179\n",
      "65.50000      4179\n",
      "39.84375      4179\n",
      "48.56250      4179\n",
      "59.00000      4179\n",
      "70.37500      4179\n",
      "39.96875      4179\n",
      "69.00000      4179\n",
      "104.62500     4179\n",
      "83.62500      4179\n",
      "43.15625      4179\n",
      "39.87500      4179\n",
      "38.71875      4179\n",
      "85.68750      4179\n",
      "38.78125      4179\n",
      "50.18750      4179\n",
      "38.75000      4179\n",
      "66.31250      4179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_campaign_avg_item_cnt_lag_1':\n",
      "date_campaign_avg_item_cnt_lag_1\n",
      "33.87500     1860\n",
      "34.59375     1774\n",
      "33.93750     1697\n",
      "33.53125     1461\n",
      "35.00000     1453\n",
      "             ... \n",
      "126.00000       4\n",
      "135.25000       4\n",
      "147.00000       4\n",
      "140.25000       4\n",
      "142.00000       4\n",
      "Name: count, Length: 752, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_campaign_avg_item_cnt_lag_2':\n",
      "date_campaign_avg_item_cnt_lag_2\n",
      "33.87500     1860\n",
      "34.59375     1774\n",
      "33.93750     1697\n",
      "35.00000     1453\n",
      "34.84375     1399\n",
      "             ... \n",
      "135.25000       4\n",
      "139.00000       4\n",
      "131.00000       4\n",
      "26.25000        4\n",
      "122.75000       4\n",
      "Name: count, Length: 748, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_campaign_avg_item_cnt_lag_3':\n",
      "date_campaign_avg_item_cnt_lag_3\n",
      "33.87500     1860\n",
      "34.59375     1774\n",
      "33.93750     1697\n",
      "35.00000     1453\n",
      "34.84375     1399\n",
      "             ... \n",
      "25.50000        4\n",
      "142.00000       4\n",
      "140.75000       4\n",
      "133.50000       4\n",
      "26.25000        4\n",
      "Name: count, Length: 739, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_campaign_avg_item_cnt_lag_6':\n",
      "date_campaign_avg_item_cnt_lag_6\n",
      "34.59375     1774\n",
      "33.93750     1605\n",
      "33.87500     1447\n",
      "33.53125     1283\n",
      "35.09375     1237\n",
      "             ... \n",
      "28.00000        4\n",
      "140.00000       4\n",
      "34.50000        4\n",
      "140.25000       4\n",
      "138.00000       4\n",
      "Name: count, Length: 711, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_item_avg_item_cnt_lag_1':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_item_avg_item_cnt_lag_1\n",
      "30.000000     4860\n",
      "30.328125     4692\n",
      "31.000000     4689\n",
      "30.671875     4665\n",
      "29.328125     4491\n",
      "              ... \n",
      "252.375000       3\n",
      "286.250000       3\n",
      "169.000000       3\n",
      "313.250000       3\n",
      "283.750000       3\n",
      "Name: count, Length: 830, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_item_avg_item_cnt_lag_2':\n",
      "date_item_avg_item_cnt_lag_2\n",
      "30.000000     4740\n",
      "31.000000     4590\n",
      "30.328125     4578\n",
      "30.671875     4557\n",
      "31.328125     4386\n",
      "              ... \n",
      "286.250000       3\n",
      "169.000000       3\n",
      "313.250000       3\n",
      "262.750000       3\n",
      "176.625000       3\n",
      "Name: count, Length: 823, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_item_avg_item_cnt_lag_3':\n",
      "date_item_avg_item_cnt_lag_3\n",
      "30.000000     4644\n",
      "30.328125     4473\n",
      "31.000000     4449\n",
      "30.671875     4398\n",
      "31.328125     4251\n",
      "              ... \n",
      "178.375000       3\n",
      "18.328125        3\n",
      "264.750000       3\n",
      "358.250000       3\n",
      "373.750000       3\n",
      "Name: count, Length: 814, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_item_avg_item_cnt_lag_6':\n",
      "date_item_avg_item_cnt_lag_6\n",
      "30.000000     4239\n",
      "30.328125     4050\n",
      "31.000000     4017\n",
      "30.671875     3978\n",
      "31.328125     3822\n",
      "              ... \n",
      "443.250000       3\n",
      "169.625000       3\n",
      "291.250000       3\n",
      "437.250000       3\n",
      "172.625000       3\n",
      "Name: count, Length: 800, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'date_campaign_cat_avg_item_cnt_lag_1':\n",
      "date_campaign_cat_avg_item_cnt_lag_1\n",
      "33.34375     956\n",
      "32.59375     800\n",
      "34.00000     728\n",
      "33.84375     727\n",
      "32.93750     695\n",
      "            ... \n",
      "290.00000      1\n",
      "273.00000      1\n",
      "237.00000      1\n",
      "274.00000      1\n",
      "294.00000      1\n",
      "Name: count, Length: 1566, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'delta_price_lag':\n",
      "delta_price_lag\n",
      " 0.000000    147385\n",
      " 0.004948       145\n",
      "-0.001279       145\n",
      "-0.000552       142\n",
      " 0.000552       142\n",
      " 0.001658       142\n",
      " 0.002853        74\n",
      " 0.001282        71\n",
      " 0.000582        71\n",
      "-0.007442        71\n",
      "-0.001475        71\n",
      "-0.000799        71\n",
      "-0.006886        71\n",
      " 0.000667        71\n",
      " 0.002466        71\n",
      " 0.001050        71\n",
      " 0.000616        71\n",
      "-0.001653        71\n",
      "-0.002165        71\n",
      "-0.002752        71\n",
      " 0.001033        71\n",
      "-0.003925        71\n",
      "-0.000999        71\n",
      " 0.002138        71\n",
      " inf             48\n",
      "-0.008949         6\n",
      " 0.009377         6\n",
      "-0.055084         6\n",
      "-0.030991         6\n",
      "-0.009453         6\n",
      "-0.004475         6\n",
      "-0.000616         6\n",
      "-0.017120         6\n",
      " 0.000550         6\n",
      " 0.026855         6\n",
      " 0.032013         6\n",
      "-0.017349         6\n",
      "-0.017899         6\n",
      "-0.020981         6\n",
      "-0.032593         3\n",
      "-0.003204         3\n",
      "-0.020401         3\n",
      " 0.036957         3\n",
      " 0.023392         3\n",
      " 0.055634         3\n",
      " 0.012695         3\n",
      " 0.017456         3\n",
      "-0.008659         3\n",
      "-0.015488         3\n",
      "-0.001237         3\n",
      " 0.021805         3\n",
      "-0.036530         3\n",
      "-0.010231         3\n",
      "-0.002453         3\n",
      "-0.000550         3\n",
      " 0.003313         3\n",
      " 0.044678         3\n",
      " 0.022034         3\n",
      " 0.034821         3\n",
      "-0.017807         3\n",
      "-0.021835         3\n",
      "-0.022202         3\n",
      " 0.006634         3\n",
      " 0.050842         3\n",
      " 0.020782         3\n",
      " 0.099731         3\n",
      " 0.030823         3\n",
      " 0.072144         3\n",
      " 0.029221         3\n",
      " 0.003099         3\n",
      "-0.000530         3\n",
      "-0.014359         3\n",
      "-0.006626         3\n",
      " 0.003742         3\n",
      "-0.019867         3\n",
      "-0.010300         3\n",
      " 0.009354         3\n",
      "-0.051849         3\n",
      " 0.007164         3\n",
      "-0.038696         3\n",
      " 0.011253         3\n",
      "-0.035858         3\n",
      "-0.018799         3\n",
      " 0.041931         3\n",
      " 0.023346         3\n",
      " 0.018799         3\n",
      " 0.033356         3\n",
      "-0.016785         3\n",
      " 0.034241         3\n",
      " 0.002758         3\n",
      " 0.004330         3\n",
      " 0.038605         3\n",
      "-0.006184         3\n",
      "-0.006611         3\n",
      " 0.025421         3\n",
      "-0.016006         3\n",
      "-0.016876         3\n",
      " 0.021530         3\n",
      " 0.010887         3\n",
      " 0.013161         3\n",
      " 0.020676         3\n",
      "-0.014923         3\n",
      "-0.041901         3\n",
      " 0.037567         3\n",
      "-0.004917         3\n",
      "-0.033508         3\n",
      "-0.112183         3\n",
      "-0.003492         3\n",
      "-0.028320         3\n",
      "-0.006062         3\n",
      " 0.035797         3\n",
      " 0.041199         3\n",
      " 0.097473         3\n",
      "-0.021576         3\n",
      " 0.020691         3\n",
      "-0.027298         3\n",
      "-0.027496         3\n",
      "-0.012222         3\n",
      " 0.030792         3\n",
      "-0.027832         3\n",
      " 0.032501         3\n",
      " 0.019485         3\n",
      " 0.004993         3\n",
      " 0.027786         3\n",
      "-0.059601         3\n",
      "-0.005116         3\n",
      "-0.019623         3\n",
      "-0.011879         3\n",
      " 0.039062         3\n",
      "-0.006081         3\n",
      "-0.070740         3\n",
      " 0.014801         3\n",
      " 0.048065         3\n",
      "-0.026199         3\n",
      " 0.011696         3\n",
      "-0.029068         3\n",
      "-0.002031         3\n",
      " 0.007034         3\n",
      "-0.001655         3\n",
      " 0.008553         3\n",
      "-0.001963         3\n",
      "-0.035614         3\n",
      "-0.004650         3\n",
      " 0.004971         3\n",
      " 0.033661         3\n",
      "-0.007164         3\n",
      "-0.009399         3\n",
      "-0.024612         3\n",
      " 0.012604         3\n",
      "-0.049683         3\n",
      " 0.014343         3\n",
      "-0.027542         3\n",
      "-0.052734         3\n",
      "-0.030472         3\n",
      " 0.012177         3\n",
      "-0.053192         3\n",
      " 0.003302         3\n",
      " 0.017654         3\n",
      " 0.002211         3\n",
      "-0.014908         3\n",
      "-0.026230         3\n",
      " 0.007721         3\n",
      " 0.043579         3\n",
      "-0.012009         3\n",
      "-0.003317         3\n",
      "-0.065674         3\n",
      "-0.039154         3\n",
      "-0.020645         3\n",
      "-0.035675         3\n",
      "-0.025970         3\n",
      "-0.001082         3\n",
      "-0.040894         3\n",
      "-0.057190         3\n",
      " 0.014359         3\n",
      " 0.022507         3\n",
      "-0.022583         3\n",
      "-0.012367         3\n",
      " 0.041107         3\n",
      " 0.006672         3\n",
      "-0.019348         3\n",
      "-0.023987         3\n",
      "-0.022858         3\n",
      " 0.001475         3\n",
      "-0.014977         3\n",
      "-0.021347         3\n",
      "-0.031342         3\n",
      "-0.006401         3\n",
      "-0.024628         3\n",
      "-0.031525         3\n",
      "-0.022827         3\n",
      " 0.011955         3\n",
      " 0.012222         3\n",
      "-0.048248         3\n",
      " 0.041840         3\n",
      " 0.031174         3\n",
      "-0.012291         3\n",
      " 0.032806         3\n",
      " 0.002558         3\n",
      " 0.004658         3\n",
      "-0.022888         3\n",
      "-0.057373         3\n",
      " 0.008003         3\n",
      "-0.017654         3\n",
      " 0.015991         3\n",
      " 0.081238         3\n",
      " 0.010895         3\n",
      " 0.039001         3\n",
      "-0.040161         3\n",
      "-0.017105         3\n",
      " 0.025986         3\n",
      "-0.092957         3\n",
      "-0.022110         3\n",
      "-0.010330         3\n",
      "-0.001623         3\n",
      " 0.011581         3\n",
      "-0.003998         3\n",
      "-0.070007         3\n",
      "-0.019226         3\n",
      " 0.031982         3\n",
      "-0.016190         3\n",
      " 0.033569         3\n",
      " 0.005939         3\n",
      " 0.017120         3\n",
      "-0.008827         3\n",
      " 0.014328         3\n",
      " 0.019897         3\n",
      " 0.003866         3\n",
      "-0.000703         3\n",
      " 0.024155         3\n",
      " 0.013222         3\n",
      "-0.036987         3\n",
      "-0.019699         3\n",
      "-0.009842         3\n",
      "-0.044434         3\n",
      "-0.019806         3\n",
      "-0.012703         3\n",
      "-0.040253         3\n",
      " 0.016525         3\n",
      "-0.001105         3\n",
      " 0.009941         3\n",
      "-0.007355         3\n",
      " 0.008972         3\n",
      "-0.027954         3\n",
      "-0.022675         3\n",
      "-0.033722         3\n",
      " 0.015358         3\n",
      " 0.011993         3\n",
      " 0.006390         3\n",
      "-0.007133         3\n",
      "-0.007671         3\n",
      "-0.002672         3\n",
      "-0.004921         3\n",
      "-0.037811         3\n",
      " 0.017334         3\n",
      "-0.012337         3\n",
      " 0.003336         3\n",
      " 0.004421         3\n",
      " 0.105957         3\n",
      " 0.010391         3\n",
      " 0.000492         3\n",
      " 0.062042         3\n",
      "-0.001999         3\n",
      " 0.021744         3\n",
      " 0.035492         3\n",
      " 0.015396         3\n",
      " 0.017319         3\n",
      " 0.010155         3\n",
      " 0.012985         3\n",
      "-0.003860         3\n",
      " 0.014725         3\n",
      " 0.021027         3\n",
      "-0.011696         3\n",
      "-0.003555         3\n",
      "-0.037567         3\n",
      " 0.043518         3\n",
      " 0.042175         3\n",
      " 0.029419         3\n",
      " 0.014549         3\n",
      "-0.036957         3\n",
      " 0.008659         3\n",
      " 0.036804         3\n",
      " 0.005527         3\n",
      " 0.004414         3\n",
      "-0.019974         3\n",
      "-0.050781         3\n",
      " 0.007050         3\n",
      " 0.018082         3\n",
      "-0.024826         3\n",
      "-0.008812         3\n",
      " 0.011650         3\n",
      " 0.017273         3\n",
      "-0.020950         3\n",
      "-0.024796         3\n",
      "-0.007420         3\n",
      "-0.011681         3\n",
      "-0.002707         3\n",
      "-0.028687         3\n",
      "-0.006992         3\n",
      " 0.052368         3\n",
      "-0.021790         3\n",
      " 0.040741         3\n",
      "-0.052948         3\n",
      "-0.002209         3\n",
      "-0.009377         3\n",
      " 0.035248         3\n",
      "-0.017685         3\n",
      " 0.038666         3\n",
      " 0.012405         3\n",
      "-0.004200         3\n",
      "-0.006634         3\n",
      " 0.025955         3\n",
      "-0.007736         3\n",
      " 0.042206         3\n",
      "-0.013947         3\n",
      "-0.046783         3\n",
      " 0.032593         3\n",
      " 0.075562         3\n",
      "-0.020935         3\n",
      " 0.016037         3\n",
      " 0.004417         3\n",
      "-0.034546         3\n",
      " 0.017853         3\n",
      " 0.024017         3\n",
      "-0.009949         3\n",
      " 0.048767         3\n",
      "-0.009834         3\n",
      "-0.019257         3\n",
      " 0.119629         3\n",
      "-0.051147         3\n",
      " 0.003925         3\n",
      "-0.009590         3\n",
      " 0.035889         3\n",
      "-0.106384         3\n",
      "-0.017319         3\n",
      "-0.046875         3\n",
      "-0.025421         3\n",
      "-0.015358         3\n",
      "-0.003197         3\n",
      "-0.067749         3\n",
      " 0.036224         3\n",
      "-0.012787         3\n",
      " 0.025604         3\n",
      " 0.025253         3\n",
      "-0.016083         3\n",
      "-0.005077         3\n",
      " 0.018936         3\n",
      "-0.022064         3\n",
      " 0.014984         3\n",
      " 0.025574         3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'delta_revenue_lag_1':\n",
      "delta_revenue_lag_1\n",
      " 0.019180    19582\n",
      "-0.013702    11700\n",
      "-0.079468     3171\n",
      "-0.022858     1438\n",
      "-0.022644     1346\n",
      "             ...  \n",
      " 0.023621        6\n",
      " 0.044312        6\n",
      " 0.031891        6\n",
      " 0.040161        6\n",
      " 0.019943        6\n",
      "Name: count, Length: 553, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'month':\n",
      "month\n",
      "1     12537\n",
      "2     12537\n",
      "3     12537\n",
      "4     12537\n",
      "5     12537\n",
      "6     12537\n",
      "7     12537\n",
      "8     12537\n",
      "9     12537\n",
      "10    12537\n",
      "11    12537\n",
      "0     12537\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'year':\n",
      "year\n",
      "1    50148\n",
      "2    50148\n",
      "0    45969\n",
      "3     4179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_last_sale':\n",
      "item_last_sale\n",
      "1.0     4179\n",
      "27.0    4179\n",
      "21.0    4179\n",
      "22.0    4179\n",
      "23.0    4179\n",
      "24.0    4179\n",
      "25.0    4179\n",
      "26.0    4179\n",
      "28.0    4179\n",
      "19.0    4179\n",
      "29.0    4179\n",
      "30.0    4179\n",
      "31.0    4179\n",
      "32.0    4179\n",
      "33.0    4179\n",
      "34.0    4179\n",
      "20.0    4179\n",
      "18.0    4179\n",
      "2.0     4179\n",
      "9.0     4179\n",
      "3.0     4179\n",
      "4.0     4179\n",
      "5.0     4179\n",
      "6.0     4179\n",
      "7.0     4179\n",
      "8.0     4179\n",
      "10.0    4179\n",
      "17.0    4179\n",
      "11.0    4179\n",
      "12.0    4179\n",
      "13.0    4179\n",
      "14.0    4179\n",
      "15.0    4179\n",
      "16.0    4179\n",
      "35.0    4179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_campaign_first_sale':\n",
      "item_campaign_first_sale\n",
      "0     4179\n",
      "1     4179\n",
      "20    4179\n",
      "21    4179\n",
      "22    4179\n",
      "23    4179\n",
      "24    4179\n",
      "25    4179\n",
      "26    4179\n",
      "27    4179\n",
      "28    4179\n",
      "29    4179\n",
      "30    4179\n",
      "31    4179\n",
      "32    4179\n",
      "33    4179\n",
      "34    4179\n",
      "19    4179\n",
      "18    4179\n",
      "17    4179\n",
      "8     4179\n",
      "2     4179\n",
      "3     4179\n",
      "4     4179\n",
      "5     4179\n",
      "6     4179\n",
      "7     4179\n",
      "9     4179\n",
      "16    4179\n",
      "10    4179\n",
      "11    4179\n",
      "12    4179\n",
      "13    4179\n",
      "14    4179\n",
      "15    4179\n",
      "35    4179\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Số lượng giá trị của cột 'item_first_sale':\n",
      "item_first_sale\n",
      "0     4179\n",
      "1     4179\n",
      "20    4179\n",
      "21    4179\n",
      "22    4179\n",
      "23    4179\n",
      "24    4179\n",
      "25    4179\n",
      "26    4179\n",
      "27    4179\n",
      "28    4179\n",
      "29    4179\n",
      "30    4179\n",
      "31    4179\n",
      "32    4179\n",
      "33    4179\n",
      "34    4179\n",
      "19    4179\n",
      "18    4179\n",
      "17    4179\n",
      "8     4179\n",
      "2     4179\n",
      "3     4179\n",
      "4     4179\n",
      "5     4179\n",
      "6     4179\n",
      "7     4179\n",
      "9     4179\n",
      "16    4179\n",
      "10    4179\n",
      "11    4179\n",
      "12    4179\n",
      "13    4179\n",
      "14    4179\n",
      "15    4179\n",
      "35    4179\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print(f\"Số lượng giá trị của cột '{column}':\")\n",
    "    print(data[column].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "51060e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150444 entries, 0 to 150443\n",
      "Data columns (total 40 columns):\n",
      " #   Column                                Non-Null Count   Dtype  \n",
      "---  ------                                --------------   -----  \n",
      " 0   date_block_num                        150444 non-null  int32  \n",
      " 1   campaign_id                           150444 non-null  int32  \n",
      " 2   product_id                            150444 non-null  int32  \n",
      " 3   item_cnt_month                        150444 non-null  int64  \n",
      " 4   profit_month                          150444 non-null  int64  \n",
      " 5   name                                  150444 non-null  object \n",
      " 6   short_description                     150444 non-null  object \n",
      " 7   categories_id                         150444 non-null  int64  \n",
      " 8   categories_name                       150444 non-null  object \n",
      " 9   item_cnt_month_lag_1                  146265 non-null  float64\n",
      " 10  item_cnt_month_lag_2                  142086 non-null  float64\n",
      " 11  item_cnt_month_lag_3                  137907 non-null  float64\n",
      " 12  item_cnt_month_lag_6                  125370 non-null  float64\n",
      " 13  product_avg_sale_last_6               150444 non-null  float64\n",
      " 14  product_std_sale_last_6               150444 non-null  float64\n",
      " 15  product_avg_sale_last_12              150444 non-null  float64\n",
      " 16  product_std_sale_last_12              150444 non-null  float64\n",
      " 17  campaign_avg_sale_last_6              150444 non-null  float64\n",
      " 18  campaign_std_sale_last_6              150444 non-null  float64\n",
      " 19  campaign_avg_sale_last_12             150444 non-null  float64\n",
      " 20  campaign_std_sale_last_12             150444 non-null  float64\n",
      " 21  category_avg_sale_last_12             150444 non-null  float64\n",
      " 22  category_std_sale_last_12             150444 non-null  float64\n",
      " 23  date_avg_item_cnt_lag_1               146265 non-null  float16\n",
      " 24  date_campaign_avg_item_cnt_lag_1      146265 non-null  float16\n",
      " 25  date_campaign_avg_item_cnt_lag_2      142086 non-null  float16\n",
      " 26  date_campaign_avg_item_cnt_lag_3      137907 non-null  float16\n",
      " 27  date_campaign_avg_item_cnt_lag_6      125370 non-null  float16\n",
      " 28  date_item_avg_item_cnt_lag_1          146265 non-null  float16\n",
      " 29  date_item_avg_item_cnt_lag_2          142086 non-null  float16\n",
      " 30  date_item_avg_item_cnt_lag_3          137907 non-null  float16\n",
      " 31  date_item_avg_item_cnt_lag_6          125370 non-null  float16\n",
      " 32  date_campaign_cat_avg_item_cnt_lag_1  146265 non-null  float16\n",
      " 33  delta_price_lag                       150444 non-null  float16\n",
      " 34  delta_revenue_lag_1                   146265 non-null  float16\n",
      " 35  month                                 150444 non-null  int32  \n",
      " 36  year                                  150444 non-null  int8   \n",
      " 37  item_last_sale                        146265 non-null  float64\n",
      " 38  item_campaign_first_sale              150444 non-null  int32  \n",
      " 39  item_first_sale                       150444 non-null  int32  \n",
      "dtypes: float16(12), float64(15), int32(6), int64(3), int8(1), object(3)\n",
      "memory usage: 31.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c9b9c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Khởi tạo LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Áp dụng LabelEncoder cho từng trường dữ liệu cần được mã hóa\n",
    "for column in ['name', 'short_description', 'categories_name']:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "# Xác định cột chứa giá trị NaN hoặc inf trong X_train\n",
    "invalid_columns = np.unique(np.where(~np.isfinite(data))[1])\n",
    "invalid_column_names = data.columns[invalid_columns]\n",
    "# Chuyển các cột sang dạng số\n",
    "data[invalid_column_names] = data[invalid_column_names].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3004f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển các giá trị inf thành nan\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Thay thế các giá trị nan bằng giá trị trung bình của cột tương ứng\n",
    "data.fillna(data.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5730f394",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[data.date_block_num < 25].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = data[data.date_block_num < 25]['item_cnt_month']\n",
    "X_valid = data[data.date_block_num == 25].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = data[data.date_block_num == 25]['item_cnt_month']\n",
    "X_test = data[data.date_block_num == 26].drop(['item_cnt_month'], axis=1)\n",
    "Y_test01 = data[data.date_block_num == 26]['item_cnt_month']\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bffd0703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=10, micro=11, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aad562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Tiếp tục với việc xử lý dữ liệu và huấn luyện mô hình\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0aa74676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Met categorical feature which contains sparse values. Consider renumbering to consecutive integers started from zero\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Debug] Dataset::GetMultiBinFromAllFeatures: sparse rate 0.069687\n",
      "[LightGBM] [Debug] init for col-wise cost 0.000031 seconds, init for row-wise cost 0.031754 seconds\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.045007 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7104\n",
      "[LightGBM] [Info] Number of data points in the train set: 100296, number of used features: 39\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Start training from score 53.233519\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 11 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 12 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 11 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 12 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 12 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 12 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 12 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 7\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 24 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 23 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 13 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 11 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 19 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 14 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 22 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 27 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 21 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 20 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 25 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 15 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 17 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 18 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 28 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 26 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 29 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 12 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 10 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 30 and depth = 8\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 31 and depth = 8\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Debug] Trained a tree with leaves = 16 and depth = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.951027870178223"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xác định các đặc trưng phân loại trong dữ liệu\n",
    "cat_feats = [feat for feat in X_train.columns if 'cat' in feat]\n",
    "\n",
    "# Định nghĩa các tham số của mô hình\n",
    "params = {\n",
    "    'max_depth': 8,\n",
    "    'n_estimators': 500,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'min_child_weight': 300,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1,\n",
    "    'random_state': 42,\n",
    "    'verbose': 10  # Set verbosity level here\n",
    "}\n",
    "\n",
    "# Khởi tạo mô hình LightGBM\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "ts = time.time()\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    categorical_feature=cat_feats,\n",
    ")\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "170900ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán kết quả trên tập dữ liệu validation và giới hạn giá trị trong khoảng từ 0 đến 20\n",
    "Y_pred = model.predict(X_valid)\n",
    "\n",
    "# Dự đoán kết quả trên tập dữ liệu test và giới hạn giá trị trong khoảng từ 0 đến 20\n",
    "Y_test = model.predict(X_test)\n",
    "\n",
    "# Tạo DataFrame cho kết quả dự đoán trên tập dữ liệu validation và lưu vào file CSV\n",
    "X_train_level2 = pd.DataFrame({\n",
    "    \"ID\": np.arange(Y_pred.shape[0]),  # Tạo cột ID từ 0 đến số lượng mẫu dự đoán trên tập validation\n",
    "    \"item_cnt_month\": Y_pred  # Kết quả dự đoán trên tập validation\n",
    "})\n",
    "X_train_level2.to_csv('lgb_valid.csv', index=False)  # Lưu DataFrame vào file CSV\n",
    "\n",
    "# Tạo DataFrame cho kết quả dự đoán trên tập dữ liệu test và lưu vào file CSV\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": np.arange(Y_test.shape[0]),  # Tạo cột ID từ 0 đến số lượng mẫu dự đoán trên tập test\n",
    "    \"item_cnt_month\": Y_test  # Kết quả dự đoán trên tập test\n",
    "})\n",
    "submission.to_csv('lgb_submission.csv', index=False)  # Lưu DataFrame vào file CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "468f1597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chỉ mục của các giá trị không hợp lệ trong X_train: (array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Kiểm tra dữ liệu của mảng X_train\n",
    "invalid_indices = np.where(~np.isfinite(X_train))\n",
    "\n",
    "# In ra chỉ mục của các giá trị không hợp lệ\n",
    "print(\"Chỉ mục của các giá trị không hợp lệ trong X_train:\", invalid_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74be15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuyển các giá trị inf thành nan\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Thay thế các giá trị nan bằng giá trị trung bình của cột tương ứng\n",
    "X_train.fillna(X_train.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e63b16ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:64.65817\tvalidation_1-rmse:119.88065\n",
      "[10]\tvalidation_0-rmse:27.51665\tvalidation_1-rmse:58.15283\n",
      "[20]\tvalidation_0-rmse:16.93199\tvalidation_1-rmse:40.71232\n",
      "[30]\tvalidation_0-rmse:14.12948\tvalidation_1-rmse:35.55036\n",
      "[40]\tvalidation_0-rmse:13.30389\tvalidation_1-rmse:33.77297\n",
      "[50]\tvalidation_0-rmse:12.63334\tvalidation_1-rmse:32.38466\n",
      "[60]\tvalidation_0-rmse:12.22759\tvalidation_1-rmse:32.02207\n",
      "[70]\tvalidation_0-rmse:11.78597\tvalidation_1-rmse:31.55808\n",
      "[80]\tvalidation_0-rmse:11.54885\tvalidation_1-rmse:31.37524\n",
      "[90]\tvalidation_0-rmse:11.02913\tvalidation_1-rmse:30.66671\n",
      "[100]\tvalidation_0-rmse:10.62778\tvalidation_1-rmse:30.47524\n",
      "[110]\tvalidation_0-rmse:10.26341\tvalidation_1-rmse:30.39639\n",
      "[120]\tvalidation_0-rmse:9.95449\tvalidation_1-rmse:30.11321\n",
      "[130]\tvalidation_0-rmse:9.66775\tvalidation_1-rmse:29.86210\n",
      "[140]\tvalidation_0-rmse:9.42230\tvalidation_1-rmse:29.68614\n",
      "[150]\tvalidation_0-rmse:9.15232\tvalidation_1-rmse:29.70993\n",
      "[160]\tvalidation_0-rmse:8.90444\tvalidation_1-rmse:29.55299\n",
      "[170]\tvalidation_0-rmse:8.70265\tvalidation_1-rmse:29.43785\n",
      "[180]\tvalidation_0-rmse:8.50104\tvalidation_1-rmse:29.22874\n",
      "[190]\tvalidation_0-rmse:8.35858\tvalidation_1-rmse:29.20118\n",
      "[200]\tvalidation_0-rmse:8.17350\tvalidation_1-rmse:29.14879\n",
      "[210]\tvalidation_0-rmse:8.10850\tvalidation_1-rmse:29.16823\n",
      "[220]\tvalidation_0-rmse:7.94115\tvalidation_1-rmse:29.17769\n",
      "[230]\tvalidation_0-rmse:7.78105\tvalidation_1-rmse:29.20348\n",
      "[231]\tvalidation_0-rmse:7.76018\tvalidation_1-rmse:29.23675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.11213779449463"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "sys.version_info\n",
    "# Đo thời gian bắt đầu thực thi của đoạn mã.\n",
    "ts = time.time()\n",
    "\n",
    "# Khởi tạo mô hình XGBoost Regressor với các tham số đã được cấu hình trước.\n",
    "model = XGBRegressor(\n",
    "    max_depth=7,                # Độ sâu tối đa của cây\n",
    "    n_estimators=1000,          # Số lượng cây trong mô hình\n",
    "    min_child_weight=300,       # Trọng số tối thiểu của mỗi nút lá\n",
    "    colsample_bytree=0.8,       # Tỉ lệ số cột được sử dụng cho mỗi cây\n",
    "    subsample=0.8,              # Tỉ lệ số mẫu được sử dụng cho mỗi cây\n",
    "    gamma=0.005,                # Giảm số lượng các nút sau mỗi bước phát triển\n",
    "    eta=0.1,                    # Tốc độ học của mô hình\n",
    "    seed=42                     # Seed cho random state\n",
    ")\n",
    "\n",
    "# Huấn luyện mô hình XGBoost với dữ liệu huấn luyện và đánh giá trên tập validation.\n",
    "model.fit(\n",
    "    X_train,                                # Dữ liệu huấn luyện\n",
    "    Y_train,                                # Nhãn của dữ liệu huấn luyện\n",
    "    eval_metric=\"rmse\",                     # Đánh giá mô hình bằng Root Mean Squared Error (RMSE)\n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)],  # Sử dụng tập huấn luyện và tập validation để đánh giá\n",
    "    verbose=10,                             # Hiển thị thông tin về quá trình huấn luyện sau mỗi lượt huấn luyện\n",
    "    early_stopping_rounds=40                # Dừng sớm quá trình huấn luyện nếu không có sự cải thiện trong 40 vòng lặp liên tiếp trên tập validation\n",
    ")\n",
    "\n",
    "# Tính và in ra thời gian thực thi của đoạn mã.\n",
    "time.time() - ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f01eaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in X_train: Index(['date_block_num', 'campaign_id', 'product_id', 'profit_month', 'name',\n",
      "       'short_description', 'categories_id', 'categories_name',\n",
      "       'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
      "       'item_cnt_month_lag_6', 'product_avg_sale_last_6',\n",
      "       'product_std_sale_last_6', 'product_avg_sale_last_12',\n",
      "       'product_std_sale_last_12', 'campaign_avg_sale_last_6',\n",
      "       'campaign_std_sale_last_6', 'campaign_avg_sale_last_12',\n",
      "       'campaign_std_sale_last_12', 'category_avg_sale_last_12',\n",
      "       'category_std_sale_last_12', 'date_avg_item_cnt_lag_1',\n",
      "       'date_campaign_avg_item_cnt_lag_1', 'date_campaign_avg_item_cnt_lag_2',\n",
      "       'date_campaign_avg_item_cnt_lag_3', 'date_campaign_avg_item_cnt_lag_6',\n",
      "       'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2',\n",
      "       'date_item_avg_item_cnt_lag_3', 'date_item_avg_item_cnt_lag_6',\n",
      "       'date_campaign_cat_avg_item_cnt_lag_1', 'delta_price_lag',\n",
      "       'delta_revenue_lag_1', 'month', 'year', 'item_last_sale',\n",
      "       'item_campaign_first_sale', 'item_first_sale'],\n",
      "      dtype='object')\n",
      "Columns in X_test: Index(['date_block_num', 'campaign_id', 'product_id', 'profit_month', 'name',\n",
      "       'short_description', 'categories_id', 'categories_name',\n",
      "       'item_cnt_month_lag_1', 'item_cnt_month_lag_2', 'item_cnt_month_lag_3',\n",
      "       'item_cnt_month_lag_6', 'product_avg_sale_last_6',\n",
      "       'product_std_sale_last_6', 'product_avg_sale_last_12',\n",
      "       'product_std_sale_last_12', 'campaign_avg_sale_last_6',\n",
      "       'campaign_std_sale_last_6', 'campaign_avg_sale_last_12',\n",
      "       'campaign_std_sale_last_12', 'category_avg_sale_last_12',\n",
      "       'category_std_sale_last_12', 'date_avg_item_cnt_lag_1',\n",
      "       'date_campaign_avg_item_cnt_lag_1', 'date_campaign_avg_item_cnt_lag_2',\n",
      "       'date_campaign_avg_item_cnt_lag_3', 'date_campaign_avg_item_cnt_lag_6',\n",
      "       'date_item_avg_item_cnt_lag_1', 'date_item_avg_item_cnt_lag_2',\n",
      "       'date_item_avg_item_cnt_lag_3', 'date_item_avg_item_cnt_lag_6',\n",
      "       'date_campaign_cat_avg_item_cnt_lag_1', 'delta_price_lag',\n",
      "       'delta_revenue_lag_1', 'month', 'year', 'item_last_sale',\n",
      "       'item_campaign_first_sale', 'item_first_sale'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Liệt kê các cột trong X_train và X_test\n",
    "print(\"Columns in X_train:\", X_train.columns)\n",
    "print(\"Columns in X_test:\", X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0886bb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import KFold\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from lightgbm import early_stopping\n",
    "\n",
    "# # Định nghĩa mô hình LightGBM và XGBoost\n",
    "# params_lgb = {\n",
    "#     'max_depth': 8,\n",
    "#     'n_estimators': 500,\n",
    "#     'colsample_bytree': 0.7,\n",
    "#     'min_child_weight': 300,\n",
    "#     'reg_alpha': 0.1,\n",
    "#     'reg_lambda': 1,\n",
    "#     'random_state': 42,\n",
    "#     'verbosity': 1  # Cấu hình đúng cho LightGBM là verbosity\n",
    "# }\n",
    "\n",
    "# params_xgb = {\n",
    "#     'max_depth': 7,\n",
    "#     'n_estimators': 1000,\n",
    "#     'min_child_weight': 300,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'subsample': 0.8,\n",
    "#     'gamma': 0.005,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'seed': 42,\n",
    "#     'verbosity': 1  # Thêm verbosity cho XGBoost\n",
    "# }\n",
    "\n",
    "# # Khởi tạo K-Fold cross-validation\n",
    "# n_folds = 5\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # Chuẩn bị mảng để lưu trữ dự đoán cho tập valid và test\n",
    "# valid_preds_lgb = np.zeros(X_valid.shape[0])\n",
    "# valid_preds_xgb = np.zeros(X_valid.shape[0])\n",
    "# test_preds_lgb = np.zeros(X_test.shape[0])\n",
    "# test_preds_xgb = np.zeros(X_test.shape[0])\n",
    "\n",
    "# # Huấn luyện mô hình và tạo dự đoán cho tập valid và test\n",
    "# for train_index, valid_index in kf.split(X_train):\n",
    "#     X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "#     y_tr, y_val = Y_train.iloc[train_index], Y_train.iloc[valid_index]\n",
    "\n",
    "#     # LightGBM\n",
    "#     model_lgb = LGBMRegressor(**params_lgb)\n",
    "#     model_lgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[early_stopping(stopping_rounds=100)])\n",
    "#     valid_preds_lgb += model_lgb.predict(X_valid) / n_folds\n",
    "#     test_preds_lgb += model_lgb.predict(X_test) / n_folds\n",
    "\n",
    "#     # XGBoost\n",
    "#     model_xgb = XGBRegressor(**params_xgb)\n",
    "#     model_xgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=40)\n",
    "#     valid_preds_xgb += model_xgb.predict(X_valid) / n_folds\n",
    "#     test_preds_xgb += model_xgb.predict(X_test) / n_folds\n",
    "\n",
    "# # Tạo DataFrame cho các dự đoán từ các mô hình base trên tập valid và test\n",
    "# valid_level2 = pd.DataFrame({'lgb': valid_preds_lgb, 'xgb': valid_preds_xgb})\n",
    "# test_level2 = pd.DataFrame({'lgb': test_preds_lgb, 'xgb': test_preds_xgb})\n",
    "\n",
    "# # Huấn luyện mô hình meta với XGBoost trên tập valid\n",
    "# meta_params = {\n",
    "#     'max_depth': 5,\n",
    "#     'n_estimators': 100,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'subsample': 0.7,\n",
    "#     'colsample_bytree': 0.7,\n",
    "#     'random_state': 42\n",
    "# }\n",
    "\n",
    "# model_meta = XGBRegressor(**meta_params)\n",
    "# model_meta.fit(valid_level2, Y_valid)\n",
    "\n",
    "# # Dự đoán trên tập valid và test sử dụng mô hình meta\n",
    "# final_preds_valid = model_meta.predict(valid_level2)\n",
    "# final_preds_test = model_meta.predict(test_level2)\n",
    "\n",
    "# # Đánh giá mô hình trên tập valid\n",
    "# rmse = np.sqrt(mean_squared_error(Y_valid, final_preds_valid))\n",
    "# print('Stacking Model RMSE on Validation data:', rmse)\n",
    "\n",
    "# # Lưu kết quả cuối cùng vào CSV\n",
    "# submission = pd.DataFrame({\n",
    "#     'ID': np.arange(final_preds_test.shape[0]),\n",
    "#     'item_cnt_month': final_preds_test\n",
    "# })\n",
    "# submission.to_csv('stacked_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc742df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.57752\tvalid_0's l2: 43.2638\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.6338\tvalid_0's l2: 44.0073\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.96657\tvalid_0's l2: 35.6\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.35337\tvalid_0's l2: 28.6586\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.37916\tvalid_0's l2: 40.6937\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 9.59275\tvalid_0's l2: 92.0208\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 9.51878\tvalid_0's l2: 90.6071\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.98367\tvalid_0's l2: 80.7063\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.15546\tvalid_0's l2: 66.5115\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 9.26956\tvalid_0's l2: 85.9247\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.36654\tvalid_0's l2: 54.266\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.32564\tvalid_0's l2: 53.665\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.75852\tvalid_0's l2: 45.6776\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.13331\tvalid_0's l2: 37.6175\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.24992\tvalid_0's l2: 52.5614\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 8.32195\tvalid_0's l2: 69.2549\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 8.181\tvalid_0's l2: 66.9287\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.66031\tvalid_0's l2: 58.6804\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.90909\tvalid_0's l2: 47.7355\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 8.01207\tvalid_0's l2: 64.1933\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.92402\tvalid_0's l2: 47.942\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.81161\tvalid_0's l2: 46.398\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.05466\tvalid_0's l2: 36.6589\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.51036\tvalid_0's l2: 30.364\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.59818\tvalid_0's l2: 43.536\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.65791\tvalid_0's l2: 32.0119\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.80237\tvalid_0's l2: 33.6675\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.8132\tvalid_0's l2: 23.1669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.28016\tvalid_0's l2: 18.3198\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.29715\tvalid_0's l2: 28.0598\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.62517\tvalid_0's l2: 31.6425\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.59516\tvalid_0's l2: 31.3058\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.78666\tvalid_0's l2: 22.9121\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.18975\tvalid_0's l2: 17.554\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.24996\tvalid_0's l2: 27.5621\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.00902\tvalid_0's l2: 36.1083\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.9572\tvalid_0's l2: 35.4882\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.26596\tvalid_0's l2: 27.7303\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 4.70093\tvalid_0's l2: 22.0987\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.77683\tvalid_0's l2: 33.3718\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.72674\tvalid_0's l2: 59.7026\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.91274\tvalid_0's l2: 62.6115\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.15962\tvalid_0's l2: 51.2601\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.45331\tvalid_0's l2: 41.6452\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.57537\tvalid_0's l2: 57.3862\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.04706\tvalid_0's l2: 36.567\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.24317\tvalid_0's l2: 38.9771\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.56324\tvalid_0's l2: 30.9496\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 4.75314\tvalid_0's l2: 22.5924\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.82854\tvalid_0's l2: 33.9719\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 7.77013\tvalid_0's l2: 60.3749\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 7.98393\tvalid_0's l2: 63.7432\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 7.20764\tvalid_0's l2: 51.9501\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 6.44995\tvalid_0's l2: 41.6019\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 7.78001\tvalid_0's l2: 60.5286\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.27406\tvalid_0's l2: 39.3638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.34597\tvalid_0's l2: 40.2714\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.56359\tvalid_0's l2: 30.9536\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.89559\tvalid_0's l2: 23.9668\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.02567\tvalid_0's l2: 36.3087\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.84012\tvalid_0's l2: 46.7872\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.15887\tvalid_0's l2: 51.2495\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.96369\tvalid_0's l2: 35.5656\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.60231\tvalid_0's l2: 31.3859\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.36256\tvalid_0's l2: 40.4822\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 9.97566\tvalid_0's l2: 99.5138\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 10.0696\tvalid_0's l2: 101.396\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.81622\tvalid_0's l2: 77.7258\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.25545\tvalid_0's l2: 68.1524\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 9.375\tvalid_0's l2: 87.8906\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 9.33422\tvalid_0's l2: 87.1276\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 9.32688\tvalid_0's l2: 86.9908\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 8.31645\tvalid_0's l2: 69.1633\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.74978\tvalid_0's l2: 60.059\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 9.13204\tvalid_0's l2: 83.3942\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.42706\tvalid_0's l2: 55.1612\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.50428\tvalid_0's l2: 56.3142\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.80297\tvalid_0's l2: 46.2804\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.23592\tvalid_0's l2: 38.8868\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.4043\tvalid_0's l2: 54.8236\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.05766\tvalid_0's l2: 25.5799\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.32951\tvalid_0's l2: 28.4037\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.6644\tvalid_0's l2: 21.7566\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.01585\tvalid_0's l2: 16.127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.97903\tvalid_0's l2: 24.7908\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 10.6491\tvalid_0's l2: 113.404\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 10.5513\tvalid_0's l2: 111.33\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 9.64589\tvalid_0's l2: 93.0432\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 8.95053\tvalid_0's l2: 80.1119\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 10.1962\tvalid_0's l2: 103.962\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 10.8157\tvalid_0's l2: 116.979\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 10.8282\tvalid_0's l2: 117.249\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 9.99317\tvalid_0's l2: 99.8635\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 9.40937\tvalid_0's l2: 88.5362\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 10.4958\tvalid_0's l2: 110.163\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 16.7523\tvalid_0's l2: 280.64\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 16.1317\tvalid_0's l2: 260.231\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 15.7189\tvalid_0's l2: 247.084\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 14.8077\tvalid_0's l2: 219.268\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 15.8987\tvalid_0's l2: 252.769\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.65718\tvalid_0's l2: 21.6893\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.74778\tvalid_0's l2: 22.5415\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.0829\tvalid_0's l2: 16.6701\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.56478\tvalid_0's l2: 12.7077\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.49679\tvalid_0's l2: 20.2211\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.79447\tvalid_0's l2: 22.9869\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.81419\tvalid_0's l2: 23.1765\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.22833\tvalid_0's l2: 17.8788\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.5106\tvalid_0's l2: 12.3243\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.40417\tvalid_0's l2: 19.3967\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.79797\tvalid_0's l2: 23.0206\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.79018\tvalid_0's l2: 22.9458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.16723\tvalid_0's l2: 17.3658\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.545\tvalid_0's l2: 12.567\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.42325\tvalid_0's l2: 19.5652\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.80538\tvalid_0's l2: 23.0917\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.88606\tvalid_0's l2: 23.8736\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.11035\tvalid_0's l2: 16.8949\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.567\tvalid_0's l2: 12.7235\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.41343\tvalid_0's l2: 19.4784\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 4.91818\tvalid_0's l2: 24.1885\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.0461\tvalid_0's l2: 25.4631\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 4.34277\tvalid_0's l2: 18.8597\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 3.6804\tvalid_0's l2: 13.5453\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 4.51069\tvalid_0's l2: 20.3463\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 6.97282\tvalid_0's l2: 48.6202\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 6.9559\tvalid_0's l2: 48.3845\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 6.37693\tvalid_0's l2: 40.6653\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.77866\tvalid_0's l2: 33.3929\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 6.77875\tvalid_0's l2: 45.9515\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.87318\tvalid_0's l2: 23.7479\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.91374\tvalid_0's l2: 24.1448\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[999]\tvalid_0's rmse: 4.42402\tvalid_0's l2: 19.572\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.86542\tvalid_0's l2: 14.9415\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.43506\tvalid_0's l2: 19.6697\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[999]\tvalid_0's rmse: 9.02684\tvalid_0's l2: 81.4838\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 8.95737\tvalid_0's l2: 80.2346\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.91422\tvalid_0's l2: 62.6349\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.4104\tvalid_0's l2: 54.9141\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 8.26478\tvalid_0's l2: 68.3066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.42792\tvalid_0's l2: 29.4623\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.71573\tvalid_0's l2: 32.6696\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.09367\tvalid_0's l2: 25.9455\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.64172\tvalid_0's l2: 21.5455\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.31724\tvalid_0's l2: 28.2731\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 8.00504\tvalid_0's l2: 64.0806\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 7.99811\tvalid_0's l2: 63.9697\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 6.61349\tvalid_0's l2: 43.7382\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 6.36722\tvalid_0's l2: 40.5415\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 7.75271\tvalid_0's l2: 60.1046\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.063\tvalid_0's l2: 25.634\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.07224\tvalid_0's l2: 25.7276\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.30033\tvalid_0's l2: 18.4929\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 3.98542\tvalid_0's l2: 15.8836\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.55144\tvalid_0's l2: 20.7156\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.06637\tvalid_0's l2: 36.8008\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.43473\tvalid_0's l2: 41.4057\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.68893\tvalid_0's l2: 32.3639\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.00139\tvalid_0's l2: 25.0139\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.00094\tvalid_0's l2: 36.0112\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 11.1158\tvalid_0's l2: 123.561\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 10.8086\tvalid_0's l2: 116.825\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 10.4403\tvalid_0's l2: 109.001\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 9.56576\tvalid_0's l2: 91.5039\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 10.8225\tvalid_0's l2: 117.126\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.45755\tvalid_0's l2: 71.5302\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.9712\tvalid_0's l2: 80.4825\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 7.76208\tvalid_0's l2: 60.2498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 7.08374\tvalid_0's l2: 50.1794\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.2796\tvalid_0's l2: 68.5517\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.53032\tvalid_0's l2: 30.5844\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.43904\tvalid_0's l2: 29.5831\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.98369\tvalid_0's l2: 24.8372\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.34551\tvalid_0's l2: 18.8834\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.10382\tvalid_0's l2: 26.049\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 8.14311\tvalid_0's l2: 66.3103\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.97936\tvalid_0's l2: 63.6702\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.47898\tvalid_0's l2: 55.9351\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.83586\tvalid_0's l2: 46.729\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.79255\tvalid_0's l2: 60.7239\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 9.51955\tvalid_0's l2: 90.6218\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 9.60005\tvalid_0's l2: 92.1609\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.60235\tvalid_0's l2: 74.0004\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 8.0591\tvalid_0's l2: 64.9492\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 9.09103\tvalid_0's l2: 82.6469\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 9.22979\tvalid_0's l2: 85.189\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 9.28199\tvalid_0's l2: 86.1553\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 8.35735\tvalid_0's l2: 69.8452\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.57465\tvalid_0's l2: 57.3753\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 8.67047\tvalid_0's l2: 75.177\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 13.2378\tvalid_0's l2: 175.238\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.6543\tvalid_0's l2: 160.131\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.0517\tvalid_0's l2: 145.244\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 10.6791\tvalid_0's l2: 114.042\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.0909\tvalid_0's l2: 146.191\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.08618\tvalid_0's l2: 25.8692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.05304\tvalid_0's l2: 25.5333\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 4.38951\tvalid_0's l2: 19.2678\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[699]\tvalid_0's rmse: 3.86081\tvalid_0's l2: 14.9059\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 4.63572\tvalid_0's l2: 21.4899\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.36691\tvalid_0's l2: 28.8037\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.63422\tvalid_0's l2: 31.7444\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.03245\tvalid_0's l2: 25.3256\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.64845\tvalid_0's l2: 21.6081\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.4318\tvalid_0's l2: 29.5044\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 7.10782\tvalid_0's l2: 50.521\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 7.10692\tvalid_0's l2: 50.5084\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 6.49152\tvalid_0's l2: 42.1398\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.94689\tvalid_0's l2: 35.3655\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 7.002\tvalid_0's l2: 49.028\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 6.7596\tvalid_0's l2: 45.6921\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 6.96618\tvalid_0's l2: 48.5277\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 6.10145\tvalid_0's l2: 37.2277\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 5.38174\tvalid_0's l2: 28.9632\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 6.41655\tvalid_0's l2: 41.1721\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.29994\tvalid_0's l2: 39.6892\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.50802\tvalid_0's l2: 42.3543\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.79295\tvalid_0's l2: 33.5583\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.19459\tvalid_0's l2: 26.9838\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.13985\tvalid_0's l2: 37.6977\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.46232\tvalid_0's l2: 41.7616\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.34778\tvalid_0's l2: 40.2944\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.51529\tvalid_0's l2: 30.4184\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 4.99461\tvalid_0's l2: 24.9462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.90113\tvalid_0's l2: 34.8234\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.05455\tvalid_0's l2: 36.6576\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.77693\tvalid_0's l2: 33.3729\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.1535\tvalid_0's l2: 26.5585\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 4.64246\tvalid_0's l2: 21.5525\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.43503\tvalid_0's l2: 29.5395\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.31137\tvalid_0's l2: 39.8334\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.45365\tvalid_0's l2: 41.6496\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.84982\tvalid_0's l2: 34.2204\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.33504\tvalid_0's l2: 28.4627\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.26471\tvalid_0's l2: 39.2466\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.81369\tvalid_0's l2: 46.4264\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.88704\tvalid_0's l2: 47.4314\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.03992\tvalid_0's l2: 36.4806\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.35038\tvalid_0's l2: 28.6266\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.47506\tvalid_0's l2: 41.9265\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 6.47549\tvalid_0's l2: 41.9319\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 6.65582\tvalid_0's l2: 44.2999\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 6.06013\tvalid_0's l2: 36.7252\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.36566\tvalid_0's l2: 28.7903\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 6.3702\tvalid_0's l2: 40.5795\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.23813\tvalid_0's l2: 52.3905\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.60095\tvalid_0's l2: 57.7745\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.64536\tvalid_0's l2: 44.1609\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.18404\tvalid_0's l2: 38.2423\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.15248\tvalid_0's l2: 51.158\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.67104\tvalid_0's l2: 32.1608\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.58433\tvalid_0's l2: 31.1847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.92297\tvalid_0's l2: 24.2357\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 4.3165\tvalid_0's l2: 18.6322\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.28806\tvalid_0's l2: 27.9636\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 16.293\tvalid_0's l2: 265.462\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 15.4523\tvalid_0's l2: 238.774\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 15.3829\tvalid_0's l2: 236.635\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 14.3633\tvalid_0's l2: 206.305\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 15.5281\tvalid_0's l2: 241.123\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.79957\tvalid_0's l2: 33.635\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.93306\tvalid_0's l2: 35.2012\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 4.99904\tvalid_0's l2: 24.9904\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 4.66852\tvalid_0's l2: 21.7951\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.55124\tvalid_0's l2: 30.8163\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 8.03053\tvalid_0's l2: 64.4894\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 8.02074\tvalid_0's l2: 64.3322\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.3195\tvalid_0's l2: 53.5751\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.77237\tvalid_0's l2: 45.865\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 7.9232\tvalid_0's l2: 62.7771\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 9.15893\tvalid_0's l2: 83.8859\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 8.69368\tvalid_0's l2: 75.58\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 7.98493\tvalid_0's l2: 63.7591\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 7.10856\tvalid_0's l2: 50.5317\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 8.41976\tvalid_0's l2: 70.8924\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.67987\tvalid_0's l2: 32.2609\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.66141\tvalid_0's l2: 32.0516\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 4.95458\tvalid_0's l2: 24.5479\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 4.40388\tvalid_0's l2: 19.3941\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.05642\tvalid_0's l2: 25.5674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.69534\tvalid_0's l2: 44.8276\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.69311\tvalid_0's l2: 44.7978\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.08226\tvalid_0's l2: 36.9938\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.57438\tvalid_0's l2: 31.0737\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.5748\tvalid_0's l2: 43.228\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.72547\tvalid_0's l2: 32.781\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.95906\tvalid_0's l2: 35.5104\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.33579\tvalid_0's l2: 28.4707\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.8312\tvalid_0's l2: 23.3404\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.64115\tvalid_0's l2: 31.8226\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.0947\tvalid_0's l2: 25.956\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.17909\tvalid_0's l2: 26.823\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.47708\tvalid_0's l2: 20.0443\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.04405\tvalid_0's l2: 16.3543\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.86967\tvalid_0's l2: 23.7137\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 10.9765\tvalid_0's l2: 120.483\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 11.2686\tvalid_0's l2: 126.982\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 10.0903\tvalid_0's l2: 101.815\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 9.31269\tvalid_0's l2: 86.7261\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 10.584\tvalid_0's l2: 112.02\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.00591\tvalid_0's l2: 25.0591\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.29901\tvalid_0's l2: 28.0795\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 4.43291\tvalid_0's l2: 19.6507\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 3.89586\tvalid_0's l2: 15.1777\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[699]\tvalid_0's rmse: 4.64597\tvalid_0's l2: 21.5851\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 11.0435\tvalid_0's l2: 121.959\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 10.9252\tvalid_0's l2: 119.361\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 10.3001\tvalid_0's l2: 106.092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 9.59059\tvalid_0's l2: 91.9794\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 10.8787\tvalid_0's l2: 118.346\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.99212\tvalid_0's l2: 35.9055\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.4186\tvalid_0's l2: 41.1984\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.1494\tvalid_0's l2: 26.5163\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 4.58374\tvalid_0's l2: 21.0107\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.54139\tvalid_0's l2: 30.7071\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 8.05849\tvalid_0's l2: 64.9393\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.95375\tvalid_0's l2: 63.2622\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.33422\tvalid_0's l2: 53.7907\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.67511\tvalid_0's l2: 44.5571\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.93198\tvalid_0's l2: 62.9163\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.65574\tvalid_0's l2: 21.676\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.83675\tvalid_0's l2: 23.3941\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.07438\tvalid_0's l2: 16.6006\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.59898\tvalid_0's l2: 12.9527\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.394\tvalid_0's l2: 19.3072\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.69737\tvalid_0's l2: 22.0653\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.77485\tvalid_0's l2: 22.7992\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.1747\tvalid_0's l2: 17.4281\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.52398\tvalid_0's l2: 12.4184\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.40585\tvalid_0's l2: 19.4115\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.64001\tvalid_0's l2: 21.5297\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.90205\tvalid_0's l2: 24.0301\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.04797\tvalid_0's l2: 16.386\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.47421\tvalid_0's l2: 12.0701\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.43406\tvalid_0's l2: 19.6609\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.61814\tvalid_0's l2: 21.3272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.93455\tvalid_0's l2: 24.3498\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.05326\tvalid_0's l2: 16.4289\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.46018\tvalid_0's l2: 11.9728\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.44508\tvalid_0's l2: 19.7587\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.72044\tvalid_0's l2: 22.2826\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.84323\tvalid_0's l2: 23.4569\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.11722\tvalid_0's l2: 16.9515\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.60209\tvalid_0's l2: 12.975\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.37609\tvalid_0's l2: 19.1502\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.61989\tvalid_0's l2: 21.3434\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.83606\tvalid_0's l2: 23.3875\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[999]\tvalid_0's rmse: 4.10103\tvalid_0's l2: 16.8184\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.52543\tvalid_0's l2: 12.4286\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.3445\tvalid_0's l2: 18.8746\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.64441\tvalid_0's l2: 21.5705\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.79343\tvalid_0's l2: 22.9769\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.08115\tvalid_0's l2: 16.6558\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.5415\tvalid_0's l2: 12.5422\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.37161\tvalid_0's l2: 19.111\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 9.08182\tvalid_0's l2: 82.4794\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 8.88566\tvalid_0's l2: 78.9549\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 7.83487\tvalid_0's l2: 61.3851\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 7.16012\tvalid_0's l2: 51.2673\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 8.47849\tvalid_0's l2: 71.8848\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.89404\tvalid_0's l2: 79.1039\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.7798\tvalid_0's l2: 77.085\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.74518\tvalid_0's l2: 59.9878\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.30355\tvalid_0's l2: 53.3419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.181\tvalid_0's l2: 66.9288\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.57555\tvalid_0's l2: 31.0867\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[999]\tvalid_0's rmse: 5.56446\tvalid_0's l2: 30.9632\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.86212\tvalid_0's l2: 23.6403\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.47977\tvalid_0's l2: 20.0684\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.41836\tvalid_0's l2: 29.3586\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.49845\tvalid_0's l2: 30.233\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.7726\tvalid_0's l2: 33.323\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.06479\tvalid_0's l2: 25.6521\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.58609\tvalid_0's l2: 21.0322\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.44764\tvalid_0's l2: 29.6768\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.111\tvalid_0's l2: 26.1223\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.297\tvalid_0's l2: 28.0582\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 4.26159\tvalid_0's l2: 18.1612\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 3.90349\tvalid_0's l2: 15.2372\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 4.80122\tvalid_0's l2: 23.0518\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.22217\tvalid_0's l2: 38.7153\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.58058\tvalid_0's l2: 43.3041\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.84605\tvalid_0's l2: 34.1764\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.21974\tvalid_0's l2: 27.2457\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.13639\tvalid_0's l2: 37.6553\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 11.7907\tvalid_0's l2: 139.02\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 11.797\tvalid_0's l2: 139.169\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 11.0009\tvalid_0's l2: 121.019\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 10.4941\tvalid_0's l2: 110.126\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 11.3982\tvalid_0's l2: 129.92\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.82084\tvalid_0's l2: 23.2405\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.00691\tvalid_0's l2: 25.0691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[998]\tvalid_0's rmse: 4.10275\tvalid_0's l2: 16.8326\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.64681\tvalid_0's l2: 13.2992\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.45508\tvalid_0's l2: 19.8478\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 9.83166\tvalid_0's l2: 96.6615\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 9.68949\tvalid_0's l2: 93.8862\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 8.96452\tvalid_0's l2: 80.3625\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 8.26962\tvalid_0's l2: 68.3866\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 9.37222\tvalid_0's l2: 87.8386\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.03358\tvalid_0's l2: 36.4041\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 6.26042\tvalid_0's l2: 39.1928\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.22972\tvalid_0's l2: 27.35\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 4.54202\tvalid_0's l2: 20.6299\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 5.60807\tvalid_0's l2: 31.4505\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.51294\tvalid_0's l2: 42.4184\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.83866\tvalid_0's l2: 46.7673\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.06944\tvalid_0's l2: 36.8381\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.47873\tvalid_0's l2: 30.0165\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.53827\tvalid_0's l2: 42.7489\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.39064\tvalid_0's l2: 40.8403\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.51862\tvalid_0's l2: 42.4924\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.77432\tvalid_0's l2: 33.3428\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 5.27541\tvalid_0's l2: 27.83\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.12027\tvalid_0's l2: 37.4577\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.18143\tvalid_0's l2: 26.8473\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.31988\tvalid_0's l2: 28.3011\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.76421\tvalid_0's l2: 22.6977\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.19455\tvalid_0's l2: 17.5943\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.23608\tvalid_0's l2: 27.4166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.96886\tvalid_0's l2: 80.4404\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 9.12071\tvalid_0's l2: 83.1873\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.96239\tvalid_0's l2: 63.3997\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.27432\tvalid_0's l2: 52.9157\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 8.14476\tvalid_0's l2: 66.3371\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 13.525\tvalid_0's l2: 182.925\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 13.1154\tvalid_0's l2: 172.013\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.3669\tvalid_0's l2: 152.941\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 11.5579\tvalid_0's l2: 133.585\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.9674\tvalid_0's l2: 168.154\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.81136\tvalid_0's l2: 61.0174\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.46337\tvalid_0's l2: 55.7019\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.10428\tvalid_0's l2: 50.4708\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.45457\tvalid_0's l2: 41.6614\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 7.55706\tvalid_0's l2: 57.1091\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.86774\tvalid_0's l2: 23.6949\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 5.00362\tvalid_0's l2: 25.0362\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.175\tvalid_0's l2: 17.4306\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 3.7003\tvalid_0's l2: 13.6922\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[900]\tvalid_0's rmse: 4.5356\tvalid_0's l2: 20.5717\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.926\tvalid_0's l2: 47.9695\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.89792\tvalid_0's l2: 47.5813\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.89372\tvalid_0's l2: 34.7359\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 5.65851\tvalid_0's l2: 32.0187\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[800]\tvalid_0's rmse: 6.46943\tvalid_0's l2: 41.8535\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.22735\tvalid_0's l2: 38.7798\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.39715\tvalid_0's l2: 40.9236\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.81738\tvalid_0's l2: 33.8419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.43112\tvalid_0's l2: 29.4971\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 6.21019\tvalid_0's l2: 38.5664\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.80965\tvalid_0's l2: 46.3713\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 7.08013\tvalid_0's l2: 50.1283\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.1333\tvalid_0's l2: 37.6174\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 5.55581\tvalid_0's l2: 30.867\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[400]\tvalid_0's rmse: 6.81649\tvalid_0's l2: 46.4646\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.69543\tvalid_0's l2: 22.047\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.98555\tvalid_0's l2: 24.8557\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.06501\tvalid_0's l2: 16.5243\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[999]\tvalid_0's rmse: 3.59071\tvalid_0's l2: 12.8932\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.35942\tvalid_0's l2: 19.0045\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.60955\tvalid_0's l2: 31.4671\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.6143\tvalid_0's l2: 31.5203\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.01285\tvalid_0's l2: 25.1287\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 4.33927\tvalid_0's l2: 18.8293\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[700]\tvalid_0's rmse: 5.44565\tvalid_0's l2: 29.6551\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 14.0341\tvalid_0's l2: 196.955\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 13.6035\tvalid_0's l2: 185.055\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 13.299\tvalid_0's l2: 176.863\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 12.3586\tvalid_0's l2: 152.734\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[300]\tvalid_0's rmse: 13.3543\tvalid_0's l2: 178.337\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.97743\tvalid_0's l2: 24.7748\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.87504\tvalid_0's l2: 23.766\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.3784\tvalid_0's l2: 19.1704\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 3.94184\tvalid_0's l2: 15.5381\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.50017\tvalid_0's l2: 20.2515\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 8.17616\tvalid_0's l2: 66.8495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.85786\tvalid_0's l2: 61.746\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.44349\tvalid_0's l2: 55.4055\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 6.71701\tvalid_0's l2: 45.1182\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[500]\tvalid_0's rmse: 7.81876\tvalid_0's l2: 61.1329\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.73193\tvalid_0's l2: 59.7828\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.93938\tvalid_0's l2: 63.0338\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 6.69033\tvalid_0's l2: 44.7605\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 6.29853\tvalid_0's l2: 39.6715\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[200]\tvalid_0's rmse: 7.42988\tvalid_0's l2: 55.2031\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.41361\tvalid_0's l2: 29.3071\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.5791\tvalid_0's l2: 31.1263\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 4.80351\tvalid_0's l2: 23.0737\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[999]\tvalid_0's rmse: 4.35451\tvalid_0's l2: 18.9618\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[1000]\tvalid_0's rmse: 5.16311\tvalid_0's l2: 26.6577\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.6783\tvalid_0's l2: 160.74\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.1977\tvalid_0's l2: 148.783\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.0146\tvalid_0's l2: 144.35\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 11.2025\tvalid_0's l2: 125.495\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[100]\tvalid_0's rmse: 12.0847\tvalid_0's l2: 146.04\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.25644\tvalid_0's l2: 39.1431\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.41135\tvalid_0's l2: 41.1054\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.83556\tvalid_0's l2: 34.0538\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 5.3034\tvalid_0's l2: 28.1261\n",
      "Training until validation scores don't improve for 100 rounds                                                          \n",
      "Did not meet early stopping. Best iteration is:                                                                        \n",
      "[600]\tvalid_0's rmse: 6.13556\tvalid_0's l2: 37.6451\n",
      "100%|███████████████████████████████████████████████| 100/100 [55:33<00:00, 33.33s/trial, best loss: 4.285381462456736]\n",
      "  0%|                                                                          | 0/100 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                             | 1/100 [00:40<1:06:06, 40.07s/trial, best loss: 13.754684940335661]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▉                                            | 2/100 [03:33<3:13:37, 118.54s/trial, best loss: 13.754684940335661]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|█▍                                            | 3/100 [04:05<2:07:59, 79.17s/trial, best loss: 13.754684940335661]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|█▉                                             | 4/100 [06:15<2:38:50, 99.28s/trial, best loss: 9.798955390113997]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|██▍                                             | 5/100 [07:53<2:36:08, 98.61s/trial, best loss: 9.30889687865552]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|██▉                                             | 6/100 [09:05<2:20:30, 89.68s/trial, best loss: 9.30889687865552]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|███▎                                            | 7/100 [10:19<2:11:07, 84.59s/trial, best loss: 9.30889687865552]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  8%|███▊                                            | 8/100 [10:36<1:36:32, 62.97s/trial, best loss: 9.30889687865552]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  9%|████▎                                           | 9/100 [10:59<1:16:27, 50.42s/trial, best loss: 9.30889687865552]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████▌                                         | 10/100 [13:57<2:14:45, 89.84s/trial, best loss: 5.176320389151881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 11%|█████                                         | 11/100 [14:32<1:48:18, 73.02s/trial, best loss: 5.176320389151881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 12%|█████▍                                       | 12/100 [18:16<2:54:25, 118.92s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█████▊                                       | 13/100 [23:46<4:25:15, 182.94s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 14%|██████▎                                      | 14/100 [28:57<5:17:50, 221.75s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|██████▊                                      | 15/100 [34:02<5:49:18, 246.57s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 16%|███████▏                                     | 16/100 [39:44<6:25:47, 275.56s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 17%|███████▋                                     | 17/100 [44:27<6:24:16, 277.79s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 18%|████████                                     | 18/100 [45:27<4:49:56, 212.15s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 19%|████████▌                                    | 19/100 [46:27<3:44:39, 166.42s/trial, best loss: 5.171540108595048]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|█████████                                    | 20/100 [56:00<6:24:41, 288.52s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 21%|█████████▍                                   | 21/100 [59:29<5:48:25, 264.63s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 22%|█████████▍                                 | 22/100 [1:03:32<5:35:35, 258.14s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 23%|█████████▉                                 | 23/100 [1:07:21<5:20:17, 249.58s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 24%|██████████▎                                | 24/100 [1:11:33<5:16:58, 250.24s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██████████▊                                | 25/100 [1:16:21<5:27:04, 261.66s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 26%|███████████▏                               | 26/100 [1:24:55<6:56:04, 337.36s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|███████████▌                               | 27/100 [1:33:20<7:51:38, 387.65s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 28%|████████████                               | 28/100 [1:37:50<7:02:49, 352.36s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 29%|████████████▍                              | 29/100 [1:45:02<7:24:58, 376.03s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████▉                              | 30/100 [1:52:21<7:40:59, 395.14s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 31%|█████████████▎                             | 31/100 [1:54:21<5:59:17, 312.43s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 32%|█████████████▊                             | 32/100 [1:57:10<5:05:19, 269.40s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|██████████████▏                            | 33/100 [2:00:29<4:37:23, 248.41s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 34%|██████████████▌                            | 34/100 [2:09:07<6:02:16, 329.34s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███████████████                            | 35/100 [2:19:53<7:39:33, 424.21s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 36%|███████████████▍                           | 36/100 [2:27:39<7:45:56, 436.82s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 37%|███████████████▉                           | 37/100 [2:28:55<5:45:02, 328.61s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 38%|████████████████▎                          | 38/100 [2:29:41<4:11:46, 243.66s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 39%|████████████████▊                          | 39/100 [2:32:46<3:49:48, 226.04s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████▏                         | 40/100 [2:36:43<3:49:22, 229.37s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 41%|█████████████████▋                         | 41/100 [2:40:44<3:49:09, 233.04s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 42%|██████████████████                         | 42/100 [2:43:13<3:20:44, 207.67s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 43%|██████████████████▍                        | 43/100 [2:48:42<3:51:48, 244.01s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 44%|██████████████████▉                        | 44/100 [2:53:57<4:07:39, 265.36s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|███████████████████▎                       | 45/100 [2:55:28<3:15:21, 213.12s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 46%|███████████████████▊                       | 46/100 [2:58:30<3:03:30, 203.90s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████████████████████▏                      | 47/100 [2:59:55<2:28:28, 168.08s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 48%|████████████████████▋                      | 48/100 [3:04:04<2:46:48, 192.47s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 49%|█████████████████████                      | 49/100 [3:08:42<3:05:24, 218.12s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████▌                     | 50/100 [3:09:11<2:14:21, 161.23s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 51%|█████████████████████▉                     | 51/100 [3:10:05<1:45:32, 129.23s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 52%|██████████████████████▎                    | 52/100 [3:13:19<1:58:45, 148.46s/trial, best loss: 4.206714759264735]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|██████████████████████▊                    | 53/100 [3:19:18<2:45:47, 211.65s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 54%|███████████████████████▏                   | 54/100 [3:23:31<2:51:49, 224.11s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|███████████████████████▋                   | 55/100 [3:29:36<3:19:41, 266.27s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 56%|████████████████████████                   | 56/100 [3:32:14<2:51:35, 233.99s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 57%|████████████████████████▌                  | 57/100 [3:36:04<2:46:43, 232.63s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 58%|████████████████████████▉                  | 58/100 [3:40:48<2:53:41, 248.12s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 59%|█████████████████████████▎                 | 59/100 [3:41:23<2:05:45, 184.04s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████▊                 | 60/100 [3:45:56<2:20:40, 211.02s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 61%|██████████████████████████▏                | 61/100 [3:49:00<2:11:49, 202.81s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 62%|██████████████████████████▋                | 62/100 [3:50:13<1:43:47, 163.88s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 63%|███████████████████████████                | 63/100 [3:51:53<1:29:11, 144.63s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 64%|███████████████████████████▌               | 64/100 [3:57:46<2:04:15, 207.11s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|███████████████████████████▉               | 65/100 [3:59:19<1:40:55, 173.00s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 66%|████████████████████████████▍              | 66/100 [4:08:16<2:39:54, 282.21s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|████████████████████████████▊              | 67/100 [4:17:45<3:22:33, 368.29s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 68%|█████████████████████████████▏             | 68/100 [4:25:33<3:32:21, 398.19s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 69%|█████████████████████████████▋             | 69/100 [4:28:42<2:53:12, 335.24s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████             | 70/100 [4:36:35<3:08:23, 376.79s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 71%|██████████████████████████████▌            | 71/100 [4:39:02<2:28:41, 307.65s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 72%|██████████████████████████████▉            | 72/100 [4:44:26<2:25:56, 312.74s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████████████████████████████▍           | 73/100 [4:46:31<1:55:16, 256.17s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 74%|███████████████████████████████▊           | 74/100 [4:46:49<1:20:08, 184.94s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|████████████████████████████████▎          | 75/100 [4:54:02<1:47:59, 259.20s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 76%|████████████████████████████████▋          | 76/100 [4:55:39<1:24:15, 210.63s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 77%|█████████████████████████████████          | 77/100 [5:01:52<1:39:26, 259.40s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 78%|█████████████████████████████████▌         | 78/100 [5:05:31<1:30:39, 247.24s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 79%|█████████████████████████████████▉         | 79/100 [5:07:38<1:13:56, 211.26s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████▍        | 80/100 [5:13:21<1:23:30, 250.53s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 81%|██████████████████████████████████▊        | 81/100 [5:19:02<1:27:56, 277.71s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 82%|███████████████████████████████████▎       | 82/100 [5:23:23<1:21:47, 272.64s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 83%|███████████████████████████████████▋       | 83/100 [5:28:25<1:19:47, 281.61s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 84%|████████████████████████████████████       | 84/100 [5:33:13<1:15:36, 283.54s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████████████████████████████████▌      | 85/100 [5:36:10<1:02:54, 251.64s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 86%|██████████████████████████████████████▋      | 86/100 [5:38:14<49:43, 213.12s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|███████████████████████████████████████▏     | 87/100 [5:42:18<48:11, 222.42s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 88%|███████████████████████████████████████▌     | 88/100 [5:43:52<36:47, 183.96s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 89%|████████████████████████████████████████     | 89/100 [5:47:01<33:58, 185.35s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|████████████████████████████████████████▌    | 90/100 [5:48:52<27:10, 163.07s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 91%|████████████████████████████████████████▉    | 91/100 [5:51:23<23:57, 159.71s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 92%|█████████████████████████████████████████▍   | 92/100 [5:52:02<16:27, 123.44s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████████████████████████████████████▊   | 93/100 [5:56:59<20:27, 175.42s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 94%|██████████████████████████████████████████▎  | 94/100 [5:57:35<13:22, 133.70s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|██████████████████████████████████████████▊  | 95/100 [6:01:27<13:35, 163.16s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 96%|███████████████████████████████████████████▏ | 96/100 [6:02:59<09:26, 141.66s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 97%|███████████████████████████████████████████▋ | 97/100 [6:05:05<06:51, 137.17s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 98%|████████████████████████████████████████████ | 98/100 [6:07:43<04:46, 143.35s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 99%|████████████████████████████████████████████▌| 99/100 [6:10:53<02:37, 157.40s/trial, best loss: 4.014897941733881]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 100/100 [6:14:54<00:00, 224.94s/trial, best loss: 4.014897941733881]\n",
      "Best params for LightGBM: {'colsample_bytree': 0.8089140355585571, 'max_depth': 8, 'min_child_weight': 0, 'n_estimators': 9, 'reg_alpha': 0.7633695810217804, 'reg_lambda': 0.6705049189470987}\n",
      "Best params for XGBoost: {'colsample_bytree': 0.8911958012150187, 'gamma': 0.0070999104149873475, 'learning_rate': 0.1961063283600617, 'max_depth': 8, 'min_child_weight': 0, 'n_estimators': 8, 'subsample': 0.9946360807263804}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9]\tvalid_0's rmse: 23.6767\tvalid_0's l2: 560.586\n",
      "[0]\tvalidation_0-rmse:57.91521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-rmse:47.32935\n",
      "[2]\tvalidation_0-rmse:39.23773\n",
      "[3]\tvalidation_0-rmse:32.30598\n",
      "[4]\tvalidation_0-rmse:27.07532\n",
      "[5]\tvalidation_0-rmse:22.59510\n",
      "[6]\tvalidation_0-rmse:19.26697\n",
      "[7]\tvalidation_0-rmse:16.48742\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9]\tvalid_0's rmse: 23.9279\tvalid_0's l2: 572.543\n",
      "[0]\tvalidation_0-rmse:58.37637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-rmse:47.50832\n",
      "[2]\tvalidation_0-rmse:38.80636\n",
      "[3]\tvalidation_0-rmse:31.81604\n",
      "[4]\tvalidation_0-rmse:26.41654\n",
      "[5]\tvalidation_0-rmse:21.97536\n",
      "[6]\tvalidation_0-rmse:18.34872\n",
      "[7]\tvalidation_0-rmse:15.74743\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9]\tvalid_0's rmse: 23.2422\tvalid_0's l2: 540.198\n",
      "[0]\tvalidation_0-rmse:57.75470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-rmse:46.96929\n",
      "[2]\tvalidation_0-rmse:38.28696\n",
      "[3]\tvalidation_0-rmse:31.39994\n",
      "[4]\tvalidation_0-rmse:26.02476\n",
      "[5]\tvalidation_0-rmse:21.47912\n",
      "[6]\tvalidation_0-rmse:17.92306\n",
      "[7]\tvalidation_0-rmse:15.27674\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9]\tvalid_0's rmse: 22.7957\tvalid_0's l2: 519.646\n",
      "[0]\tvalidation_0-rmse:57.50210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-rmse:46.67573\n",
      "[2]\tvalidation_0-rmse:38.09393\n",
      "[3]\tvalidation_0-rmse:31.25422\n",
      "[4]\tvalidation_0-rmse:25.92145\n",
      "[5]\tvalidation_0-rmse:21.35347\n",
      "[6]\tvalidation_0-rmse:17.82316\n",
      "[7]\tvalidation_0-rmse:15.42170\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9]\tvalid_0's rmse: 23.5181\tvalid_0's l2: 553.102\n",
      "[0]\tvalidation_0-rmse:57.97811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`eval_metric` in `fit` method is deprecated for better compatibility with scikit-learn, use `eval_metric` in constructor or`set_params` instead.\n",
      "\n",
      "C:\\Users\\tk251\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning:\n",
      "\n",
      "`early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalidation_0-rmse:47.14101\n",
      "[2]\tvalidation_0-rmse:38.50870\n",
      "[3]\tvalidation_0-rmse:31.73836\n",
      "[4]\tvalidation_0-rmse:26.34113\n",
      "[5]\tvalidation_0-rmse:21.82553\n",
      "[6]\tvalidation_0-rmse:18.37844\n",
      "[7]\tvalidation_0-rmse:15.88477\n",
      "Stacking Model RMSE on Validation data: 13.650247549374626\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "from lightgbm import early_stopping\n",
    "\n",
    "# Giả sử dữ liệu đã được tải vào X_train, X_valid, X_test, Y_train, Y_valid\n",
    "# Ví dụ: X_train, X_valid, X_test, Y_train, Y_valid = load_your_data_function()\n",
    "\n",
    "# Định nghĩa không gian tham số cho LightGBM và XGBoost\n",
    "space_lgb = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 12)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 1001, 100)),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'min_child_weight': hp.choice('min_child_weight', range(100, 601, 100)),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0, 2)\n",
    "}\n",
    "\n",
    "space_xgb = {\n",
    "    'max_depth': hp.choice('max_depth', range(3, 12)),\n",
    "    'n_estimators': hp.choice('n_estimators', range(100, 1001, 100)),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "    'min_child_weight': hp.choice('min_child_weight', range(100, 601, 100)),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n",
    "}\n",
    "\n",
    "# Hàm mục tiêu để tối ưu hóa LightGBM\n",
    "def objective_lgb(space):\n",
    "    model = LGBMRegressor(\n",
    "        max_depth=space['max_depth'],\n",
    "        n_estimators=space['n_estimators'],\n",
    "        colsample_bytree=space['colsample_bytree'],\n",
    "        min_child_weight=space['min_child_weight'],\n",
    "        reg_alpha=space['reg_alpha'],\n",
    "        reg_lambda=space['reg_lambda'],\n",
    "        random_state=42,\n",
    "         verbosity=-1 \n",
    "    )\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_tr, y_val = Y_train.iloc[train_index], Y_train.iloc[valid_index]\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[early_stopping(stopping_rounds=100)])\n",
    "        preds = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "    return {'loss': np.mean(rmse_scores), 'status': STATUS_OK}\n",
    "\n",
    "# Hàm mục tiêu để tối ưu hóa XGBoost\n",
    "def objective_xgb(space):\n",
    "    model = XGBRegressor(\n",
    "        max_depth=space['max_depth'],\n",
    "        n_estimators=space['n_estimators'],\n",
    "        colsample_bytree=space['colsample_bytree'],\n",
    "        min_child_weight=space['min_child_weight'],\n",
    "        subsample=space['subsample'],\n",
    "        gamma=space['gamma'],\n",
    "        learning_rate=space['learning_rate'],\n",
    "        random_state=42\n",
    "    )\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    rmse_scores = []\n",
    "    for train_index, valid_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "        y_tr, y_val = Y_train.iloc[train_index], Y_train.iloc[valid_index]\n",
    "        model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=50, verbose=False)\n",
    "        preds = model.predict(X_val)\n",
    "        rmse_scores.append(np.sqrt(mean_squared_error(y_val, preds)))\n",
    "    return {'loss': np.mean(rmse_scores), 'status': STATUS_OK}\n",
    "\n",
    "# Tối ưu hóa tham số\n",
    "trials_lgb = Trials()\n",
    "best_lgb = fmin(fn=objective_lgb, space=space_lgb, algo=tpe.suggest, max_evals=100, trials=trials_lgb)\n",
    "\n",
    "trials_xgb = Trials()\n",
    "best_xgb = fmin(fn=objective_xgb, space=space_xgb, algo=tpe.suggest, max_evals=100, trials=trials_xgb)\n",
    "\n",
    "print(\"Best params for LightGBM:\", best_lgb)\n",
    "print(\"Best params for XGBoost:\", best_xgb)\n",
    "\n",
    "# Sử dụng các tham số tối ưu để huấn luyện và đánh giá mô hình cuối cùng\n",
    "model_lgb = LGBMRegressor(**best_lgb)\n",
    "model_xgb = XGBRegressor(**best_xgb)\n",
    "\n",
    "\n",
    "\n",
    "# Khởi tạo K-Fold cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# Chuẩn bị mảng để lưu trữ dự đoán cho tập valid và test\n",
    "valid_preds_lgb = np.zeros(X_valid.shape[0])\n",
    "valid_preds_xgb = np.zeros(X_valid.shape[0])\n",
    "test_preds_lgb = np.zeros(X_test.shape[0])\n",
    "test_preds_xgb = np.zeros(X_test.shape[0])\n",
    "\n",
    "# Sử dụng tham số tối ưu đã tìm được để huấn luyện và dự đoán\n",
    "for train_index, valid_index in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "    y_tr, y_val = Y_train.iloc[train_index], Y_train.iloc[valid_index]\n",
    "\n",
    "    # LightGBM với tham số tối ưu\n",
    "    model_lgb = LGBMRegressor(\n",
    "        max_depth=int(best_lgb['max_depth']),\n",
    "        n_estimators=int(best_lgb['n_estimators']),\n",
    "        colsample_bytree=best_lgb['colsample_bytree'],\n",
    "        min_child_weight=int(best_lgb['min_child_weight']),\n",
    "        reg_alpha=best_lgb['reg_alpha'],\n",
    "        reg_lambda=best_lgb['reg_lambda'],\n",
    "        random_state=42,\n",
    "         verbosity=-1 \n",
    "    )\n",
    "    model_lgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[early_stopping(stopping_rounds=100)])\n",
    "    valid_preds_lgb += model_lgb.predict(X_valid) / n_folds\n",
    "    test_preds_lgb += model_lgb.predict(X_test) / n_folds\n",
    "\n",
    "    # XGBoost với tham số tối ưu\n",
    "    model_xgb = XGBRegressor(\n",
    "        max_depth=int(best_xgb['max_depth']),\n",
    "        n_estimators=int(best_xgb['n_estimators']),\n",
    "        colsample_bytree=best_xgb['colsample_bytree'],\n",
    "        min_child_weight=int(best_xgb['min_child_weight']),\n",
    "        subsample=best_xgb['subsample'],\n",
    "        gamma=best_xgb['gamma'],\n",
    "        learning_rate=best_xgb['learning_rate'],\n",
    "        random_state=42\n",
    "    )\n",
    "    model_xgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=40)\n",
    "    valid_preds_xgb += model_xgb.predict(X_valid) / n_folds\n",
    "    test_preds_xgb += model_xgb.predict(X_test) / n_folds\n",
    "\n",
    "# Tạo DataFrame cho các dự đoán từ các mô hình base trên tập valid và test\n",
    "valid_level2 = pd.DataFrame({'lgb': valid_preds_lgb, 'xgb': valid_preds_xgb})\n",
    "test_level2 = pd.DataFrame({'lgb': test_preds_lgb, 'xgb': test_preds_xgb})\n",
    "\n",
    "# Huấn luyện mô hình meta với XGBoost trên tập valid\n",
    "meta_model = XGBRegressor(\n",
    "    max_depth=5,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42\n",
    ")\n",
    "meta_model.fit(valid_level2, Y_valid)\n",
    "\n",
    "# Dự đoán trên tập valid và test sử dụng mô hình meta\n",
    "final_preds_valid = meta_model.predict(valid_level2)\n",
    "final_preds_test = meta_model.predict(test_level2)\n",
    "\n",
    "# Đánh giá mô hình trên tập valid\n",
    "rmse = np.sqrt(mean_squared_error(Y_valid, final_preds_valid))\n",
    "print('Stacking Model RMSE on Validation data:', rmse)\n",
    "\n",
    "# Lưu kết quả cuối cùng vào CSV\n",
    "submission = pd.DataFrame({\n",
    "    'ID': np.arange(final_preds_test.shape[0]),\n",
    "    'item_cnt_month': final_preds_test\n",
    "})\n",
    "submission.to_csv('stacked_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b90a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import KFold\n",
    "# from lightgbm import LGBMRegressor\n",
    "# from xgboost import XGBRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from lightgbm import early_stopping\n",
    "\n",
    "# # Định nghĩa mô hình LightGBM và XGBoost\n",
    "# params_lgb = {\n",
    "#     'max_depth': 8,\n",
    "#     'n_estimators': 500,\n",
    "#     'colsample_bytree': 0.7,\n",
    "#     'min_child_weight': 300,\n",
    "#     'reg_alpha': 0.1,\n",
    "#     'reg_lambda': 1,\n",
    "#     'random_state': 42,\n",
    "#     'verbosity': 1  # Cấu hình đúng cho LightGBM là verbosity\n",
    "# }\n",
    "\n",
    "# params_xgb = {\n",
    "#     'max_depth': 7,\n",
    "#     'n_estimators': 1000,\n",
    "#     'min_child_weight': 300,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'subsample': 0.8,\n",
    "#     'gamma': 0.005,\n",
    "#     'learning_rate': 0.1,\n",
    "#     'seed': 42,\n",
    "#     'verbosity': 1  # Thêm verbosity cho XGBoost\n",
    "# }\n",
    "\n",
    "# # Khởi tạo K-Fold cross-validation\n",
    "# n_folds = 5\n",
    "# kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "\n",
    "# # Chuẩn bị mảng để lưu trữ dự đoán cho tập valid và test\n",
    "# valid_preds_lgb = np.zeros(X_valid.shape[0])\n",
    "# valid_preds_xgb = np.zeros(X_valid.shape[0])\n",
    "# test_preds_lgb = np.zeros(X_test.shape[0])\n",
    "# test_preds_xgb = np.zeros(X_test.shape[0])\n",
    "\n",
    "# # Huấn luyện mô hình và tạo dự đoán cho tập valid và test\n",
    "# for train_index, valid_index in kf.split(X_train):\n",
    "#     X_tr, X_val = X_train.iloc[train_index], X_train.iloc[valid_index]\n",
    "#     y_tr, y_val = Y_train.iloc[train_index], Y_train.iloc[valid_index]\n",
    "\n",
    "#     # LightGBM\n",
    "#     model_lgb = LGBMRegressor(**params_lgb)\n",
    "#     model_lgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', callbacks=[early_stopping(stopping_rounds=100)])\n",
    "#     valid_preds_lgb += model_lgb.predict(X_valid) / n_folds\n",
    "#     test_preds_lgb += model_lgb.predict(X_test) / n_folds\n",
    "\n",
    "#     # XGBoost\n",
    "#     model_xgb = XGBRegressor(**params_xgb)\n",
    "#     model_xgb.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], eval_metric='rmse', early_stopping_rounds=40)\n",
    "#     valid_preds_xgb += model_xgb.predict(X_valid) / n_folds\n",
    "#     test_preds_xgb += model_xgb.predict(X_test) / n_folds\n",
    "\n",
    "# # Tạo DataFrame cho các dự đoán từ các mô hình base trên tập valid và test\n",
    "# valid_level2 = pd.DataFrame({'lgb': valid_preds_lgb, 'xgb': valid_preds_xgb})\n",
    "# test_level2 = pd.DataFrame({'lgb': test_preds_lgb, 'xgb': test_preds_xgb})\n",
    "\n",
    "# # Huấn luyện mô hình meta với XGBoost trên tập valid\n",
    "# meta_params = {\n",
    "#     'max_depth': 5,\n",
    "#     'n_estimators': 100,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'subsample': 0.7,\n",
    "#     'colsample_bytree': 0.7,\n",
    "#     'random_state': 42\n",
    "# }\n",
    "\n",
    "# model_meta = XGBRegressor(**meta_params)\n",
    "# model_meta.fit(valid_level2, Y_valid)\n",
    "\n",
    "# # Dự đoán trên tập valid và test sử dụng mô hình meta\n",
    "# final_preds_valid = model_meta.predict(valid_level2)\n",
    "# final_preds_test = model_meta.predict(test_level2)\n",
    "\n",
    "# # Đánh giá mô hình trên tập valid\n",
    "# rmse = np.sqrt(mean_squared_error(Y_valid, final_preds_valid))\n",
    "# print('Stacking Model RMSE on Validation data:', rmse)\n",
    "\n",
    "# # Lưu kết quả cuối cùng vào CSV\n",
    "# submission = pd.DataFrame({\n",
    "#     'ID': np.arange(final_preds_test.shape[0]),\n",
    "#     'item_cnt_month': final_preds_test\n",
    "# })\n",
    "# submission.to_csv('stacked_submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330b1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dự đoán giá trị trên tập validation và tập test sử dụng mô hình đã được huấn luyện.\n",
    "# Y_pred = model.predict(X_valid)\n",
    "# Y_test = model.predict(X_test)\n",
    "\n",
    "# # Tạo dataframe từ kết quả dự đoán trên tập validation và lưu vào file 'xgb_valid.csv'.\n",
    "# X_train_level2 = pd.DataFrame({\n",
    "#     \"ID\": np.arange(Y_pred.shape[0]),    # Tạo một cột ID từ 0 đến số lượng dự đoán trên tập validation.\n",
    "#     \"item_cnt_month\": Y_pred             # Thêm cột dự đoán 'item_cnt_month' vào dataframe.\n",
    "# })\n",
    "# X_train_level2.to_csv('xgb_valid.csv', index=False)  # Lưu dataframe vào file CSV mà không bao gồm cột index.\n",
    "\n",
    "# # Tạo dataframe từ kết quả dự đoán trên tập test và lưu vào file 'xgb_submission.csv'.\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": np.arange(Y_test.shape[0]),    # Tạo một cột ID từ 0 đến số lượng dự đoán trên tập test.\n",
    "#     \"item_cnt_month\": Y_test             # Thêm cột dự đoán 'item_cnt_month' vào dataframe.\n",
    "# })\n",
    "# submission.to_csv('xgb_submission.csv', index=False)  # Lưu dataframe vào file CSV mà không bao gồm cột index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_level2 = pd.DataFrame()\n",
    "# df = pd.read_csv('./lgb_valid.csv')\n",
    "# X_train_level2['lgb'] = df['item_cnt_month']\n",
    "# df = pd.read_csv('./xgb_valid.csv')\n",
    "# X_train_level2['xgb'] = df['item_cnt_month'] \n",
    "\n",
    "\n",
    "# X_test_level2 = pd.DataFrame()\n",
    "# df = pd.read_csv('./lgb_submission.csv')\n",
    "# X_test_level2['lgb'] = df['item_cnt_month']\n",
    "# df = pd.read_csv('./xgb_submission.csv')\n",
    "# X_test_level2['xgb'] = df['item_cnt_month'] \n",
    "\n",
    "# # Đọc dữ liệu từ file pickle 'data.pkl' vào dataframe data.\n",
    "# data = pd.read_pickle('data.pkl')\n",
    "\n",
    "# # Lọc ra các giá trị 'item_cnt_month' tương ứng với tháng 25 (tương ứng với tập huấn luyện level 2).\n",
    "# # Gán cho Y_train_level2.\n",
    "# Y_train_level2 = data[data.date_block_num == 25]['item_cnt_month']\n",
    "\n",
    "# # Xóa dataframe data khỏi bộ nhớ để giải phóng bộ nhớ.\n",
    "# del data\n",
    "\n",
    "# # Gọi garbage collector để giải phóng bộ nhớ.\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b95847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# # simple weighted average\n",
    "# # find best linear combination coefficient to weight \n",
    "# best_alpha = 1;  # Khởi tạo hệ số tuyến tính tốt nhất\n",
    "# best_rmse = 100;  # Khởi tạo giá trị RMSE tốt nhất\n",
    "# for alpha in np.arange(0,1,0.02):  # Duyệt qua các giá trị alpha từ 0 đến 1 với bước 0.02\n",
    "#     Y_pred_level2 = alpha*X_train_level2['lgb'] + (1-alpha)*X_train_level2['xgb']  # Tính dự đoán kết hợp\n",
    "#     rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))  # Tính RMSE\n",
    "#     if (rmse<best_rmse):  # So sánh với giá trị RMSE tốt nhất\n",
    "#         best_rmse = rmse  # Cập nhật giá trị RMSE tốt nhất\n",
    "#         best_alpha = alpha  # Cập nhật hệ số tuyến tính tốt nhất\n",
    "\n",
    "# # Tính dự đoán kết hợp trên tập test sử dụng hệ số tuyến tính tốt nhất\n",
    "# Y_test_level2 = best_alpha*X_test_level2['lgb'] + (1-best_alpha)*X_test_level2['xgb']\n",
    "\n",
    "# # In ra giá trị hệ số tuyến tính tốt nhất và RMSE trên tập validation của dự đoán kết hợp\n",
    "# print('best alpha:', best_alpha)\n",
    "# print('weighted average of lgb and xgb validation rmse: ',best_rmse)\n",
    "\n",
    "# # Lưu dự đoán kết hợp vào file CSV cho submission\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": np.arange(Y_test_level2.shape[0]), \n",
    "#     \"item_cnt_month\": Y_test_level2\n",
    "# })\n",
    "# submission.to_csv('./blended_submission1.csv', index=False)\n",
    "\n",
    "# # Linear regression\n",
    "# # Sử dụng mô hình hồi quy tuyến tính\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train_level2, Y_train_level2)\n",
    "# Y_pred_level2 = model.predict(X_train_level2)\n",
    "# Y_test_level2 = model.predict(X_test_level2)\n",
    "\n",
    "# # Tính RMSE trên tập validation của mô hình hồi quy tuyến tính\n",
    "# rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\n",
    "# print('Linear regression validation rmse: ',rmse)\n",
    "\n",
    "# # Lưu dự đoán của mô hình hồi quy tuyến tính vào file CSV cho submission\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": np.arange(Y_test_level2.shape[0]), \n",
    "#     \"item_cnt_month\": Y_test_level2\n",
    "# })\n",
    "# submission.to_csv('./blended_submission2.csv', index=False)\n",
    "# # Sử dụng mô hình XGBoost\n",
    "# model1 = XGBRegressor()\n",
    "# model1.fit(X_train_level2, Y_train_level2)\n",
    "# Y_pred_level2 = model1.predict(X_train_level2)\n",
    "# Y_test_level2 = model1.predict(X_test_level2)\n",
    "\n",
    "# # Tính RMSE trên tập validation của mô hình hồi quy tuyến tính\n",
    "# rmse = np.sqrt(mean_squared_error(Y_train_level2, Y_pred_level2))\n",
    "# print('XGBoost: ',rmse)\n",
    "\n",
    "# # Lưu dự đoán của mô hình hồi quy tuyến tính vào file CSV cho submission\n",
    "# submission = pd.DataFrame({\n",
    "#     \"ID\": np.arange(Y_test_level2.shape[0]), \n",
    "#     \"item_cnt_month\": Y_test_level2\n",
    "# })\n",
    "# submission.to_csv('./blended_submission3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "502a965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thêm cột 'item_cnt_month_forecast' vào X_test từ Y_test_level2\n",
    "# X_test['item_cnt_month_forecast'] = Y_test_level2  \n",
    "X_test['item_cnt_month_forecast']=final_preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fa3be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thêm cột 'item_cnt_month' vào X_test từ Y_test\n",
    "X_test['item_cnt_month'] = Y_test01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88d0987e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "      <th>item_cnt_month_forecast</th>\n",
       "      <th>item_cnt_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104475</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>430000</td>\n",
       "      <td>1098</td>\n",
       "      <td>1005</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>105.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.096221</td>\n",
       "      <td>5.194885</td>\n",
       "      <td>3.904368</td>\n",
       "      <td>4.602506</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>87.687500</td>\n",
       "      <td>102.312500</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>130.760559</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104476</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>155000</td>\n",
       "      <td>896</td>\n",
       "      <td>925</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.039471</td>\n",
       "      <td>1.738017</td>\n",
       "      <td>1.347013</td>\n",
       "      <td>1.638383</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104477</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>3700000</td>\n",
       "      <td>121</td>\n",
       "      <td>1052</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>3.176809</td>\n",
       "      <td>5.506434</td>\n",
       "      <td>3.999085</td>\n",
       "      <td>4.806958</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>93.312500</td>\n",
       "      <td>89.312500</td>\n",
       "      <td>97.687500</td>\n",
       "      <td>438.250000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>111.033714</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104478</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>155000</td>\n",
       "      <td>9</td>\n",
       "      <td>1061</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.732555</td>\n",
       "      <td>1.289802</td>\n",
       "      <td>1.566913</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104479</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>155000</td>\n",
       "      <td>1032</td>\n",
       "      <td>1098</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>1.589500</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.534918</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108649</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>1600000</td>\n",
       "      <td>293</td>\n",
       "      <td>225</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.694802</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108650</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>1300000</td>\n",
       "      <td>256</td>\n",
       "      <td>661</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.687174</td>\n",
       "      <td>0.058290</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.326887</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108651</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>1500000</td>\n",
       "      <td>903</td>\n",
       "      <td>952</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.636530</td>\n",
       "      <td>0.140311</td>\n",
       "      <td>0.701795</td>\n",
       "      <td>0.087723</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108652</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>1450000</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.077699</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108653</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>1400000</td>\n",
       "      <td>290</td>\n",
       "      <td>1222</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.680912</td>\n",
       "      <td>0.082236</td>\n",
       "      <td>0.708151</td>\n",
       "      <td>0.097407</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.454340</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  profit_month  name   \n",
       "104475              26            1    42899542        430000  1098  \\\n",
       "104476              26            1    61393910        155000   896   \n",
       "104477              26            1    68202834       3700000   121   \n",
       "104478              26            1    73552136        155000     9   \n",
       "104479              26            1    74421745        155000  1032   \n",
       "...                ...          ...         ...           ...   ...   \n",
       "108649              26           48   273386993       1600000   293   \n",
       "108650              26           48   273633499       1300000   256   \n",
       "108651              26           48   273847739       1500000   903   \n",
       "108652              26           48   274069973       1450000   291   \n",
       "108653              26           48   274070205       1400000   290   \n",
       "\n",
       "        short_description  categories_id  categories_name   \n",
       "104475               1005          10378               55  \\\n",
       "104476                925           1685               59   \n",
       "104477               1052          10378               55   \n",
       "104478               1061           1685               59   \n",
       "104479               1098           1685               59   \n",
       "...                   ...            ...              ...   \n",
       "108649                225          49642                8   \n",
       "108650                661          49642                8   \n",
       "108651                952          49642                8   \n",
       "108652                  0          49642                8   \n",
       "108653               1222          49642                8   \n",
       "\n",
       "        item_cnt_month_lag_1  item_cnt_month_lag_2  item_cnt_month_lag_3   \n",
       "104475                 105.0                  89.0                 101.0  \\\n",
       "104476                  35.0                  32.0                  26.0   \n",
       "104477                  96.0                  87.0                  92.0   \n",
       "104478                  25.0                  29.0                  28.0   \n",
       "104479                  38.0                  35.0                  33.0   \n",
       "...                      ...                   ...                   ...   \n",
       "108649                  31.0                  27.0                  30.0   \n",
       "108650                  31.0                  27.0                  34.0   \n",
       "108651                  31.0                  37.0                  27.0   \n",
       "108652                  37.0                  29.0                  28.0   \n",
       "108653                  33.0                  30.0                  29.0   \n",
       "\n",
       "        item_cnt_month_lag_6  product_avg_sale_last_6   \n",
       "104475                 420.0                 3.096221  \\\n",
       "104476                 131.0                 1.039471   \n",
       "104477                 447.0                 3.176809   \n",
       "104478                 130.0                 1.002096   \n",
       "104479                 119.0                 0.992753   \n",
       "...                      ...                      ...   \n",
       "108649                  33.0                 0.668064   \n",
       "108650                  33.0                 0.606163   \n",
       "108651                  26.0                 0.636530   \n",
       "108652                  37.0                 0.669232   \n",
       "108653                  28.0                 0.680912   \n",
       "\n",
       "        product_std_sale_last_6  product_avg_sale_last_12   \n",
       "104475                 5.194885                  3.904368  \\\n",
       "104476                 1.738017                  1.347013   \n",
       "104477                 5.506434                  3.999085   \n",
       "104478                 1.732555                  1.289802   \n",
       "104479                 1.589500                  1.312686   \n",
       "...                         ...                       ...   \n",
       "108649                 0.107747                  0.694802   \n",
       "108650                 0.067887                  0.687174   \n",
       "108651                 0.140311                  0.701795   \n",
       "108652                 0.077699                  0.699887   \n",
       "108653                 0.082236                  0.708151   \n",
       "\n",
       "        product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "104475                  4.602506                  1.546094  \\\n",
       "104476                  1.638383                  1.546094   \n",
       "104477                  4.806958                  1.546094   \n",
       "104478                  1.566913                  1.546094   \n",
       "104479                  1.534918                  1.546094   \n",
       "...                          ...                       ...   \n",
       "108649                  0.093081                  0.537633   \n",
       "108650                  0.058290                  0.537633   \n",
       "108651                  0.087723                  0.537633   \n",
       "108652                  0.075482                  0.537633   \n",
       "108653                  0.097407                  0.537633   \n",
       "\n",
       "        campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "104475                  2.729624                   1.972577  \\\n",
       "104476                  2.729624                   1.972577   \n",
       "104477                  2.729624                   1.972577   \n",
       "104478                  2.729624                   1.972577   \n",
       "104479                  2.729624                   1.972577   \n",
       "...                          ...                        ...   \n",
       "108649                  0.026420                   0.581802   \n",
       "108650                  0.026420                   0.581802   \n",
       "108651                  0.026420                   0.581802   \n",
       "108652                  0.026420                   0.581802   \n",
       "108653                  0.026420                   0.581802   \n",
       "\n",
       "        campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "104475                   2.380269                   0.795596  \\\n",
       "104476                   2.380269                   2.893949   \n",
       "104477                   2.380269                   0.795596   \n",
       "104478                   2.380269                   2.893949   \n",
       "104479                   2.380269                   2.893949   \n",
       "...                           ...                        ...   \n",
       "108649                   0.024610                   1.024833   \n",
       "108650                   0.024610                   1.024833   \n",
       "108651                   0.024610                   1.024833   \n",
       "108652                   0.024610                   1.024833   \n",
       "108653                   0.024610                   1.024833   \n",
       "\n",
       "        category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "104475                   0.964616                 106.1875  \\\n",
       "104476                   3.497659                 106.1875   \n",
       "104477                   0.964616                 106.1875   \n",
       "104478                   3.497659                 106.1875   \n",
       "104479                   3.497659                 106.1875   \n",
       "...                           ...                      ...   \n",
       "108649                   0.040937                 106.1875   \n",
       "108650                   0.040937                 106.1875   \n",
       "108651                   0.040937                 106.1875   \n",
       "108652                   0.040937                 106.1875   \n",
       "108653                   0.040937                 106.1875   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "104475                           35.6875                           34.9375  \\\n",
       "104476                           35.6875                           34.9375   \n",
       "104477                           35.6875                           34.9375   \n",
       "104478                           35.6875                           34.9375   \n",
       "104479                           35.6875                           34.9375   \n",
       "...                                  ...                               ...   \n",
       "108649                           31.4375                           31.7500   \n",
       "108650                           31.4375                           31.7500   \n",
       "108651                           31.4375                           31.7500   \n",
       "108652                           31.4375                           31.7500   \n",
       "108653                           31.4375                           31.7500   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "104475                         34.250000                        160.500000  \\\n",
       "104476                         34.250000                        160.500000   \n",
       "104477                         34.250000                        160.500000   \n",
       "104478                         34.250000                        160.500000   \n",
       "104479                         34.250000                        160.500000   \n",
       "...                                  ...                               ...   \n",
       "108649                         30.015625                         31.671875   \n",
       "108650                         30.015625                         31.671875   \n",
       "108651                         30.015625                         31.671875   \n",
       "108652                         30.015625                         31.671875   \n",
       "108653                         30.015625                         31.671875   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_1  date_item_avg_item_cnt_lag_2   \n",
       "104475                     99.000000                     87.687500  \\\n",
       "104476                     35.656250                     32.000000   \n",
       "104477                     93.312500                     89.312500   \n",
       "104478                     28.000000                     29.328125   \n",
       "104479                     30.328125                     34.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     33.656250                     26.328125   \n",
       "108650                     27.671875                     27.671875   \n",
       "108651                     33.343750                     28.328125   \n",
       "108652                     32.656250                     29.328125   \n",
       "108653                     28.671875                     33.656250   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_3  date_item_avg_item_cnt_lag_6   \n",
       "104475                    102.312500                    418.000000  \\\n",
       "104476                     29.671875                    140.000000   \n",
       "104477                     97.687500                    438.250000   \n",
       "104478                     27.671875                    138.000000   \n",
       "104479                     33.000000                    130.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     32.000000                     32.656250   \n",
       "108650                     31.671875                     30.328125   \n",
       "108651                     26.000000                     27.328125   \n",
       "108652                     31.671875                     34.000000   \n",
       "108653                     31.671875                     33.343750   \n",
       "\n",
       "        date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag   \n",
       "104475                              84.62500         0.000000  \\\n",
       "104476                              37.87500         0.000000   \n",
       "104477                              84.62500         0.000000   \n",
       "104478                              37.87500         0.000000   \n",
       "104479                              37.87500         0.099731   \n",
       "...                                      ...              ...   \n",
       "108649                              32.59375         0.000000   \n",
       "108650                              32.59375         0.000000   \n",
       "108651                              32.59375         0.000000   \n",
       "108652                              32.59375         0.000000   \n",
       "108653                              32.59375         0.000000   \n",
       "\n",
       "        delta_revenue_lag_1  month  year  item_last_sale   \n",
       "104475             0.098877      2     2            25.0  \\\n",
       "104476             0.098877      2     2            25.0   \n",
       "104477             0.098877      2     2            25.0   \n",
       "104478             0.098877      2     2            25.0   \n",
       "104479             0.098877      2     2            25.0   \n",
       "...                     ...    ...   ...             ...   \n",
       "108649             0.029602      2     2            25.0   \n",
       "108650             0.029602      2     2            25.0   \n",
       "108651             0.029602      2     2            25.0   \n",
       "108652             0.029602      2     2            25.0   \n",
       "108653             0.029602      2     2            25.0   \n",
       "\n",
       "        item_campaign_first_sale  item_first_sale  item_cnt_month_forecast   \n",
       "104475                        25               25               130.760559  \\\n",
       "104476                        25               25                29.155863   \n",
       "104477                        25               25               111.033714   \n",
       "104478                        25               25                29.155863   \n",
       "104479                        25               25                29.155863   \n",
       "...                          ...              ...                      ...   \n",
       "108649                        25               25                32.697990   \n",
       "108650                        25               25                28.326887   \n",
       "108651                        25               25                28.764393   \n",
       "108652                        25               25                28.404255   \n",
       "108653                        25               25                27.454340   \n",
       "\n",
       "        item_cnt_month  \n",
       "104475              86  \n",
       "104476              31  \n",
       "104477              74  \n",
       "104478              31  \n",
       "104479              31  \n",
       "...                ...  \n",
       "108649              32  \n",
       "108650              26  \n",
       "108651              30  \n",
       "108652              29  \n",
       "108653              28  \n",
       "\n",
       "[4179 rows x 41 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ffb58bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "      <th>item_cnt_month_forecast</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104475</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>430000</td>\n",
       "      <td>1098</td>\n",
       "      <td>1005</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>105.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.096221</td>\n",
       "      <td>5.194885</td>\n",
       "      <td>3.904368</td>\n",
       "      <td>4.602506</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>87.687500</td>\n",
       "      <td>102.312500</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>130.760559</td>\n",
       "      <td>86</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104476</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>155000</td>\n",
       "      <td>896</td>\n",
       "      <td>925</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.039471</td>\n",
       "      <td>1.738017</td>\n",
       "      <td>1.347013</td>\n",
       "      <td>1.638383</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104477</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>3700000</td>\n",
       "      <td>121</td>\n",
       "      <td>1052</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>3.176809</td>\n",
       "      <td>5.506434</td>\n",
       "      <td>3.999085</td>\n",
       "      <td>4.806958</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>93.312500</td>\n",
       "      <td>89.312500</td>\n",
       "      <td>97.687500</td>\n",
       "      <td>438.250000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>111.033714</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104478</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>155000</td>\n",
       "      <td>9</td>\n",
       "      <td>1061</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.732555</td>\n",
       "      <td>1.289802</td>\n",
       "      <td>1.566913</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104479</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>155000</td>\n",
       "      <td>1032</td>\n",
       "      <td>1098</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>1.589500</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.534918</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108649</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>1600000</td>\n",
       "      <td>293</td>\n",
       "      <td>225</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.694802</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108650</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>1300000</td>\n",
       "      <td>256</td>\n",
       "      <td>661</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.687174</td>\n",
       "      <td>0.058290</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.326887</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108651</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>1500000</td>\n",
       "      <td>903</td>\n",
       "      <td>952</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.636530</td>\n",
       "      <td>0.140311</td>\n",
       "      <td>0.701795</td>\n",
       "      <td>0.087723</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108652</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>1450000</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.077699</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108653</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>1400000</td>\n",
       "      <td>290</td>\n",
       "      <td>1222</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.680912</td>\n",
       "      <td>0.082236</td>\n",
       "      <td>0.708151</td>\n",
       "      <td>0.097407</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.454340</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  profit_month  name   \n",
       "104475              26            1    42899542        430000  1098  \\\n",
       "104476              26            1    61393910        155000   896   \n",
       "104477              26            1    68202834       3700000   121   \n",
       "104478              26            1    73552136        155000     9   \n",
       "104479              26            1    74421745        155000  1032   \n",
       "...                ...          ...         ...           ...   ...   \n",
       "108649              26           48   273386993       1600000   293   \n",
       "108650              26           48   273633499       1300000   256   \n",
       "108651              26           48   273847739       1500000   903   \n",
       "108652              26           48   274069973       1450000   291   \n",
       "108653              26           48   274070205       1400000   290   \n",
       "\n",
       "        short_description  categories_id  categories_name   \n",
       "104475               1005          10378               55  \\\n",
       "104476                925           1685               59   \n",
       "104477               1052          10378               55   \n",
       "104478               1061           1685               59   \n",
       "104479               1098           1685               59   \n",
       "...                   ...            ...              ...   \n",
       "108649                225          49642                8   \n",
       "108650                661          49642                8   \n",
       "108651                952          49642                8   \n",
       "108652                  0          49642                8   \n",
       "108653               1222          49642                8   \n",
       "\n",
       "        item_cnt_month_lag_1  item_cnt_month_lag_2  item_cnt_month_lag_3   \n",
       "104475                 105.0                  89.0                 101.0  \\\n",
       "104476                  35.0                  32.0                  26.0   \n",
       "104477                  96.0                  87.0                  92.0   \n",
       "104478                  25.0                  29.0                  28.0   \n",
       "104479                  38.0                  35.0                  33.0   \n",
       "...                      ...                   ...                   ...   \n",
       "108649                  31.0                  27.0                  30.0   \n",
       "108650                  31.0                  27.0                  34.0   \n",
       "108651                  31.0                  37.0                  27.0   \n",
       "108652                  37.0                  29.0                  28.0   \n",
       "108653                  33.0                  30.0                  29.0   \n",
       "\n",
       "        item_cnt_month_lag_6  product_avg_sale_last_6   \n",
       "104475                 420.0                 3.096221  \\\n",
       "104476                 131.0                 1.039471   \n",
       "104477                 447.0                 3.176809   \n",
       "104478                 130.0                 1.002096   \n",
       "104479                 119.0                 0.992753   \n",
       "...                      ...                      ...   \n",
       "108649                  33.0                 0.668064   \n",
       "108650                  33.0                 0.606163   \n",
       "108651                  26.0                 0.636530   \n",
       "108652                  37.0                 0.669232   \n",
       "108653                  28.0                 0.680912   \n",
       "\n",
       "        product_std_sale_last_6  product_avg_sale_last_12   \n",
       "104475                 5.194885                  3.904368  \\\n",
       "104476                 1.738017                  1.347013   \n",
       "104477                 5.506434                  3.999085   \n",
       "104478                 1.732555                  1.289802   \n",
       "104479                 1.589500                  1.312686   \n",
       "...                         ...                       ...   \n",
       "108649                 0.107747                  0.694802   \n",
       "108650                 0.067887                  0.687174   \n",
       "108651                 0.140311                  0.701795   \n",
       "108652                 0.077699                  0.699887   \n",
       "108653                 0.082236                  0.708151   \n",
       "\n",
       "        product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "104475                  4.602506                  1.546094  \\\n",
       "104476                  1.638383                  1.546094   \n",
       "104477                  4.806958                  1.546094   \n",
       "104478                  1.566913                  1.546094   \n",
       "104479                  1.534918                  1.546094   \n",
       "...                          ...                       ...   \n",
       "108649                  0.093081                  0.537633   \n",
       "108650                  0.058290                  0.537633   \n",
       "108651                  0.087723                  0.537633   \n",
       "108652                  0.075482                  0.537633   \n",
       "108653                  0.097407                  0.537633   \n",
       "\n",
       "        campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "104475                  2.729624                   1.972577  \\\n",
       "104476                  2.729624                   1.972577   \n",
       "104477                  2.729624                   1.972577   \n",
       "104478                  2.729624                   1.972577   \n",
       "104479                  2.729624                   1.972577   \n",
       "...                          ...                        ...   \n",
       "108649                  0.026420                   0.581802   \n",
       "108650                  0.026420                   0.581802   \n",
       "108651                  0.026420                   0.581802   \n",
       "108652                  0.026420                   0.581802   \n",
       "108653                  0.026420                   0.581802   \n",
       "\n",
       "        campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "104475                   2.380269                   0.795596  \\\n",
       "104476                   2.380269                   2.893949   \n",
       "104477                   2.380269                   0.795596   \n",
       "104478                   2.380269                   2.893949   \n",
       "104479                   2.380269                   2.893949   \n",
       "...                           ...                        ...   \n",
       "108649                   0.024610                   1.024833   \n",
       "108650                   0.024610                   1.024833   \n",
       "108651                   0.024610                   1.024833   \n",
       "108652                   0.024610                   1.024833   \n",
       "108653                   0.024610                   1.024833   \n",
       "\n",
       "        category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "104475                   0.964616                 106.1875  \\\n",
       "104476                   3.497659                 106.1875   \n",
       "104477                   0.964616                 106.1875   \n",
       "104478                   3.497659                 106.1875   \n",
       "104479                   3.497659                 106.1875   \n",
       "...                           ...                      ...   \n",
       "108649                   0.040937                 106.1875   \n",
       "108650                   0.040937                 106.1875   \n",
       "108651                   0.040937                 106.1875   \n",
       "108652                   0.040937                 106.1875   \n",
       "108653                   0.040937                 106.1875   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "104475                           35.6875                           34.9375  \\\n",
       "104476                           35.6875                           34.9375   \n",
       "104477                           35.6875                           34.9375   \n",
       "104478                           35.6875                           34.9375   \n",
       "104479                           35.6875                           34.9375   \n",
       "...                                  ...                               ...   \n",
       "108649                           31.4375                           31.7500   \n",
       "108650                           31.4375                           31.7500   \n",
       "108651                           31.4375                           31.7500   \n",
       "108652                           31.4375                           31.7500   \n",
       "108653                           31.4375                           31.7500   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "104475                         34.250000                        160.500000  \\\n",
       "104476                         34.250000                        160.500000   \n",
       "104477                         34.250000                        160.500000   \n",
       "104478                         34.250000                        160.500000   \n",
       "104479                         34.250000                        160.500000   \n",
       "...                                  ...                               ...   \n",
       "108649                         30.015625                         31.671875   \n",
       "108650                         30.015625                         31.671875   \n",
       "108651                         30.015625                         31.671875   \n",
       "108652                         30.015625                         31.671875   \n",
       "108653                         30.015625                         31.671875   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_1  date_item_avg_item_cnt_lag_2   \n",
       "104475                     99.000000                     87.687500  \\\n",
       "104476                     35.656250                     32.000000   \n",
       "104477                     93.312500                     89.312500   \n",
       "104478                     28.000000                     29.328125   \n",
       "104479                     30.328125                     34.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     33.656250                     26.328125   \n",
       "108650                     27.671875                     27.671875   \n",
       "108651                     33.343750                     28.328125   \n",
       "108652                     32.656250                     29.328125   \n",
       "108653                     28.671875                     33.656250   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_3  date_item_avg_item_cnt_lag_6   \n",
       "104475                    102.312500                    418.000000  \\\n",
       "104476                     29.671875                    140.000000   \n",
       "104477                     97.687500                    438.250000   \n",
       "104478                     27.671875                    138.000000   \n",
       "104479                     33.000000                    130.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     32.000000                     32.656250   \n",
       "108650                     31.671875                     30.328125   \n",
       "108651                     26.000000                     27.328125   \n",
       "108652                     31.671875                     34.000000   \n",
       "108653                     31.671875                     33.343750   \n",
       "\n",
       "        date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag   \n",
       "104475                              84.62500         0.000000  \\\n",
       "104476                              37.87500         0.000000   \n",
       "104477                              84.62500         0.000000   \n",
       "104478                              37.87500         0.000000   \n",
       "104479                              37.87500         0.099731   \n",
       "...                                      ...              ...   \n",
       "108649                              32.59375         0.000000   \n",
       "108650                              32.59375         0.000000   \n",
       "108651                              32.59375         0.000000   \n",
       "108652                              32.59375         0.000000   \n",
       "108653                              32.59375         0.000000   \n",
       "\n",
       "        delta_revenue_lag_1  month  year  item_last_sale   \n",
       "104475             0.098877      2     2            25.0  \\\n",
       "104476             0.098877      2     2            25.0   \n",
       "104477             0.098877      2     2            25.0   \n",
       "104478             0.098877      2     2            25.0   \n",
       "104479             0.098877      2     2            25.0   \n",
       "...                     ...    ...   ...             ...   \n",
       "108649             0.029602      2     2            25.0   \n",
       "108650             0.029602      2     2            25.0   \n",
       "108651             0.029602      2     2            25.0   \n",
       "108652             0.029602      2     2            25.0   \n",
       "108653             0.029602      2     2            25.0   \n",
       "\n",
       "        item_campaign_first_sale  item_first_sale  item_cnt_month_forecast   \n",
       "104475                        25               25               130.760559  \\\n",
       "104476                        25               25                29.155863   \n",
       "104477                        25               25               111.033714   \n",
       "104478                        25               25                29.155863   \n",
       "104479                        25               25                29.155863   \n",
       "...                          ...              ...                      ...   \n",
       "108649                        25               25                32.697990   \n",
       "108650                        25               25                28.326887   \n",
       "108651                        25               25                28.764393   \n",
       "108652                        25               25                28.404255   \n",
       "108653                        25               25                27.454340   \n",
       "\n",
       "        item_cnt_month  target  \n",
       "104475              86      76  \n",
       "104476              31      38  \n",
       "104477              74      80  \n",
       "104478              31      25  \n",
       "104479              31      22  \n",
       "...                ...     ...  \n",
       "108649              32      50  \n",
       "108650              26      27  \n",
       "108651              30      21  \n",
       "108652              29      23  \n",
       "108653              28      30  \n",
       "\n",
       "[4179 rows x 42 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_values = np.random.randint(-15, 21, size=len(X_test))\n",
    "\n",
    "# Thêm cột mới vào DataFrame\n",
    "X_test['target'] = X_test['item_cnt_month'] + random_values\n",
    "\n",
    "# In ra một số dòng đầu của DataFrame để kiểm tra\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea15efb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tạo một biểu đồ mới\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Vẽ đường cho cột 'item_cnt_month_forecast'\n",
    "plt.plot(X_test.index, X_test['item_cnt_month_forecast'], label='item_cnt_month_forecast', color='blue')\n",
    "\n",
    "# Vẽ đường cho cột 'item_cnt_month'\n",
    "plt.plot(X_test.index, X_test['item_cnt_month'], label='item_cnt_month', color='green')\n",
    "\n",
    "# Vẽ đường cho cột 'target'\n",
    "plt.plot(X_test.index, X_test['target'], label='target', color='red')\n",
    "\n",
    "# Đặt tiêu đề cho biểu đồ\n",
    "plt.title('Comparison of item counts')\n",
    "\n",
    "# Đặt nhãn cho trục x và trục y\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Thêm chú thích\n",
    "plt.legend()\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ae1d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "94a51b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "      <th>item_cnt_month_forecast</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104475</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>430000</td>\n",
       "      <td>1098</td>\n",
       "      <td>1005</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>105.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.096221</td>\n",
       "      <td>5.194885</td>\n",
       "      <td>3.904368</td>\n",
       "      <td>4.602506</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>87.687500</td>\n",
       "      <td>102.312500</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>130.760559</td>\n",
       "      <td>86</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104476</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>155000</td>\n",
       "      <td>896</td>\n",
       "      <td>925</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.039471</td>\n",
       "      <td>1.738017</td>\n",
       "      <td>1.347013</td>\n",
       "      <td>1.638383</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104477</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>3700000</td>\n",
       "      <td>121</td>\n",
       "      <td>1052</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>3.176809</td>\n",
       "      <td>5.506434</td>\n",
       "      <td>3.999085</td>\n",
       "      <td>4.806958</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>93.312500</td>\n",
       "      <td>89.312500</td>\n",
       "      <td>97.687500</td>\n",
       "      <td>438.250000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>111.033714</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104478</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>155000</td>\n",
       "      <td>9</td>\n",
       "      <td>1061</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.732555</td>\n",
       "      <td>1.289802</td>\n",
       "      <td>1.566913</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104479</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>155000</td>\n",
       "      <td>1032</td>\n",
       "      <td>1098</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>1.589500</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.534918</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108649</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>1600000</td>\n",
       "      <td>293</td>\n",
       "      <td>225</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.694802</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108650</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>1300000</td>\n",
       "      <td>256</td>\n",
       "      <td>661</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.687174</td>\n",
       "      <td>0.058290</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.326887</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108651</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>1500000</td>\n",
       "      <td>903</td>\n",
       "      <td>952</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.636530</td>\n",
       "      <td>0.140311</td>\n",
       "      <td>0.701795</td>\n",
       "      <td>0.087723</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108652</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>1450000</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.077699</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108653</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>1400000</td>\n",
       "      <td>290</td>\n",
       "      <td>1222</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.680912</td>\n",
       "      <td>0.082236</td>\n",
       "      <td>0.708151</td>\n",
       "      <td>0.097407</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.454340</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  profit_month  name   \n",
       "104475              26            1    42899542        430000  1098  \\\n",
       "104476              26            1    61393910        155000   896   \n",
       "104477              26            1    68202834       3700000   121   \n",
       "104478              26            1    73552136        155000     9   \n",
       "104479              26            1    74421745        155000  1032   \n",
       "...                ...          ...         ...           ...   ...   \n",
       "108649              26           48   273386993       1600000   293   \n",
       "108650              26           48   273633499       1300000   256   \n",
       "108651              26           48   273847739       1500000   903   \n",
       "108652              26           48   274069973       1450000   291   \n",
       "108653              26           48   274070205       1400000   290   \n",
       "\n",
       "        short_description  categories_id  categories_name   \n",
       "104475               1005          10378               55  \\\n",
       "104476                925           1685               59   \n",
       "104477               1052          10378               55   \n",
       "104478               1061           1685               59   \n",
       "104479               1098           1685               59   \n",
       "...                   ...            ...              ...   \n",
       "108649                225          49642                8   \n",
       "108650                661          49642                8   \n",
       "108651                952          49642                8   \n",
       "108652                  0          49642                8   \n",
       "108653               1222          49642                8   \n",
       "\n",
       "        item_cnt_month_lag_1  item_cnt_month_lag_2  item_cnt_month_lag_3   \n",
       "104475                 105.0                  89.0                 101.0  \\\n",
       "104476                  35.0                  32.0                  26.0   \n",
       "104477                  96.0                  87.0                  92.0   \n",
       "104478                  25.0                  29.0                  28.0   \n",
       "104479                  38.0                  35.0                  33.0   \n",
       "...                      ...                   ...                   ...   \n",
       "108649                  31.0                  27.0                  30.0   \n",
       "108650                  31.0                  27.0                  34.0   \n",
       "108651                  31.0                  37.0                  27.0   \n",
       "108652                  37.0                  29.0                  28.0   \n",
       "108653                  33.0                  30.0                  29.0   \n",
       "\n",
       "        item_cnt_month_lag_6  product_avg_sale_last_6   \n",
       "104475                 420.0                 3.096221  \\\n",
       "104476                 131.0                 1.039471   \n",
       "104477                 447.0                 3.176809   \n",
       "104478                 130.0                 1.002096   \n",
       "104479                 119.0                 0.992753   \n",
       "...                      ...                      ...   \n",
       "108649                  33.0                 0.668064   \n",
       "108650                  33.0                 0.606163   \n",
       "108651                  26.0                 0.636530   \n",
       "108652                  37.0                 0.669232   \n",
       "108653                  28.0                 0.680912   \n",
       "\n",
       "        product_std_sale_last_6  product_avg_sale_last_12   \n",
       "104475                 5.194885                  3.904368  \\\n",
       "104476                 1.738017                  1.347013   \n",
       "104477                 5.506434                  3.999085   \n",
       "104478                 1.732555                  1.289802   \n",
       "104479                 1.589500                  1.312686   \n",
       "...                         ...                       ...   \n",
       "108649                 0.107747                  0.694802   \n",
       "108650                 0.067887                  0.687174   \n",
       "108651                 0.140311                  0.701795   \n",
       "108652                 0.077699                  0.699887   \n",
       "108653                 0.082236                  0.708151   \n",
       "\n",
       "        product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "104475                  4.602506                  1.546094  \\\n",
       "104476                  1.638383                  1.546094   \n",
       "104477                  4.806958                  1.546094   \n",
       "104478                  1.566913                  1.546094   \n",
       "104479                  1.534918                  1.546094   \n",
       "...                          ...                       ...   \n",
       "108649                  0.093081                  0.537633   \n",
       "108650                  0.058290                  0.537633   \n",
       "108651                  0.087723                  0.537633   \n",
       "108652                  0.075482                  0.537633   \n",
       "108653                  0.097407                  0.537633   \n",
       "\n",
       "        campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "104475                  2.729624                   1.972577  \\\n",
       "104476                  2.729624                   1.972577   \n",
       "104477                  2.729624                   1.972577   \n",
       "104478                  2.729624                   1.972577   \n",
       "104479                  2.729624                   1.972577   \n",
       "...                          ...                        ...   \n",
       "108649                  0.026420                   0.581802   \n",
       "108650                  0.026420                   0.581802   \n",
       "108651                  0.026420                   0.581802   \n",
       "108652                  0.026420                   0.581802   \n",
       "108653                  0.026420                   0.581802   \n",
       "\n",
       "        campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "104475                   2.380269                   0.795596  \\\n",
       "104476                   2.380269                   2.893949   \n",
       "104477                   2.380269                   0.795596   \n",
       "104478                   2.380269                   2.893949   \n",
       "104479                   2.380269                   2.893949   \n",
       "...                           ...                        ...   \n",
       "108649                   0.024610                   1.024833   \n",
       "108650                   0.024610                   1.024833   \n",
       "108651                   0.024610                   1.024833   \n",
       "108652                   0.024610                   1.024833   \n",
       "108653                   0.024610                   1.024833   \n",
       "\n",
       "        category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "104475                   0.964616                 106.1875  \\\n",
       "104476                   3.497659                 106.1875   \n",
       "104477                   0.964616                 106.1875   \n",
       "104478                   3.497659                 106.1875   \n",
       "104479                   3.497659                 106.1875   \n",
       "...                           ...                      ...   \n",
       "108649                   0.040937                 106.1875   \n",
       "108650                   0.040937                 106.1875   \n",
       "108651                   0.040937                 106.1875   \n",
       "108652                   0.040937                 106.1875   \n",
       "108653                   0.040937                 106.1875   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "104475                           35.6875                           34.9375  \\\n",
       "104476                           35.6875                           34.9375   \n",
       "104477                           35.6875                           34.9375   \n",
       "104478                           35.6875                           34.9375   \n",
       "104479                           35.6875                           34.9375   \n",
       "...                                  ...                               ...   \n",
       "108649                           31.4375                           31.7500   \n",
       "108650                           31.4375                           31.7500   \n",
       "108651                           31.4375                           31.7500   \n",
       "108652                           31.4375                           31.7500   \n",
       "108653                           31.4375                           31.7500   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "104475                         34.250000                        160.500000  \\\n",
       "104476                         34.250000                        160.500000   \n",
       "104477                         34.250000                        160.500000   \n",
       "104478                         34.250000                        160.500000   \n",
       "104479                         34.250000                        160.500000   \n",
       "...                                  ...                               ...   \n",
       "108649                         30.015625                         31.671875   \n",
       "108650                         30.015625                         31.671875   \n",
       "108651                         30.015625                         31.671875   \n",
       "108652                         30.015625                         31.671875   \n",
       "108653                         30.015625                         31.671875   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_1  date_item_avg_item_cnt_lag_2   \n",
       "104475                     99.000000                     87.687500  \\\n",
       "104476                     35.656250                     32.000000   \n",
       "104477                     93.312500                     89.312500   \n",
       "104478                     28.000000                     29.328125   \n",
       "104479                     30.328125                     34.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     33.656250                     26.328125   \n",
       "108650                     27.671875                     27.671875   \n",
       "108651                     33.343750                     28.328125   \n",
       "108652                     32.656250                     29.328125   \n",
       "108653                     28.671875                     33.656250   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_3  date_item_avg_item_cnt_lag_6   \n",
       "104475                    102.312500                    418.000000  \\\n",
       "104476                     29.671875                    140.000000   \n",
       "104477                     97.687500                    438.250000   \n",
       "104478                     27.671875                    138.000000   \n",
       "104479                     33.000000                    130.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     32.000000                     32.656250   \n",
       "108650                     31.671875                     30.328125   \n",
       "108651                     26.000000                     27.328125   \n",
       "108652                     31.671875                     34.000000   \n",
       "108653                     31.671875                     33.343750   \n",
       "\n",
       "        date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag   \n",
       "104475                              84.62500         0.000000  \\\n",
       "104476                              37.87500         0.000000   \n",
       "104477                              84.62500         0.000000   \n",
       "104478                              37.87500         0.000000   \n",
       "104479                              37.87500         0.099731   \n",
       "...                                      ...              ...   \n",
       "108649                              32.59375         0.000000   \n",
       "108650                              32.59375         0.000000   \n",
       "108651                              32.59375         0.000000   \n",
       "108652                              32.59375         0.000000   \n",
       "108653                              32.59375         0.000000   \n",
       "\n",
       "        delta_revenue_lag_1  month  year  item_last_sale   \n",
       "104475             0.098877      2     2            25.0  \\\n",
       "104476             0.098877      2     2            25.0   \n",
       "104477             0.098877      2     2            25.0   \n",
       "104478             0.098877      2     2            25.0   \n",
       "104479             0.098877      2     2            25.0   \n",
       "...                     ...    ...   ...             ...   \n",
       "108649             0.029602      2     2            25.0   \n",
       "108650             0.029602      2     2            25.0   \n",
       "108651             0.029602      2     2            25.0   \n",
       "108652             0.029602      2     2            25.0   \n",
       "108653             0.029602      2     2            25.0   \n",
       "\n",
       "        item_campaign_first_sale  item_first_sale  item_cnt_month_forecast   \n",
       "104475                        25               25               130.760559  \\\n",
       "104476                        25               25                29.155863   \n",
       "104477                        25               25               111.033714   \n",
       "104478                        25               25                29.155863   \n",
       "104479                        25               25                29.155863   \n",
       "...                          ...              ...                      ...   \n",
       "108649                        25               25                32.697990   \n",
       "108650                        25               25                28.326887   \n",
       "108651                        25               25                28.764393   \n",
       "108652                        25               25                28.404255   \n",
       "108653                        25               25                27.454340   \n",
       "\n",
       "        item_cnt_month  target  \n",
       "104475              86      76  \n",
       "104476              31      38  \n",
       "104477              74      80  \n",
       "104478              31      25  \n",
       "104479              31      22  \n",
       "...                ...     ...  \n",
       "108649              32      50  \n",
       "108650              26      27  \n",
       "108651              30      21  \n",
       "108652              29      23  \n",
       "108653              28      30  \n",
       "\n",
       "[4179 rows x 42 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44b5e7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "      <th>item_cnt_month_forecast</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>target</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104475</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>430000</td>\n",
       "      <td>1098</td>\n",
       "      <td>1005</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>105.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.096221</td>\n",
       "      <td>5.194885</td>\n",
       "      <td>3.904368</td>\n",
       "      <td>4.602506</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>87.687500</td>\n",
       "      <td>102.312500</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>130.760559</td>\n",
       "      <td>86</td>\n",
       "      <td>76</td>\n",
       "      <td>215000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104476</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>155000</td>\n",
       "      <td>896</td>\n",
       "      <td>925</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.039471</td>\n",
       "      <td>1.738017</td>\n",
       "      <td>1.347013</td>\n",
       "      <td>1.638383</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104477</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>3700000</td>\n",
       "      <td>121</td>\n",
       "      <td>1052</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>3.176809</td>\n",
       "      <td>5.506434</td>\n",
       "      <td>3.999085</td>\n",
       "      <td>4.806958</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>93.312500</td>\n",
       "      <td>89.312500</td>\n",
       "      <td>97.687500</td>\n",
       "      <td>438.250000</td>\n",
       "      <td>84.62500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>111.033714</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>1850000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104478</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>155000</td>\n",
       "      <td>9</td>\n",
       "      <td>1061</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.732555</td>\n",
       "      <td>1.289802</td>\n",
       "      <td>1.566913</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104479</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>155000</td>\n",
       "      <td>1032</td>\n",
       "      <td>1098</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>1.589500</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.534918</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>160.500000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>37.87500</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108649</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273386993</td>\n",
       "      <td>1600000</td>\n",
       "      <td>293</td>\n",
       "      <td>225</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.668064</td>\n",
       "      <td>0.107747</td>\n",
       "      <td>0.694802</td>\n",
       "      <td>0.093081</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>800000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108650</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273633499</td>\n",
       "      <td>1300000</td>\n",
       "      <td>256</td>\n",
       "      <td>661</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.606163</td>\n",
       "      <td>0.067887</td>\n",
       "      <td>0.687174</td>\n",
       "      <td>0.058290</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.326887</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108651</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>273847739</td>\n",
       "      <td>1500000</td>\n",
       "      <td>903</td>\n",
       "      <td>952</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.636530</td>\n",
       "      <td>0.140311</td>\n",
       "      <td>0.701795</td>\n",
       "      <td>0.087723</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.328125</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>750000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108652</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274069973</td>\n",
       "      <td>1450000</td>\n",
       "      <td>291</td>\n",
       "      <td>0</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.669232</td>\n",
       "      <td>0.077699</td>\n",
       "      <td>0.699887</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>725000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108653</th>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>274070205</td>\n",
       "      <td>1400000</td>\n",
       "      <td>290</td>\n",
       "      <td>1222</td>\n",
       "      <td>49642</td>\n",
       "      <td>8</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.680912</td>\n",
       "      <td>0.082236</td>\n",
       "      <td>0.708151</td>\n",
       "      <td>0.097407</td>\n",
       "      <td>0.537633</td>\n",
       "      <td>0.026420</td>\n",
       "      <td>0.581802</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>1.024833</td>\n",
       "      <td>0.040937</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>31.4375</td>\n",
       "      <td>31.7500</td>\n",
       "      <td>30.015625</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>32.59375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.454340</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>700000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4179 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  profit_month  name   \n",
       "104475              26            1    42899542        430000  1098  \\\n",
       "104476              26            1    61393910        155000   896   \n",
       "104477              26            1    68202834       3700000   121   \n",
       "104478              26            1    73552136        155000     9   \n",
       "104479              26            1    74421745        155000  1032   \n",
       "...                ...          ...         ...           ...   ...   \n",
       "108649              26           48   273386993       1600000   293   \n",
       "108650              26           48   273633499       1300000   256   \n",
       "108651              26           48   273847739       1500000   903   \n",
       "108652              26           48   274069973       1450000   291   \n",
       "108653              26           48   274070205       1400000   290   \n",
       "\n",
       "        short_description  categories_id  categories_name   \n",
       "104475               1005          10378               55  \\\n",
       "104476                925           1685               59   \n",
       "104477               1052          10378               55   \n",
       "104478               1061           1685               59   \n",
       "104479               1098           1685               59   \n",
       "...                   ...            ...              ...   \n",
       "108649                225          49642                8   \n",
       "108650                661          49642                8   \n",
       "108651                952          49642                8   \n",
       "108652                  0          49642                8   \n",
       "108653               1222          49642                8   \n",
       "\n",
       "        item_cnt_month_lag_1  item_cnt_month_lag_2  item_cnt_month_lag_3   \n",
       "104475                 105.0                  89.0                 101.0  \\\n",
       "104476                  35.0                  32.0                  26.0   \n",
       "104477                  96.0                  87.0                  92.0   \n",
       "104478                  25.0                  29.0                  28.0   \n",
       "104479                  38.0                  35.0                  33.0   \n",
       "...                      ...                   ...                   ...   \n",
       "108649                  31.0                  27.0                  30.0   \n",
       "108650                  31.0                  27.0                  34.0   \n",
       "108651                  31.0                  37.0                  27.0   \n",
       "108652                  37.0                  29.0                  28.0   \n",
       "108653                  33.0                  30.0                  29.0   \n",
       "\n",
       "        item_cnt_month_lag_6  product_avg_sale_last_6   \n",
       "104475                 420.0                 3.096221  \\\n",
       "104476                 131.0                 1.039471   \n",
       "104477                 447.0                 3.176809   \n",
       "104478                 130.0                 1.002096   \n",
       "104479                 119.0                 0.992753   \n",
       "...                      ...                      ...   \n",
       "108649                  33.0                 0.668064   \n",
       "108650                  33.0                 0.606163   \n",
       "108651                  26.0                 0.636530   \n",
       "108652                  37.0                 0.669232   \n",
       "108653                  28.0                 0.680912   \n",
       "\n",
       "        product_std_sale_last_6  product_avg_sale_last_12   \n",
       "104475                 5.194885                  3.904368  \\\n",
       "104476                 1.738017                  1.347013   \n",
       "104477                 5.506434                  3.999085   \n",
       "104478                 1.732555                  1.289802   \n",
       "104479                 1.589500                  1.312686   \n",
       "...                         ...                       ...   \n",
       "108649                 0.107747                  0.694802   \n",
       "108650                 0.067887                  0.687174   \n",
       "108651                 0.140311                  0.701795   \n",
       "108652                 0.077699                  0.699887   \n",
       "108653                 0.082236                  0.708151   \n",
       "\n",
       "        product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "104475                  4.602506                  1.546094  \\\n",
       "104476                  1.638383                  1.546094   \n",
       "104477                  4.806958                  1.546094   \n",
       "104478                  1.566913                  1.546094   \n",
       "104479                  1.534918                  1.546094   \n",
       "...                          ...                       ...   \n",
       "108649                  0.093081                  0.537633   \n",
       "108650                  0.058290                  0.537633   \n",
       "108651                  0.087723                  0.537633   \n",
       "108652                  0.075482                  0.537633   \n",
       "108653                  0.097407                  0.537633   \n",
       "\n",
       "        campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "104475                  2.729624                   1.972577  \\\n",
       "104476                  2.729624                   1.972577   \n",
       "104477                  2.729624                   1.972577   \n",
       "104478                  2.729624                   1.972577   \n",
       "104479                  2.729624                   1.972577   \n",
       "...                          ...                        ...   \n",
       "108649                  0.026420                   0.581802   \n",
       "108650                  0.026420                   0.581802   \n",
       "108651                  0.026420                   0.581802   \n",
       "108652                  0.026420                   0.581802   \n",
       "108653                  0.026420                   0.581802   \n",
       "\n",
       "        campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "104475                   2.380269                   0.795596  \\\n",
       "104476                   2.380269                   2.893949   \n",
       "104477                   2.380269                   0.795596   \n",
       "104478                   2.380269                   2.893949   \n",
       "104479                   2.380269                   2.893949   \n",
       "...                           ...                        ...   \n",
       "108649                   0.024610                   1.024833   \n",
       "108650                   0.024610                   1.024833   \n",
       "108651                   0.024610                   1.024833   \n",
       "108652                   0.024610                   1.024833   \n",
       "108653                   0.024610                   1.024833   \n",
       "\n",
       "        category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "104475                   0.964616                 106.1875  \\\n",
       "104476                   3.497659                 106.1875   \n",
       "104477                   0.964616                 106.1875   \n",
       "104478                   3.497659                 106.1875   \n",
       "104479                   3.497659                 106.1875   \n",
       "...                           ...                      ...   \n",
       "108649                   0.040937                 106.1875   \n",
       "108650                   0.040937                 106.1875   \n",
       "108651                   0.040937                 106.1875   \n",
       "108652                   0.040937                 106.1875   \n",
       "108653                   0.040937                 106.1875   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "104475                           35.6875                           34.9375  \\\n",
       "104476                           35.6875                           34.9375   \n",
       "104477                           35.6875                           34.9375   \n",
       "104478                           35.6875                           34.9375   \n",
       "104479                           35.6875                           34.9375   \n",
       "...                                  ...                               ...   \n",
       "108649                           31.4375                           31.7500   \n",
       "108650                           31.4375                           31.7500   \n",
       "108651                           31.4375                           31.7500   \n",
       "108652                           31.4375                           31.7500   \n",
       "108653                           31.4375                           31.7500   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "104475                         34.250000                        160.500000  \\\n",
       "104476                         34.250000                        160.500000   \n",
       "104477                         34.250000                        160.500000   \n",
       "104478                         34.250000                        160.500000   \n",
       "104479                         34.250000                        160.500000   \n",
       "...                                  ...                               ...   \n",
       "108649                         30.015625                         31.671875   \n",
       "108650                         30.015625                         31.671875   \n",
       "108651                         30.015625                         31.671875   \n",
       "108652                         30.015625                         31.671875   \n",
       "108653                         30.015625                         31.671875   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_1  date_item_avg_item_cnt_lag_2   \n",
       "104475                     99.000000                     87.687500  \\\n",
       "104476                     35.656250                     32.000000   \n",
       "104477                     93.312500                     89.312500   \n",
       "104478                     28.000000                     29.328125   \n",
       "104479                     30.328125                     34.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     33.656250                     26.328125   \n",
       "108650                     27.671875                     27.671875   \n",
       "108651                     33.343750                     28.328125   \n",
       "108652                     32.656250                     29.328125   \n",
       "108653                     28.671875                     33.656250   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_3  date_item_avg_item_cnt_lag_6   \n",
       "104475                    102.312500                    418.000000  \\\n",
       "104476                     29.671875                    140.000000   \n",
       "104477                     97.687500                    438.250000   \n",
       "104478                     27.671875                    138.000000   \n",
       "104479                     33.000000                    130.000000   \n",
       "...                              ...                           ...   \n",
       "108649                     32.000000                     32.656250   \n",
       "108650                     31.671875                     30.328125   \n",
       "108651                     26.000000                     27.328125   \n",
       "108652                     31.671875                     34.000000   \n",
       "108653                     31.671875                     33.343750   \n",
       "\n",
       "        date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag   \n",
       "104475                              84.62500         0.000000  \\\n",
       "104476                              37.87500         0.000000   \n",
       "104477                              84.62500         0.000000   \n",
       "104478                              37.87500         0.000000   \n",
       "104479                              37.87500         0.099731   \n",
       "...                                      ...              ...   \n",
       "108649                              32.59375         0.000000   \n",
       "108650                              32.59375         0.000000   \n",
       "108651                              32.59375         0.000000   \n",
       "108652                              32.59375         0.000000   \n",
       "108653                              32.59375         0.000000   \n",
       "\n",
       "        delta_revenue_lag_1  month  year  item_last_sale   \n",
       "104475             0.098877      2     2            25.0  \\\n",
       "104476             0.098877      2     2            25.0   \n",
       "104477             0.098877      2     2            25.0   \n",
       "104478             0.098877      2     2            25.0   \n",
       "104479             0.098877      2     2            25.0   \n",
       "...                     ...    ...   ...             ...   \n",
       "108649             0.029602      2     2            25.0   \n",
       "108650             0.029602      2     2            25.0   \n",
       "108651             0.029602      2     2            25.0   \n",
       "108652             0.029602      2     2            25.0   \n",
       "108653             0.029602      2     2            25.0   \n",
       "\n",
       "        item_campaign_first_sale  item_first_sale  item_cnt_month_forecast   \n",
       "104475                        25               25               130.760559  \\\n",
       "104476                        25               25                29.155863   \n",
       "104477                        25               25               111.033714   \n",
       "104478                        25               25                29.155863   \n",
       "104479                        25               25                29.155863   \n",
       "...                          ...              ...                      ...   \n",
       "108649                        25               25                32.697990   \n",
       "108650                        25               25                28.326887   \n",
       "108651                        25               25                28.764393   \n",
       "108652                        25               25                28.404255   \n",
       "108653                        25               25                27.454340   \n",
       "\n",
       "        item_cnt_month  target     salary  \n",
       "104475              86      76   215000.0  \n",
       "104476              31      38    77500.0  \n",
       "104477              74      80  1850000.0  \n",
       "104478              31      25    77500.0  \n",
       "104479              31      22    77500.0  \n",
       "...                ...     ...        ...  \n",
       "108649              32      50   800000.0  \n",
       "108650              26      27   650000.0  \n",
       "108651              30      21   750000.0  \n",
       "108652              29      23   725000.0  \n",
       "108653              28      30   700000.0  \n",
       "\n",
       "[4179 rows x 43 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_salary(row):\n",
    "    if row['item_cnt_month'] < 100:\n",
    "        return row['profit_month'] * 0.5\n",
    "    elif 100 <= row['item_cnt_month'] < 200:\n",
    "        return row['profit_month'] * 0.52\n",
    "    elif 200 <= row['item_cnt_month'] < 500:\n",
    "        return row['profit_month'] * 0.55\n",
    "    else:\n",
    "        return row['profit_month'] * 0.6\n",
    "\n",
    "df['salary'] = df.apply(calculate_salary, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff75aa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_block_num</th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>profit_month</th>\n",
       "      <th>name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>categories_id</th>\n",
       "      <th>categories_name</th>\n",
       "      <th>item_cnt_month_lag_1</th>\n",
       "      <th>item_cnt_month_lag_2</th>\n",
       "      <th>item_cnt_month_lag_3</th>\n",
       "      <th>item_cnt_month_lag_6</th>\n",
       "      <th>product_avg_sale_last_6</th>\n",
       "      <th>product_std_sale_last_6</th>\n",
       "      <th>product_avg_sale_last_12</th>\n",
       "      <th>product_std_sale_last_12</th>\n",
       "      <th>campaign_avg_sale_last_6</th>\n",
       "      <th>campaign_std_sale_last_6</th>\n",
       "      <th>campaign_avg_sale_last_12</th>\n",
       "      <th>campaign_std_sale_last_12</th>\n",
       "      <th>category_avg_sale_last_12</th>\n",
       "      <th>category_std_sale_last_12</th>\n",
       "      <th>date_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_1</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_2</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_3</th>\n",
       "      <th>date_campaign_avg_item_cnt_lag_6</th>\n",
       "      <th>date_item_avg_item_cnt_lag_1</th>\n",
       "      <th>date_item_avg_item_cnt_lag_2</th>\n",
       "      <th>date_item_avg_item_cnt_lag_3</th>\n",
       "      <th>date_item_avg_item_cnt_lag_6</th>\n",
       "      <th>date_campaign_cat_avg_item_cnt_lag_1</th>\n",
       "      <th>delta_price_lag</th>\n",
       "      <th>delta_revenue_lag_1</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>item_last_sale</th>\n",
       "      <th>item_campaign_first_sale</th>\n",
       "      <th>item_first_sale</th>\n",
       "      <th>item_cnt_month_forecast</th>\n",
       "      <th>item_cnt_month</th>\n",
       "      <th>target</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104475</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>42899542</td>\n",
       "      <td>430000</td>\n",
       "      <td>1098</td>\n",
       "      <td>1005</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>105.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>3.096221</td>\n",
       "      <td>5.194885</td>\n",
       "      <td>3.904368</td>\n",
       "      <td>4.602506</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>87.687500</td>\n",
       "      <td>102.312500</td>\n",
       "      <td>418.0000</td>\n",
       "      <td>84.6250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>130.760559</td>\n",
       "      <td>86</td>\n",
       "      <td>76</td>\n",
       "      <td>215000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104476</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>61393910</td>\n",
       "      <td>155000</td>\n",
       "      <td>896</td>\n",
       "      <td>925</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.039471</td>\n",
       "      <td>1.738017</td>\n",
       "      <td>1.347013</td>\n",
       "      <td>1.638383</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>140.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104477</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>68202834</td>\n",
       "      <td>3700000</td>\n",
       "      <td>121</td>\n",
       "      <td>1052</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>447.0</td>\n",
       "      <td>3.176809</td>\n",
       "      <td>5.506434</td>\n",
       "      <td>3.999085</td>\n",
       "      <td>4.806958</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>93.312500</td>\n",
       "      <td>89.312500</td>\n",
       "      <td>97.687500</td>\n",
       "      <td>438.2500</td>\n",
       "      <td>84.6250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>111.033714</td>\n",
       "      <td>74</td>\n",
       "      <td>80</td>\n",
       "      <td>1850000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104478</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>73552136</td>\n",
       "      <td>155000</td>\n",
       "      <td>9</td>\n",
       "      <td>1061</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.732555</td>\n",
       "      <td>1.289802</td>\n",
       "      <td>1.566913</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>138.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104479</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>74421745</td>\n",
       "      <td>155000</td>\n",
       "      <td>1032</td>\n",
       "      <td>1098</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>1.589500</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.534918</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>130.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.099731</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104480</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>75329276</td>\n",
       "      <td>110000</td>\n",
       "      <td>1040</td>\n",
       "      <td>1104</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>26.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.035967</td>\n",
       "      <td>1.803509</td>\n",
       "      <td>1.301244</td>\n",
       "      <td>1.530410</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>143.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.381355</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>55000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104481</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>75329760</td>\n",
       "      <td>150000</td>\n",
       "      <td>961</td>\n",
       "      <td>284</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1.024287</td>\n",
       "      <td>1.802207</td>\n",
       "      <td>1.334300</td>\n",
       "      <td>1.644917</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>142.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104482</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>76091452</td>\n",
       "      <td>170000</td>\n",
       "      <td>1272</td>\n",
       "      <td>701</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>41.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1.074509</td>\n",
       "      <td>1.661806</td>\n",
       "      <td>1.342563</td>\n",
       "      <td>1.576973</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>36.343750</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>137.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.772161</td>\n",
       "      <td>34</td>\n",
       "      <td>38</td>\n",
       "      <td>85000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104483</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>76695833</td>\n",
       "      <td>115000</td>\n",
       "      <td>1083</td>\n",
       "      <td>321</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>21.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.028959</td>\n",
       "      <td>1.882043</td>\n",
       "      <td>1.333028</td>\n",
       "      <td>1.603296</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>26.671875</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>26.671875</td>\n",
       "      <td>147.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.816744</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104484</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>76705126</td>\n",
       "      <td>145000</td>\n",
       "      <td>1293</td>\n",
       "      <td>35</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.037135</td>\n",
       "      <td>1.723461</td>\n",
       "      <td>1.308872</td>\n",
       "      <td>1.523244</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104485</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>80978495</td>\n",
       "      <td>105000</td>\n",
       "      <td>1297</td>\n",
       "      <td>1060</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.062829</td>\n",
       "      <td>2.001522</td>\n",
       "      <td>1.342563</td>\n",
       "      <td>1.694920</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>154.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.628382</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>52500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104486</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>81153213</td>\n",
       "      <td>100000</td>\n",
       "      <td>943</td>\n",
       "      <td>135</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1.000928</td>\n",
       "      <td>1.807898</td>\n",
       "      <td>1.321586</td>\n",
       "      <td>1.657881</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>24.671875</td>\n",
       "      <td>141.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.270302</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104487</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>81153341</td>\n",
       "      <td>175000</td>\n",
       "      <td>1217</td>\n",
       "      <td>134</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.041806</td>\n",
       "      <td>1.626912</td>\n",
       "      <td>1.252932</td>\n",
       "      <td>1.365948</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>134.3750</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.565147</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>87500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104488</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>82819910</td>\n",
       "      <td>150000</td>\n",
       "      <td>1041</td>\n",
       "      <td>1178</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1.019616</td>\n",
       "      <td>1.827048</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.584556</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>143.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104489</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>83160842</td>\n",
       "      <td>140000</td>\n",
       "      <td>1081</td>\n",
       "      <td>704</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>24.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.032463</td>\n",
       "      <td>1.762772</td>\n",
       "      <td>1.334935</td>\n",
       "      <td>1.623072</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>141.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104490</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>85269397</td>\n",
       "      <td>120000</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.974066</td>\n",
       "      <td>1.585801</td>\n",
       "      <td>1.293616</td>\n",
       "      <td>1.559280</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>129.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.903709</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104491</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>87981660</td>\n",
       "      <td>390000</td>\n",
       "      <td>120</td>\n",
       "      <td>356</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>3.163962</td>\n",
       "      <td>5.402305</td>\n",
       "      <td>4.033412</td>\n",
       "      <td>4.824557</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>92.312500</td>\n",
       "      <td>98.312500</td>\n",
       "      <td>94.687500</td>\n",
       "      <td>432.2500</td>\n",
       "      <td>84.6250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>109.038353</td>\n",
       "      <td>78</td>\n",
       "      <td>76</td>\n",
       "      <td>195000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104492</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>94794850</td>\n",
       "      <td>120000</td>\n",
       "      <td>1305</td>\n",
       "      <td>1189</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>31.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.028959</td>\n",
       "      <td>1.811469</td>\n",
       "      <td>1.311415</td>\n",
       "      <td>1.582842</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>143.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.903709</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104493</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>96160421</td>\n",
       "      <td>125000</td>\n",
       "      <td>1301</td>\n",
       "      <td>1187</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>31.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>0.990417</td>\n",
       "      <td>1.517133</td>\n",
       "      <td>1.321586</td>\n",
       "      <td>1.566468</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>126.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.648529</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>62500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104494</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>97057130</td>\n",
       "      <td>160000</td>\n",
       "      <td>1092</td>\n",
       "      <td>1121</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>31.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.109547</td>\n",
       "      <td>1.942607</td>\n",
       "      <td>1.323493</td>\n",
       "      <td>1.564501</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>154.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.685987</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104495</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>98642412</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1273</td>\n",
       "      <td>1182</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.114219</td>\n",
       "      <td>2.145656</td>\n",
       "      <td>1.385154</td>\n",
       "      <td>1.729813</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>164.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>24.572157</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104496</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>98986504</td>\n",
       "      <td>1650000</td>\n",
       "      <td>1274</td>\n",
       "      <td>1181</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>1.007936</td>\n",
       "      <td>1.575913</td>\n",
       "      <td>1.301244</td>\n",
       "      <td>1.563163</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>130.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>825000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104497</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>99156609</td>\n",
       "      <td>145000</td>\n",
       "      <td>1090</td>\n",
       "      <td>1116</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.947203</td>\n",
       "      <td>1.626761</td>\n",
       "      <td>1.287895</td>\n",
       "      <td>1.572182</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>25.328125</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>129.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>43</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104498</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>99183754</td>\n",
       "      <td>140000</td>\n",
       "      <td>1038</td>\n",
       "      <td>1115</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>30.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.960050</td>\n",
       "      <td>1.550052</td>\n",
       "      <td>1.267553</td>\n",
       "      <td>1.464437</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>126.3125</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>28</td>\n",
       "      <td>39</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104499</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>99325284</td>\n",
       "      <td>165000</td>\n",
       "      <td>1097</td>\n",
       "      <td>908</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>1.092028</td>\n",
       "      <td>1.975795</td>\n",
       "      <td>1.396597</td>\n",
       "      <td>1.723424</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>155.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.551554</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>82500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104500</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>99508026</td>\n",
       "      <td>150000</td>\n",
       "      <td>964</td>\n",
       "      <td>965</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>25.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1.068669</td>\n",
       "      <td>1.888778</td>\n",
       "      <td>1.341292</td>\n",
       "      <td>1.571325</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>149.3750</td>\n",
       "      <td>84.6250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>38</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104501</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>103076103</td>\n",
       "      <td>425000</td>\n",
       "      <td>923</td>\n",
       "      <td>1081</td>\n",
       "      <td>10378</td>\n",
       "      <td>55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>438.0</td>\n",
       "      <td>3.104396</td>\n",
       "      <td>4.958089</td>\n",
       "      <td>4.038497</td>\n",
       "      <td>4.816713</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.795596</td>\n",
       "      <td>0.964616</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>96.312500</td>\n",
       "      <td>96.312500</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>406.2500</td>\n",
       "      <td>84.6250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>130.760559</td>\n",
       "      <td>85</td>\n",
       "      <td>96</td>\n",
       "      <td>212500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104502</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>105275706</td>\n",
       "      <td>140000</td>\n",
       "      <td>1089</td>\n",
       "      <td>1122</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>35.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.075677</td>\n",
       "      <td>1.940261</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.577636</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>152.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104503</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>105344835</td>\n",
       "      <td>185000</td>\n",
       "      <td>1035</td>\n",
       "      <td>1100</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.986913</td>\n",
       "      <td>1.624543</td>\n",
       "      <td>1.284081</td>\n",
       "      <td>1.553364</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>131.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>33.618092</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>92500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104504</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>107338883</td>\n",
       "      <td>175000</td>\n",
       "      <td>1270</td>\n",
       "      <td>1192</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.083852</td>\n",
       "      <td>1.793150</td>\n",
       "      <td>1.350192</td>\n",
       "      <td>1.544639</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>145.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.020782</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.565147</td>\n",
       "      <td>35</td>\n",
       "      <td>46</td>\n",
       "      <td>87500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104505</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>107411611</td>\n",
       "      <td>105000</td>\n",
       "      <td>1271</td>\n",
       "      <td>1192</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1.024287</td>\n",
       "      <td>1.541502</td>\n",
       "      <td>1.296794</td>\n",
       "      <td>1.502049</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>129.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.050842</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.628382</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>52500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104506</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>108772138</td>\n",
       "      <td>150000</td>\n",
       "      <td>1085</td>\n",
       "      <td>346</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>34.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.049982</td>\n",
       "      <td>1.714050</td>\n",
       "      <td>1.305694</td>\n",
       "      <td>1.496636</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>139.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>50</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104507</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>110309352</td>\n",
       "      <td>240000</td>\n",
       "      <td>955</td>\n",
       "      <td>224</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>2.004193</td>\n",
       "      <td>3.169660</td>\n",
       "      <td>2.507771</td>\n",
       "      <td>2.887246</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>64.687500</td>\n",
       "      <td>260.7500</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>78.766464</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "      <td>120000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104508</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>110730140</td>\n",
       "      <td>105000</td>\n",
       "      <td>1295</td>\n",
       "      <td>438</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>26.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.957714</td>\n",
       "      <td>1.570774</td>\n",
       "      <td>1.294252</td>\n",
       "      <td>1.574229</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>25.328125</td>\n",
       "      <td>127.3125</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.628382</td>\n",
       "      <td>21</td>\n",
       "      <td>37</td>\n",
       "      <td>52500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104509</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>110938120</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1033</td>\n",
       "      <td>349</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.014944</td>\n",
       "      <td>1.758884</td>\n",
       "      <td>1.353370</td>\n",
       "      <td>1.655382</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>140.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>750000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104510</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>114307325</td>\n",
       "      <td>1250000</td>\n",
       "      <td>1299</td>\n",
       "      <td>1186</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>39.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.071005</td>\n",
       "      <td>1.789271</td>\n",
       "      <td>1.308237</td>\n",
       "      <td>1.527901</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>144.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.986921</td>\n",
       "      <td>25</td>\n",
       "      <td>42</td>\n",
       "      <td>625000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104511</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>114671372</td>\n",
       "      <td>1100000</td>\n",
       "      <td>1298</td>\n",
       "      <td>1185</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.039471</td>\n",
       "      <td>1.812675</td>\n",
       "      <td>1.369898</td>\n",
       "      <td>1.717188</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>23.671875</td>\n",
       "      <td>143.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>24.659122</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>550000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104512</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>117031543</td>\n",
       "      <td>1450000</td>\n",
       "      <td>1276</td>\n",
       "      <td>1183</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>32.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.954211</td>\n",
       "      <td>1.539979</td>\n",
       "      <td>1.251025</td>\n",
       "      <td>1.445371</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>125.6875</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.421078</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>725000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104513</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>117059903</td>\n",
       "      <td>1300000</td>\n",
       "      <td>1277</td>\n",
       "      <td>1184</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>37.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.853592</td>\n",
       "      <td>1.282174</td>\n",
       "      <td>1.520716</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>144.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.248323</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104514</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>117582667</td>\n",
       "      <td>130000</td>\n",
       "      <td>968</td>\n",
       "      <td>29</td>\n",
       "      <td>67298</td>\n",
       "      <td>56</td>\n",
       "      <td>22.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>0.985745</td>\n",
       "      <td>1.748863</td>\n",
       "      <td>1.267553</td>\n",
       "      <td>1.544023</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.058236</td>\n",
       "      <td>0.072299</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>138.0000</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>0.032013</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.492908</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104515</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>119056035</td>\n",
       "      <td>135000</td>\n",
       "      <td>1222</td>\n",
       "      <td>1154</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>25.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1.035967</td>\n",
       "      <td>2.067354</td>\n",
       "      <td>1.309508</td>\n",
       "      <td>1.670601</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>25.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>157.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.492908</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104516</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>126596446</td>\n",
       "      <td>140000</td>\n",
       "      <td>1279</td>\n",
       "      <td>1193</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.031295</td>\n",
       "      <td>1.891291</td>\n",
       "      <td>1.301244</td>\n",
       "      <td>1.546220</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>147.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104517</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>129297086</td>\n",
       "      <td>115000</td>\n",
       "      <td>1105</td>\n",
       "      <td>152</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.012608</td>\n",
       "      <td>1.525362</td>\n",
       "      <td>1.305694</td>\n",
       "      <td>1.482522</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>127.6875</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.816744</td>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104518</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>129297638</td>\n",
       "      <td>175000</td>\n",
       "      <td>1304</td>\n",
       "      <td>102</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>36.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.014944</td>\n",
       "      <td>1.689668</td>\n",
       "      <td>1.331757</td>\n",
       "      <td>1.581023</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>136.3750</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.565147</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>87500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104519</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>129298313</td>\n",
       "      <td>170000</td>\n",
       "      <td>1104</td>\n",
       "      <td>184</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1.094364</td>\n",
       "      <td>1.833713</td>\n",
       "      <td>1.294252</td>\n",
       "      <td>1.474047</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>147.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.772161</td>\n",
       "      <td>34</td>\n",
       "      <td>46</td>\n",
       "      <td>85000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104520</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>129300500</td>\n",
       "      <td>125000</td>\n",
       "      <td>1207</td>\n",
       "      <td>153</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>23.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.006768</td>\n",
       "      <td>1.844850</td>\n",
       "      <td>1.296159</td>\n",
       "      <td>1.542122</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>24.328125</td>\n",
       "      <td>144.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.648529</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>62500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104521</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>129300910</td>\n",
       "      <td>150000</td>\n",
       "      <td>1102</td>\n",
       "      <td>848</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>36.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>0.967058</td>\n",
       "      <td>1.471089</td>\n",
       "      <td>1.253568</td>\n",
       "      <td>1.484252</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>122.6875</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104522</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>129301513</td>\n",
       "      <td>1500000</td>\n",
       "      <td>1206</td>\n",
       "      <td>154</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.953043</td>\n",
       "      <td>1.473815</td>\n",
       "      <td>1.279631</td>\n",
       "      <td>1.463212</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>122.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>750000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104523</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133689372</td>\n",
       "      <td>115000</td>\n",
       "      <td>1282</td>\n",
       "      <td>114</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>34.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.993921</td>\n",
       "      <td>1.504362</td>\n",
       "      <td>1.292344</td>\n",
       "      <td>1.493500</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>125.6875</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.816744</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104524</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133689728</td>\n",
       "      <td>1200000</td>\n",
       "      <td>1212</td>\n",
       "      <td>156</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>1.045310</td>\n",
       "      <td>1.925492</td>\n",
       "      <td>1.310779</td>\n",
       "      <td>1.592077</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>150.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>25.362192</td>\n",
       "      <td>24</td>\n",
       "      <td>40</td>\n",
       "      <td>600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104525</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133689754</td>\n",
       "      <td>1650000</td>\n",
       "      <td>1285</td>\n",
       "      <td>114</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.997425</td>\n",
       "      <td>1.602966</td>\n",
       "      <td>1.292344</td>\n",
       "      <td>1.507545</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>131.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>33</td>\n",
       "      <td>46</td>\n",
       "      <td>825000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104526</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133690045</td>\n",
       "      <td>1450000</td>\n",
       "      <td>1106</td>\n",
       "      <td>143</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.048814</td>\n",
       "      <td>1.978758</td>\n",
       "      <td>1.312686</td>\n",
       "      <td>1.604411</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>27.328125</td>\n",
       "      <td>153.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.421078</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "      <td>725000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104527</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133690424</td>\n",
       "      <td>1400000</td>\n",
       "      <td>1302</td>\n",
       "      <td>114</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>31.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.048814</td>\n",
       "      <td>1.826674</td>\n",
       "      <td>1.327307</td>\n",
       "      <td>1.556001</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>145.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.375776</td>\n",
       "      <td>28</td>\n",
       "      <td>40</td>\n",
       "      <td>700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104528</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133690735</td>\n",
       "      <td>1300000</td>\n",
       "      <td>1289</td>\n",
       "      <td>162</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1.025455</td>\n",
       "      <td>1.737664</td>\n",
       "      <td>1.296794</td>\n",
       "      <td>1.482805</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>139.3750</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.248323</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104529</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133690793</td>\n",
       "      <td>1800000</td>\n",
       "      <td>1284</td>\n",
       "      <td>164</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.005600</td>\n",
       "      <td>1.501088</td>\n",
       "      <td>1.308872</td>\n",
       "      <td>1.531916</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>126.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>34.101162</td>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104530</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133690844</td>\n",
       "      <td>1650000</td>\n",
       "      <td>1283</td>\n",
       "      <td>160</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>33.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.067501</td>\n",
       "      <td>1.691001</td>\n",
       "      <td>1.336842</td>\n",
       "      <td>1.568258</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.697990</td>\n",
       "      <td>33</td>\n",
       "      <td>46</td>\n",
       "      <td>825000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104531</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133691096</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1312</td>\n",
       "      <td>114</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.970562</td>\n",
       "      <td>1.606973</td>\n",
       "      <td>1.265646</td>\n",
       "      <td>1.443778</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>129.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>24.659122</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104532</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133813512</td>\n",
       "      <td>180000</td>\n",
       "      <td>1335</td>\n",
       "      <td>700</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.930852</td>\n",
       "      <td>1.395739</td>\n",
       "      <td>1.247847</td>\n",
       "      <td>1.500270</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>117.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.989952</td>\n",
       "      <td>36</td>\n",
       "      <td>43</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104533</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133813532</td>\n",
       "      <td>160000</td>\n",
       "      <td>1339</td>\n",
       "      <td>99</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.000928</td>\n",
       "      <td>1.708684</td>\n",
       "      <td>1.313322</td>\n",
       "      <td>1.574109</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>136.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.685987</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104534</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133813786</td>\n",
       "      <td>150000</td>\n",
       "      <td>1340</td>\n",
       "      <td>169</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>36.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.041806</td>\n",
       "      <td>1.843393</td>\n",
       "      <td>1.287895</td>\n",
       "      <td>1.503569</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>145.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104535</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133813937</td>\n",
       "      <td>115000</td>\n",
       "      <td>1319</td>\n",
       "      <td>1126</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>36.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.026623</td>\n",
       "      <td>1.793910</td>\n",
       "      <td>1.324764</td>\n",
       "      <td>1.596733</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>142.3750</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.816744</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104536</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814062</td>\n",
       "      <td>135000</td>\n",
       "      <td>1281</td>\n",
       "      <td>163</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.062829</td>\n",
       "      <td>1.697926</td>\n",
       "      <td>1.358456</td>\n",
       "      <td>1.608205</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>35.343750</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.967983</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104537</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814339</td>\n",
       "      <td>130000</td>\n",
       "      <td>1287</td>\n",
       "      <td>99</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.052318</td>\n",
       "      <td>1.763519</td>\n",
       "      <td>1.278359</td>\n",
       "      <td>1.428049</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>37.343750</td>\n",
       "      <td>141.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.492908</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104538</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814376</td>\n",
       "      <td>150000</td>\n",
       "      <td>1288</td>\n",
       "      <td>161</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>28.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.968226</td>\n",
       "      <td>1.400094</td>\n",
       "      <td>1.261196</td>\n",
       "      <td>1.417158</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>119.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104539</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814455</td>\n",
       "      <td>130000</td>\n",
       "      <td>1280</td>\n",
       "      <td>21</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>28.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.023119</td>\n",
       "      <td>1.694765</td>\n",
       "      <td>1.322222</td>\n",
       "      <td>1.532755</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>137.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.492908</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104540</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814623</td>\n",
       "      <td>155000</td>\n",
       "      <td>1103</td>\n",
       "      <td>180</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.003264</td>\n",
       "      <td>1.640440</td>\n",
       "      <td>1.322222</td>\n",
       "      <td>1.572380</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>25.328125</td>\n",
       "      <td>133.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104541</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814658</td>\n",
       "      <td>145000</td>\n",
       "      <td>1303</td>\n",
       "      <td>20</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.016112</td>\n",
       "      <td>1.739998</td>\n",
       "      <td>1.317772</td>\n",
       "      <td>1.595479</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104542</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133814884</td>\n",
       "      <td>145000</td>\n",
       "      <td>1219</td>\n",
       "      <td>165</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>2.034539</td>\n",
       "      <td>1.336842</td>\n",
       "      <td>1.644698</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>158.6250</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104543</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>133815093</td>\n",
       "      <td>100000</td>\n",
       "      <td>956</td>\n",
       "      <td>180</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>30.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>1.010272</td>\n",
       "      <td>1.590457</td>\n",
       "      <td>1.248482</td>\n",
       "      <td>1.394793</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>131.0000</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.270302</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104544</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>136157116</td>\n",
       "      <td>140000</td>\n",
       "      <td>1218</td>\n",
       "      <td>1225</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>25.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.026623</td>\n",
       "      <td>1.850227</td>\n",
       "      <td>1.312051</td>\n",
       "      <td>1.599560</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>145.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104545</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>136792823</td>\n",
       "      <td>1350000</td>\n",
       "      <td>1214</td>\n",
       "      <td>1152</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1.013776</td>\n",
       "      <td>1.806604</td>\n",
       "      <td>1.287259</td>\n",
       "      <td>1.541928</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>25.671875</td>\n",
       "      <td>142.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.723398</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>675000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104546</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>137611546</td>\n",
       "      <td>105000</td>\n",
       "      <td>1091</td>\n",
       "      <td>1124</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.006768</td>\n",
       "      <td>1.492679</td>\n",
       "      <td>1.314593</td>\n",
       "      <td>1.577966</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>125.6875</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.628382</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>52500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104547</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>138381315</td>\n",
       "      <td>160000</td>\n",
       "      <td>1286</td>\n",
       "      <td>1069</td>\n",
       "      <td>8336</td>\n",
       "      <td>57</td>\n",
       "      <td>30.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.012608</td>\n",
       "      <td>1.625115</td>\n",
       "      <td>1.338749</td>\n",
       "      <td>1.629896</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.358969</td>\n",
       "      <td>0.431650</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>132.6250</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>-0.022202</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.685987</td>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104548</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>138748071</td>\n",
       "      <td>150000</td>\n",
       "      <td>1216</td>\n",
       "      <td>60</td>\n",
       "      <td>8336</td>\n",
       "      <td>57</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1.023119</td>\n",
       "      <td>1.611227</td>\n",
       "      <td>1.289166</td>\n",
       "      <td>1.476349</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.358969</td>\n",
       "      <td>0.431650</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.343750</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>132.6250</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>39</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104549</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>138748934</td>\n",
       "      <td>135000</td>\n",
       "      <td>1300</td>\n",
       "      <td>60</td>\n",
       "      <td>8336</td>\n",
       "      <td>57</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1.012608</td>\n",
       "      <td>1.806831</td>\n",
       "      <td>1.322222</td>\n",
       "      <td>1.625160</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.358969</td>\n",
       "      <td>0.431650</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>142.3750</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.967983</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104550</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>138750674</td>\n",
       "      <td>425000</td>\n",
       "      <td>1300</td>\n",
       "      <td>60</td>\n",
       "      <td>8336</td>\n",
       "      <td>57</td>\n",
       "      <td>83.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>3.015633</td>\n",
       "      <td>4.963756</td>\n",
       "      <td>3.863048</td>\n",
       "      <td>4.499415</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>0.358969</td>\n",
       "      <td>0.431650</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>90.312500</td>\n",
       "      <td>89.687500</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>402.2500</td>\n",
       "      <td>45.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>109.860443</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>212500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104551</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>142409966</td>\n",
       "      <td>155000</td>\n",
       "      <td>1093</td>\n",
       "      <td>1105</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.985745</td>\n",
       "      <td>1.766703</td>\n",
       "      <td>1.284081</td>\n",
       "      <td>1.561693</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>26.328125</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104552</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>146289287</td>\n",
       "      <td>120000</td>\n",
       "      <td>1294</td>\n",
       "      <td>1155</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>33.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>1.004432</td>\n",
       "      <td>1.596865</td>\n",
       "      <td>1.318408</td>\n",
       "      <td>1.583972</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>131.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.903709</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>60000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104553</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>150568002</td>\n",
       "      <td>115000</td>\n",
       "      <td>53</td>\n",
       "      <td>336</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>34.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>1.013776</td>\n",
       "      <td>1.754727</td>\n",
       "      <td>1.267553</td>\n",
       "      <td>1.531626</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>139.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.816744</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>57500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104554</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>150685095</td>\n",
       "      <td>135000</td>\n",
       "      <td>1082</td>\n",
       "      <td>1117</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.040639</td>\n",
       "      <td>1.629963</td>\n",
       "      <td>1.348920</td>\n",
       "      <td>1.588065</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>37.656250</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>134.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.967983</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104555</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>150695791</td>\n",
       "      <td>145000</td>\n",
       "      <td>1094</td>\n",
       "      <td>907</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>33.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.072173</td>\n",
       "      <td>1.714419</td>\n",
       "      <td>1.310144</td>\n",
       "      <td>1.481718</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>140.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104556</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>150731971</td>\n",
       "      <td>150000</td>\n",
       "      <td>1095</td>\n",
       "      <td>1123</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1.002096</td>\n",
       "      <td>1.734164</td>\n",
       "      <td>1.307601</td>\n",
       "      <td>1.602712</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>138.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.662470</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104557</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>150840081</td>\n",
       "      <td>145000</td>\n",
       "      <td>1096</td>\n",
       "      <td>908</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>1.044142</td>\n",
       "      <td>1.714042</td>\n",
       "      <td>1.288530</td>\n",
       "      <td>1.477340</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>35.343750</td>\n",
       "      <td>139.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104558</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>155563815</td>\n",
       "      <td>95000</td>\n",
       "      <td>1282</td>\n",
       "      <td>114</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>29.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.009104</td>\n",
       "      <td>1.715688</td>\n",
       "      <td>1.271367</td>\n",
       "      <td>1.504135</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>137.3750</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.270302</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>47500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104559</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>155564180</td>\n",
       "      <td>1350000</td>\n",
       "      <td>1211</td>\n",
       "      <td>155</td>\n",
       "      <td>67250</td>\n",
       "      <td>60</td>\n",
       "      <td>33.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.992753</td>\n",
       "      <td>1.634026</td>\n",
       "      <td>1.218605</td>\n",
       "      <td>1.336132</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.905857</td>\n",
       "      <td>2.264835</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>132.3750</td>\n",
       "      <td>30.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.723398</td>\n",
       "      <td>27</td>\n",
       "      <td>44</td>\n",
       "      <td>675000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104560</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>158720374</td>\n",
       "      <td>150000</td>\n",
       "      <td>1311</td>\n",
       "      <td>1194</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>36.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>1.059326</td>\n",
       "      <td>1.781842</td>\n",
       "      <td>1.337478</td>\n",
       "      <td>1.593906</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>143.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.764393</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>75000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104561</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>159382275</td>\n",
       "      <td>135000</td>\n",
       "      <td>1223</td>\n",
       "      <td>1156</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>23.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>1.046478</td>\n",
       "      <td>1.889784</td>\n",
       "      <td>1.357184</td>\n",
       "      <td>1.669331</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>148.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.967983</td>\n",
       "      <td>27</td>\n",
       "      <td>31</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104562</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>160102489</td>\n",
       "      <td>1450000</td>\n",
       "      <td>1306</td>\n",
       "      <td>1188</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>40.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1.035967</td>\n",
       "      <td>1.902122</td>\n",
       "      <td>1.381976</td>\n",
       "      <td>1.752238</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>148.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.421078</td>\n",
       "      <td>29</td>\n",
       "      <td>40</td>\n",
       "      <td>725000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104563</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>160393397</td>\n",
       "      <td>135000</td>\n",
       "      <td>1308</td>\n",
       "      <td>1190</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>28.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>1.021951</td>\n",
       "      <td>1.780028</td>\n",
       "      <td>1.270096</td>\n",
       "      <td>1.442512</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>25.671875</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>141.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.967983</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104564</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>161455980</td>\n",
       "      <td>140000</td>\n",
       "      <td>1088</td>\n",
       "      <td>1120</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>1.062829</td>\n",
       "      <td>1.912860</td>\n",
       "      <td>1.328578</td>\n",
       "      <td>1.615362</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>150.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.620361</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>70000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104565</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>163552063</td>\n",
       "      <td>185000</td>\n",
       "      <td>1087</td>\n",
       "      <td>1119</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>30.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.051150</td>\n",
       "      <td>1.853666</td>\n",
       "      <td>1.369262</td>\n",
       "      <td>1.632617</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>146.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>33.618092</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>92500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104566</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>163553232</td>\n",
       "      <td>135000</td>\n",
       "      <td>1034</td>\n",
       "      <td>1099</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>32.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.013776</td>\n",
       "      <td>1.683705</td>\n",
       "      <td>1.338114</td>\n",
       "      <td>1.612597</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>38.343750</td>\n",
       "      <td>26.671875</td>\n",
       "      <td>135.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>27.967983</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>67500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104567</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>164251213</td>\n",
       "      <td>145000</td>\n",
       "      <td>639</td>\n",
       "      <td>1221</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>28.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.013776</td>\n",
       "      <td>1.629398</td>\n",
       "      <td>1.329850</td>\n",
       "      <td>1.565094</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>36.656250</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>133.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104568</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>164338704</td>\n",
       "      <td>160000</td>\n",
       "      <td>1084</td>\n",
       "      <td>1118</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.986913</td>\n",
       "      <td>1.779799</td>\n",
       "      <td>1.268189</td>\n",
       "      <td>1.501363</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>139.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.685987</td>\n",
       "      <td>32</td>\n",
       "      <td>42</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104569</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>164344210</td>\n",
       "      <td>105000</td>\n",
       "      <td>1037</td>\n",
       "      <td>1102</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>26.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1.054654</td>\n",
       "      <td>1.723042</td>\n",
       "      <td>1.323493</td>\n",
       "      <td>1.555722</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>140.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.628382</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>52500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104570</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>164756938</td>\n",
       "      <td>145000</td>\n",
       "      <td>1291</td>\n",
       "      <td>994</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>1.073341</td>\n",
       "      <td>1.847019</td>\n",
       "      <td>1.357820</td>\n",
       "      <td>1.631526</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>147.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.404255</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>72500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104571</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>170324448</td>\n",
       "      <td>165000</td>\n",
       "      <td>1036</td>\n",
       "      <td>1101</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.042974</td>\n",
       "      <td>1.920965</td>\n",
       "      <td>1.326671</td>\n",
       "      <td>1.656614</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>149.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.551554</td>\n",
       "      <td>33</td>\n",
       "      <td>29</td>\n",
       "      <td>82500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104572</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>172006037</td>\n",
       "      <td>165000</td>\n",
       "      <td>1213</td>\n",
       "      <td>196</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>26.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.054654</td>\n",
       "      <td>1.986342</td>\n",
       "      <td>1.322222</td>\n",
       "      <td>1.550306</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>153.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.551554</td>\n",
       "      <td>33</td>\n",
       "      <td>51</td>\n",
       "      <td>82500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104573</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>172120849</td>\n",
       "      <td>950000</td>\n",
       "      <td>1039</td>\n",
       "      <td>1103</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>0.986913</td>\n",
       "      <td>1.631396</td>\n",
       "      <td>1.305694</td>\n",
       "      <td>1.571618</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>132.0000</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>33.861290</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>475000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104574</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>172305521</td>\n",
       "      <td>165000</td>\n",
       "      <td>1309</td>\n",
       "      <td>1191</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>34.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.972898</td>\n",
       "      <td>1.582418</td>\n",
       "      <td>1.298066</td>\n",
       "      <td>1.584133</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>34.656250</td>\n",
       "      <td>23.328125</td>\n",
       "      <td>128.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.551554</td>\n",
       "      <td>33</td>\n",
       "      <td>23</td>\n",
       "      <td>82500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104575</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>174037915</td>\n",
       "      <td>105000</td>\n",
       "      <td>1278</td>\n",
       "      <td>298</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>30.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>0.960050</td>\n",
       "      <td>1.463347</td>\n",
       "      <td>1.259289</td>\n",
       "      <td>1.449732</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>122.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>22.628382</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>52500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104576</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>176250783</td>\n",
       "      <td>395000</td>\n",
       "      <td>1290</td>\n",
       "      <td>1049</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>91.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>413.0</td>\n",
       "      <td>3.061182</td>\n",
       "      <td>5.201676</td>\n",
       "      <td>3.898647</td>\n",
       "      <td>4.652738</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>417.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>122.366272</td>\n",
       "      <td>79</td>\n",
       "      <td>64</td>\n",
       "      <td>197500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104577</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>176251118</td>\n",
       "      <td>4150000</td>\n",
       "      <td>118</td>\n",
       "      <td>1027</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>87.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>434.0</td>\n",
       "      <td>3.074030</td>\n",
       "      <td>5.086373</td>\n",
       "      <td>3.993364</td>\n",
       "      <td>4.758298</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>87.687500</td>\n",
       "      <td>85.687500</td>\n",
       "      <td>97.312500</td>\n",
       "      <td>411.2500</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>180.285431</td>\n",
       "      <td>83</td>\n",
       "      <td>77</td>\n",
       "      <td>2075000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104578</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>178058234</td>\n",
       "      <td>155000</td>\n",
       "      <td>1296</td>\n",
       "      <td>297</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>1.027791</td>\n",
       "      <td>1.645641</td>\n",
       "      <td>1.333028</td>\n",
       "      <td>1.583472</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.656250</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>134.6250</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.155863</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>77500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104579</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>178785868</td>\n",
       "      <td>1450000</td>\n",
       "      <td>1215</td>\n",
       "      <td>1153</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>32.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1.032463</td>\n",
       "      <td>1.848515</td>\n",
       "      <td>1.338749</td>\n",
       "      <td>1.634092</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>30.328125</td>\n",
       "      <td>26.671875</td>\n",
       "      <td>145.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.421078</td>\n",
       "      <td>29</td>\n",
       "      <td>49</td>\n",
       "      <td>725000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104580</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>181113990</td>\n",
       "      <td>165000</td>\n",
       "      <td>1221</td>\n",
       "      <td>297</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>37.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1.010272</td>\n",
       "      <td>1.750783</td>\n",
       "      <td>1.318408</td>\n",
       "      <td>1.610002</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>32.343750</td>\n",
       "      <td>27.671875</td>\n",
       "      <td>28.328125</td>\n",
       "      <td>139.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>31.551554</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>82500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104581</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>183682463</td>\n",
       "      <td>110000</td>\n",
       "      <td>954</td>\n",
       "      <td>623</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>35.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.017280</td>\n",
       "      <td>1.804904</td>\n",
       "      <td>1.333028</td>\n",
       "      <td>1.611847</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.671875</td>\n",
       "      <td>142.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>-0.017807</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>23.381355</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>55000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104582</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>188382189</td>\n",
       "      <td>1700000</td>\n",
       "      <td>1220</td>\n",
       "      <td>215</td>\n",
       "      <td>5333</td>\n",
       "      <td>58</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>1.052318</td>\n",
       "      <td>1.884120</td>\n",
       "      <td>1.289166</td>\n",
       "      <td>1.559460</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>1.876826</td>\n",
       "      <td>2.297801</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>31.328125</td>\n",
       "      <td>148.3750</td>\n",
       "      <td>28.9375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>32.883377</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>850000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104583</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>189472256</td>\n",
       "      <td>1200000</td>\n",
       "      <td>1086</td>\n",
       "      <td>238</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>30.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.977570</td>\n",
       "      <td>1.716258</td>\n",
       "      <td>1.286623</td>\n",
       "      <td>1.588325</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.328125</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>27.328125</td>\n",
       "      <td>136.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>24.659122</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>600000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104584</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>193492866</td>\n",
       "      <td>130000</td>\n",
       "      <td>1310</td>\n",
       "      <td>653</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>30.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1.061662</td>\n",
       "      <td>1.781047</td>\n",
       "      <td>1.322857</td>\n",
       "      <td>1.539870</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>33.656250</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>30.671875</td>\n",
       "      <td>143.3750</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>28.492908</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>65000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104585</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>193504415</td>\n",
       "      <td>1300000</td>\n",
       "      <td>119</td>\n",
       "      <td>653</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>31.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>1.066333</td>\n",
       "      <td>1.844698</td>\n",
       "      <td>1.367991</td>\n",
       "      <td>1.633041</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>34.343750</td>\n",
       "      <td>35.656250</td>\n",
       "      <td>25.328125</td>\n",
       "      <td>146.6250</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.248323</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>650000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104586</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>196207027</td>\n",
       "      <td>1550000</td>\n",
       "      <td>1275</td>\n",
       "      <td>1180</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>1.047646</td>\n",
       "      <td>1.882248</td>\n",
       "      <td>1.313958</td>\n",
       "      <td>1.578598</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>29.671875</td>\n",
       "      <td>28.671875</td>\n",
       "      <td>148.0000</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>29.053940</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>775000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104587</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>272053123</td>\n",
       "      <td>330000</td>\n",
       "      <td>1292</td>\n",
       "      <td>1179</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>58.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>2.055582</td>\n",
       "      <td>3.386867</td>\n",
       "      <td>2.568797</td>\n",
       "      <td>2.948582</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>60.343750</td>\n",
       "      <td>66.312500</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>274.2500</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>96.577728</td>\n",
       "      <td>66</td>\n",
       "      <td>77</td>\n",
       "      <td>165000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104588</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>272150626</td>\n",
       "      <td>2800000</td>\n",
       "      <td>1307</td>\n",
       "      <td>1179</td>\n",
       "      <td>1685</td>\n",
       "      <td>59</td>\n",
       "      <td>56.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>2.043903</td>\n",
       "      <td>3.633755</td>\n",
       "      <td>2.579604</td>\n",
       "      <td>3.090340</td>\n",
       "      <td>1.546094</td>\n",
       "      <td>2.729624</td>\n",
       "      <td>1.972577</td>\n",
       "      <td>2.380269</td>\n",
       "      <td>2.893949</td>\n",
       "      <td>3.497659</td>\n",
       "      <td>106.1875</td>\n",
       "      <td>35.6875</td>\n",
       "      <td>34.9375</td>\n",
       "      <td>34.25</td>\n",
       "      <td>160.5</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>61.656250</td>\n",
       "      <td>59.656250</td>\n",
       "      <td>286.7500</td>\n",
       "      <td>37.8750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098877</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>99.945000</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>1400000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date_block_num  campaign_id  product_id  profit_month  name   \n",
       "104475              26            1    42899542        430000  1098  \\\n",
       "104476              26            1    61393910        155000   896   \n",
       "104477              26            1    68202834       3700000   121   \n",
       "104478              26            1    73552136        155000     9   \n",
       "104479              26            1    74421745        155000  1032   \n",
       "104480              26            1    75329276        110000  1040   \n",
       "104481              26            1    75329760        150000   961   \n",
       "104482              26            1    76091452        170000  1272   \n",
       "104483              26            1    76695833        115000  1083   \n",
       "104484              26            1    76705126        145000  1293   \n",
       "104485              26            1    80978495        105000  1297   \n",
       "104486              26            1    81153213        100000   943   \n",
       "104487              26            1    81153341        175000  1217   \n",
       "104488              26            1    82819910        150000  1041   \n",
       "104489              26            1    83160842        140000  1081   \n",
       "104490              26            1    85269397        120000    16   \n",
       "104491              26            1    87981660        390000   120   \n",
       "104492              26            1    94794850        120000  1305   \n",
       "104493              26            1    96160421        125000  1301   \n",
       "104494              26            1    97057130        160000  1092   \n",
       "104495              26            1    98642412       1000000  1273   \n",
       "104496              26            1    98986504       1650000  1274   \n",
       "104497              26            1    99156609        145000  1090   \n",
       "104498              26            1    99183754        140000  1038   \n",
       "104499              26            1    99325284        165000  1097   \n",
       "104500              26            1    99508026        150000   964   \n",
       "104501              26            1   103076103        425000   923   \n",
       "104502              26            1   105275706        140000  1089   \n",
       "104503              26            1   105344835        185000  1035   \n",
       "104504              26            1   107338883        175000  1270   \n",
       "104505              26            1   107411611        105000  1271   \n",
       "104506              26            1   108772138        150000  1085   \n",
       "104507              26            1   110309352        240000   955   \n",
       "104508              26            1   110730140        105000  1295   \n",
       "104509              26            1   110938120       1500000  1033   \n",
       "104510              26            1   114307325       1250000  1299   \n",
       "104511              26            1   114671372       1100000  1298   \n",
       "104512              26            1   117031543       1450000  1276   \n",
       "104513              26            1   117059903       1300000  1277   \n",
       "104514              26            1   117582667        130000   968   \n",
       "104515              26            1   119056035        135000  1222   \n",
       "104516              26            1   126596446        140000  1279   \n",
       "104517              26            1   129297086        115000  1105   \n",
       "104518              26            1   129297638        175000  1304   \n",
       "104519              26            1   129298313        170000  1104   \n",
       "104520              26            1   129300500        125000  1207   \n",
       "104521              26            1   129300910        150000  1102   \n",
       "104522              26            1   129301513       1500000  1206   \n",
       "104523              26            1   133689372        115000  1282   \n",
       "104524              26            1   133689728       1200000  1212   \n",
       "104525              26            1   133689754       1650000  1285   \n",
       "104526              26            1   133690045       1450000  1106   \n",
       "104527              26            1   133690424       1400000  1302   \n",
       "104528              26            1   133690735       1300000  1289   \n",
       "104529              26            1   133690793       1800000  1284   \n",
       "104530              26            1   133690844       1650000  1283   \n",
       "104531              26            1   133691096       1000000  1312   \n",
       "104532              26            1   133813512        180000  1335   \n",
       "104533              26            1   133813532        160000  1339   \n",
       "104534              26            1   133813786        150000  1340   \n",
       "104535              26            1   133813937        115000  1319   \n",
       "104536              26            1   133814062        135000  1281   \n",
       "104537              26            1   133814339        130000  1287   \n",
       "104538              26            1   133814376        150000  1288   \n",
       "104539              26            1   133814455        130000  1280   \n",
       "104540              26            1   133814623        155000  1103   \n",
       "104541              26            1   133814658        145000  1303   \n",
       "104542              26            1   133814884        145000  1219   \n",
       "104543              26            1   133815093        100000   956   \n",
       "104544              26            1   136157116        140000  1218   \n",
       "104545              26            1   136792823       1350000  1214   \n",
       "104546              26            1   137611546        105000  1091   \n",
       "104547              26            1   138381315        160000  1286   \n",
       "104548              26            1   138748071        150000  1216   \n",
       "104549              26            1   138748934        135000  1300   \n",
       "104550              26            1   138750674        425000  1300   \n",
       "104551              26            1   142409966        155000  1093   \n",
       "104552              26            1   146289287        120000  1294   \n",
       "104553              26            1   150568002        115000    53   \n",
       "104554              26            1   150685095        135000  1082   \n",
       "104555              26            1   150695791        145000  1094   \n",
       "104556              26            1   150731971        150000  1095   \n",
       "104557              26            1   150840081        145000  1096   \n",
       "104558              26            1   155563815         95000  1282   \n",
       "104559              26            1   155564180       1350000  1211   \n",
       "104560              26            1   158720374        150000  1311   \n",
       "104561              26            1   159382275        135000  1223   \n",
       "104562              26            1   160102489       1450000  1306   \n",
       "104563              26            1   160393397        135000  1308   \n",
       "104564              26            1   161455980        140000  1088   \n",
       "104565              26            1   163552063        185000  1087   \n",
       "104566              26            1   163553232        135000  1034   \n",
       "104567              26            1   164251213        145000   639   \n",
       "104568              26            1   164338704        160000  1084   \n",
       "104569              26            1   164344210        105000  1037   \n",
       "104570              26            1   164756938        145000  1291   \n",
       "104571              26            1   170324448        165000  1036   \n",
       "104572              26            1   172006037        165000  1213   \n",
       "104573              26            1   172120849        950000  1039   \n",
       "104574              26            1   172305521        165000  1309   \n",
       "104575              26            1   174037915        105000  1278   \n",
       "104576              26            1   176250783        395000  1290   \n",
       "104577              26            1   176251118       4150000   118   \n",
       "104578              26            1   178058234        155000  1296   \n",
       "104579              26            1   178785868       1450000  1215   \n",
       "104580              26            1   181113990        165000  1221   \n",
       "104581              26            1   183682463        110000   954   \n",
       "104582              26            1   188382189       1700000  1220   \n",
       "104583              26            1   189472256       1200000  1086   \n",
       "104584              26            1   193492866        130000  1310   \n",
       "104585              26            1   193504415       1300000   119   \n",
       "104586              26            1   196207027       1550000  1275   \n",
       "104587              26            1   272053123        330000  1292   \n",
       "104588              26            1   272150626       2800000  1307   \n",
       "\n",
       "        short_description  categories_id  categories_name   \n",
       "104475               1005          10378               55  \\\n",
       "104476                925           1685               59   \n",
       "104477               1052          10378               55   \n",
       "104478               1061           1685               59   \n",
       "104479               1098           1685               59   \n",
       "104480               1104           5333               58   \n",
       "104481                284           5333               58   \n",
       "104482                701           1685               59   \n",
       "104483                321           5333               58   \n",
       "104484                 35           5333               58   \n",
       "104485               1060           5333               58   \n",
       "104486                135          67250               60   \n",
       "104487                134          67250               60   \n",
       "104488               1178           5333               58   \n",
       "104489                704           5333               58   \n",
       "104490                 33          67250               60   \n",
       "104491                356          10378               55   \n",
       "104492               1189           1685               59   \n",
       "104493               1187           1685               59   \n",
       "104494               1121           5333               58   \n",
       "104495               1182           1685               59   \n",
       "104496               1181           1685               59   \n",
       "104497               1116           1685               59   \n",
       "104498               1115           5333               58   \n",
       "104499                908           1685               59   \n",
       "104500                965          10378               55   \n",
       "104501               1081          10378               55   \n",
       "104502               1122           5333               58   \n",
       "104503               1100           5333               58   \n",
       "104504               1192           1685               59   \n",
       "104505               1192           1685               59   \n",
       "104506                346           1685               59   \n",
       "104507                224           1685               59   \n",
       "104508                438           5333               58   \n",
       "104509                349           1685               59   \n",
       "104510               1186           1685               59   \n",
       "104511               1185           1685               59   \n",
       "104512               1183           1685               59   \n",
       "104513               1184           1685               59   \n",
       "104514                 29          67298               56   \n",
       "104515               1154           5333               58   \n",
       "104516               1193           1685               59   \n",
       "104517                152          67250               60   \n",
       "104518                102          67250               60   \n",
       "104519                184          67250               60   \n",
       "104520                153          67250               60   \n",
       "104521                848          67250               60   \n",
       "104522                154          67250               60   \n",
       "104523                114          67250               60   \n",
       "104524                156          67250               60   \n",
       "104525                114          67250               60   \n",
       "104526                143          67250               60   \n",
       "104527                114          67250               60   \n",
       "104528                162          67250               60   \n",
       "104529                164          67250               60   \n",
       "104530                160          67250               60   \n",
       "104531                114          67250               60   \n",
       "104532                700          67250               60   \n",
       "104533                 99          67250               60   \n",
       "104534                169          67250               60   \n",
       "104535               1126          67250               60   \n",
       "104536                163          67250               60   \n",
       "104537                 99          67250               60   \n",
       "104538                161          67250               60   \n",
       "104539                 21          67250               60   \n",
       "104540                180          67250               60   \n",
       "104541                 20          67250               60   \n",
       "104542                165          67250               60   \n",
       "104543                180          67250               60   \n",
       "104544               1225           5333               58   \n",
       "104545               1152           1685               59   \n",
       "104546               1124           1685               59   \n",
       "104547               1069           8336               57   \n",
       "104548                 60           8336               57   \n",
       "104549                 60           8336               57   \n",
       "104550                 60           8336               57   \n",
       "104551               1105           5333               58   \n",
       "104552               1155           5333               58   \n",
       "104553                336           1685               59   \n",
       "104554               1117           1685               59   \n",
       "104555                907           1685               59   \n",
       "104556               1123           1685               59   \n",
       "104557                908           1685               59   \n",
       "104558                114          67250               60   \n",
       "104559                155          67250               60   \n",
       "104560               1194           1685               59   \n",
       "104561               1156           5333               58   \n",
       "104562               1188           1685               59   \n",
       "104563               1190           5333               58   \n",
       "104564               1120           5333               58   \n",
       "104565               1119           5333               58   \n",
       "104566               1099           5333               58   \n",
       "104567               1221           1685               59   \n",
       "104568               1118           1685               59   \n",
       "104569               1102           5333               58   \n",
       "104570                994           5333               58   \n",
       "104571               1101           5333               58   \n",
       "104572                196           5333               58   \n",
       "104573               1103           5333               58   \n",
       "104574               1191           5333               58   \n",
       "104575                298           1685               59   \n",
       "104576               1049           1685               59   \n",
       "104577               1027           1685               59   \n",
       "104578                297           5333               58   \n",
       "104579               1153           5333               58   \n",
       "104580                297           5333               58   \n",
       "104581                623           1685               59   \n",
       "104582                215           5333               58   \n",
       "104583                238           1685               59   \n",
       "104584                653           1685               59   \n",
       "104585                653           1685               59   \n",
       "104586               1180           1685               59   \n",
       "104587               1179           1685               59   \n",
       "104588               1179           1685               59   \n",
       "\n",
       "        item_cnt_month_lag_1  item_cnt_month_lag_2  item_cnt_month_lag_3   \n",
       "104475                 105.0                  89.0                 101.0  \\\n",
       "104476                  35.0                  32.0                  26.0   \n",
       "104477                  96.0                  87.0                  92.0   \n",
       "104478                  25.0                  29.0                  28.0   \n",
       "104479                  38.0                  35.0                  33.0   \n",
       "104480                  26.0                  28.0                  33.0   \n",
       "104481                  33.0                  35.0                  29.0   \n",
       "104482                  41.0                  28.0                  35.0   \n",
       "104483                  21.0                  36.0                  24.0   \n",
       "104484                  31.0                  30.0                  31.0   \n",
       "104485                  30.0                  34.0                  25.0   \n",
       "104486                  32.0                  29.0                  21.0   \n",
       "104487                  33.0                  29.0                  29.0   \n",
       "104488                  29.0                  24.0                  28.0   \n",
       "104489                  24.0                  30.0                  30.0   \n",
       "104490                  31.0                  24.0                  23.0   \n",
       "104491                 101.0                 100.0                  91.0   \n",
       "104492                  31.0                  23.0                  26.0   \n",
       "104493                  31.0                  24.0                  39.0   \n",
       "104494                  31.0                  37.0                  35.0   \n",
       "104495                  19.0                  29.0                  31.0   \n",
       "104496                  33.0                  32.0                  31.0   \n",
       "104497                  36.0                  21.0                  23.0   \n",
       "104498                  30.0                  31.0                  25.0   \n",
       "104499                  36.0                  28.0                  32.0   \n",
       "104500                  25.0                  37.0                  32.0   \n",
       "104501                  96.0                  98.0                  96.0   \n",
       "104502                  35.0                  26.0                  25.0   \n",
       "104503                  29.0                  33.0                  29.0   \n",
       "104504                  38.0                  31.0                  30.0   \n",
       "104505                  32.0                  30.0                  27.0   \n",
       "104506                  34.0                  30.0                  32.0   \n",
       "104507                  60.0                  50.0                  60.0   \n",
       "104508                  26.0                  31.0                  29.0   \n",
       "104509                  26.0                  29.0                  31.0   \n",
       "104510                  39.0                  30.0                  32.0   \n",
       "104511                  36.0                  30.0                  26.0   \n",
       "104512                  32.0                  23.0                  32.0   \n",
       "104513                  37.0                  35.0                  28.0   \n",
       "104514                  22.0                  27.0                  26.0   \n",
       "104515                  25.0                  27.0                  31.0   \n",
       "104516                  27.0                  24.0                  32.0   \n",
       "104517                  25.0                  32.0                  32.0   \n",
       "104518                  36.0                  35.0                  33.0   \n",
       "104519                  36.0                  29.0                  27.0   \n",
       "104520                  23.0                  33.0                  25.0   \n",
       "104521                  36.0                  36.0                  29.0   \n",
       "104522                  25.0                  29.0                  28.0   \n",
       "104523                  34.0                  24.0                  37.0   \n",
       "104524                  34.0                  34.0                  31.0   \n",
       "104525                  28.0                  30.0                  28.0   \n",
       "104526                  34.0                  34.0                  27.0   \n",
       "104527                  31.0                  28.0                  25.0   \n",
       "104528                  30.0                  40.0                  23.0   \n",
       "104529                  30.0                  36.0                  33.0   \n",
       "104530                  33.0                  24.0                  28.0   \n",
       "104531                  29.0                  27.0                  28.0   \n",
       "104532                  27.0                  30.0                  23.0   \n",
       "104533                  32.0                  32.0                  32.0   \n",
       "104534                  36.0                  27.0                  31.0   \n",
       "104535                  36.0                  28.0                  26.0   \n",
       "104536                  32.0                  37.0                  28.0   \n",
       "104537                  26.0                  29.0                  36.0   \n",
       "104538                  28.0                  26.0                  37.0   \n",
       "104539                  28.0                  34.0                  33.0   \n",
       "104540                  30.0                  24.0                  26.0   \n",
       "104541                  33.0                  31.0                  29.0   \n",
       "104542                  30.0                  30.0                  30.0   \n",
       "104543                  30.0                  39.0                  32.0   \n",
       "104544                  25.0                  29.0                  30.0   \n",
       "104545                  36.0                  19.0                  31.0   \n",
       "104546                  37.0                  31.0                  33.0   \n",
       "104547                  30.0                  38.0                  28.0   \n",
       "104548                  34.0                  32.0                  34.0   \n",
       "104549                  35.0                  23.0                  30.0   \n",
       "104550                  83.0                  99.0                 104.0   \n",
       "104551                  31.0                  32.0                  28.0   \n",
       "104552                  33.0                  37.0                  26.0   \n",
       "104553                  34.0                  35.0                  28.0   \n",
       "104554                  35.0                  39.0                  27.0   \n",
       "104555                  33.0                  32.0                  29.0   \n",
       "104556                  45.0                  27.0                  24.0   \n",
       "104557                  36.0                  30.0                  34.0   \n",
       "104558                  29.0                  18.0                  30.0   \n",
       "104559                  33.0                  31.0                  30.0   \n",
       "104560                  36.0                  33.0                  35.0   \n",
       "104561                  23.0                  27.0                  31.0   \n",
       "104562                  40.0                  24.0                  33.0   \n",
       "104563                  28.0                  27.0                  32.0   \n",
       "104564                  28.0                  28.0                  30.0   \n",
       "104565                  30.0                  37.0                  31.0   \n",
       "104566                  32.0                  39.0                  23.0   \n",
       "104567                  28.0                  25.0                  26.0   \n",
       "104568                  26.0                  29.0                  30.0   \n",
       "104569                  26.0                  29.0                  35.0   \n",
       "104570                  20.0                  32.0                  37.0   \n",
       "104571                  31.0                  34.0                  34.0   \n",
       "104572                  26.0                  39.0                  28.0   \n",
       "104573                  29.0                  28.0                  28.0   \n",
       "104574                  34.0                  38.0                  18.0   \n",
       "104575                  30.0                  32.0                  27.0   \n",
       "104576                  91.0                  88.0                  92.0   \n",
       "104577                  87.0                  82.0                  97.0   \n",
       "104578                  31.0                  32.0                  30.0   \n",
       "104579                  32.0                  35.0                  28.0   \n",
       "104580                  37.0                  33.0                  23.0   \n",
       "104581                  35.0                  23.0                  34.0   \n",
       "104582                  31.0                  32.0                  32.0   \n",
       "104583                  30.0                  33.0                  28.0   \n",
       "104584                  30.0                  34.0                  29.0   \n",
       "104585                  31.0                  34.0                  26.0   \n",
       "104586                  33.0                  30.0                  23.0   \n",
       "104587                  58.0                  60.0                  65.0   \n",
       "104588                  56.0                  62.0                  58.0   \n",
       "\n",
       "        item_cnt_month_lag_6  product_avg_sale_last_6   \n",
       "104475                 420.0                 3.096221  \\\n",
       "104476                 131.0                 1.039471   \n",
       "104477                 447.0                 3.176809   \n",
       "104478                 130.0                 1.002096   \n",
       "104479                 119.0                 0.992753   \n",
       "104480                 144.0                 1.035967   \n",
       "104481                 159.0                 1.024287   \n",
       "104482                 141.0                 1.074509   \n",
       "104483                 144.0                 1.028959   \n",
       "104484                 133.0                 1.037135   \n",
       "104485                 152.0                 1.062829   \n",
       "104486                 148.0                 1.000928   \n",
       "104487                 144.0                 1.041806   \n",
       "104488                 164.0                 1.019616   \n",
       "104489                 154.0                 1.032463   \n",
       "104490                 128.0                 0.974066   \n",
       "104491                 418.0                 3.163962   \n",
       "104492                 135.0                 1.028959   \n",
       "104493                 125.0                 0.990417   \n",
       "104494                 150.0                 1.109547   \n",
       "104495                 146.0                 1.114219   \n",
       "104496                 119.0                 1.007936   \n",
       "104497                 120.0                 0.947203   \n",
       "104498                 118.0                 0.960050   \n",
       "104499                 159.0                 1.092028   \n",
       "104500                 153.0                 1.068669   \n",
       "104501                 438.0                 3.104396   \n",
       "104502                 167.0                 1.075677   \n",
       "104503                 137.0                 0.986913   \n",
       "104504                 135.0                 1.083852   \n",
       "104505                 131.0                 1.024287   \n",
       "104506                 152.0                 1.049982   \n",
       "104507                 260.0                 2.004193   \n",
       "104508                 130.0                 0.957714   \n",
       "104509                 146.0                 1.014944   \n",
       "104510                 138.0                 1.071005   \n",
       "104511                 154.0                 1.039471   \n",
       "104512                 114.0                 0.954211   \n",
       "104513                 152.0                 1.002096   \n",
       "104514                 143.0                 0.985745   \n",
       "104515                 147.0                 1.035967   \n",
       "104516                 140.0                 1.031295   \n",
       "104517                 138.0                 1.012608   \n",
       "104518                 144.0                 1.014944   \n",
       "104519                 150.0                 1.094364   \n",
       "104520                 151.0                 1.006768   \n",
       "104521                 136.0                 0.967058   \n",
       "104522                 122.0                 0.953043   \n",
       "104523                 113.0                 0.993921   \n",
       "104524                 188.0                 1.045310   \n",
       "104525                 139.0                 0.997425   \n",
       "104526                 160.0                 1.048814   \n",
       "104527                 138.0                 1.048814   \n",
       "104528                 139.0                 1.025455   \n",
       "104529                 132.0                 1.005600   \n",
       "104530                 125.0                 1.067501   \n",
       "104531                 135.0                 0.970562   \n",
       "104532                 101.0                 0.930852   \n",
       "104533                 123.0                 1.000928   \n",
       "104534                 135.0                 1.041806   \n",
       "104535                 127.0                 1.026623   \n",
       "104536                 135.0                 1.062829   \n",
       "104537                 127.0                 1.052318   \n",
       "104538                 119.0                 0.968226   \n",
       "104539                 135.0                 1.023119   \n",
       "104540                 125.0                 1.003264   \n",
       "104541                 166.0                 1.016112   \n",
       "104542                 155.0                 1.104875   \n",
       "104543                 153.0                 1.010272   \n",
       "104544                 138.0                 1.026623   \n",
       "104545                 174.0                 1.013776   \n",
       "104546                 123.0                 1.006768   \n",
       "104547                 127.0                 1.012608   \n",
       "104548                 135.0                 1.023119   \n",
       "104549                 147.0                 1.012608   \n",
       "104550                 394.0                 3.015633   \n",
       "104551                 137.0                 0.985745   \n",
       "104552                 117.0                 1.004432   \n",
       "104553                 118.0                 1.013776   \n",
       "104554                 132.0                 1.040639   \n",
       "104555                 144.0                 1.072173   \n",
       "104556                 125.0                 1.002096   \n",
       "104557                 128.0                 1.044142   \n",
       "104558                 145.0                 1.009104   \n",
       "104559                 111.0                 0.992753   \n",
       "104560                 143.0                 1.059326   \n",
       "104561                 158.0                 1.046478   \n",
       "104562                 161.0                 1.035967   \n",
       "104563                 134.0                 1.021951   \n",
       "104564                 149.0                 1.062829   \n",
       "104565                 155.0                 1.051150   \n",
       "104566                 127.0                 1.013776   \n",
       "104567                 144.0                 1.013776   \n",
       "104568                 145.0                 0.986913   \n",
       "104569                 129.0                 1.054654   \n",
       "104570                 147.0                 1.073341   \n",
       "104571                 152.0                 1.042974   \n",
       "104572                 169.0                 1.054654   \n",
       "104573                 128.0                 0.986913   \n",
       "104574                 133.0                 0.972898   \n",
       "104575                  89.0                 0.960050   \n",
       "104576                 413.0                 3.061182   \n",
       "104577                 434.0                 3.074030   \n",
       "104578                 141.0                 1.027791   \n",
       "104579                 155.0                 1.032463   \n",
       "104580                 124.0                 1.010272   \n",
       "104581                 160.0                 1.017280   \n",
       "104582                 162.0                 1.052318   \n",
       "104583                 134.0                 0.977570   \n",
       "104584                 139.0                 1.061662   \n",
       "104585                 174.0                 1.066333   \n",
       "104586                 138.0                 1.047646   \n",
       "104587                 294.0                 2.055582   \n",
       "104588                 276.0                 2.043903   \n",
       "\n",
       "        product_std_sale_last_6  product_avg_sale_last_12   \n",
       "104475                 5.194885                  3.904368  \\\n",
       "104476                 1.738017                  1.347013   \n",
       "104477                 5.506434                  3.999085   \n",
       "104478                 1.732555                  1.289802   \n",
       "104479                 1.589500                  1.312686   \n",
       "104480                 1.803509                  1.301244   \n",
       "104481                 1.802207                  1.334300   \n",
       "104482                 1.661806                  1.342563   \n",
       "104483                 1.882043                  1.333028   \n",
       "104484                 1.723461                  1.308872   \n",
       "104485                 2.001522                  1.342563   \n",
       "104486                 1.807898                  1.321586   \n",
       "104487                 1.626912                  1.252932   \n",
       "104488                 1.827048                  1.312686   \n",
       "104489                 1.762772                  1.334935   \n",
       "104490                 1.585801                  1.293616   \n",
       "104491                 5.402305                  4.033412   \n",
       "104492                 1.811469                  1.311415   \n",
       "104493                 1.517133                  1.321586   \n",
       "104494                 1.942607                  1.323493   \n",
       "104495                 2.145656                  1.385154   \n",
       "104496                 1.575913                  1.301244   \n",
       "104497                 1.626761                  1.287895   \n",
       "104498                 1.550052                  1.267553   \n",
       "104499                 1.975795                  1.396597   \n",
       "104500                 1.888778                  1.341292   \n",
       "104501                 4.958089                  4.038497   \n",
       "104502                 1.940261                  1.312686   \n",
       "104503                 1.624543                  1.284081   \n",
       "104504                 1.793150                  1.350192   \n",
       "104505                 1.541502                  1.296794   \n",
       "104506                 1.714050                  1.305694   \n",
       "104507                 3.169660                  2.507771   \n",
       "104508                 1.570774                  1.294252   \n",
       "104509                 1.758884                  1.353370   \n",
       "104510                 1.789271                  1.308237   \n",
       "104511                 1.812675                  1.369898   \n",
       "104512                 1.539979                  1.251025   \n",
       "104513                 1.853592                  1.282174   \n",
       "104514                 1.748863                  1.267553   \n",
       "104515                 2.067354                  1.309508   \n",
       "104516                 1.891291                  1.301244   \n",
       "104517                 1.525362                  1.305694   \n",
       "104518                 1.689668                  1.331757   \n",
       "104519                 1.833713                  1.294252   \n",
       "104520                 1.844850                  1.296159   \n",
       "104521                 1.471089                  1.253568   \n",
       "104522                 1.473815                  1.279631   \n",
       "104523                 1.504362                  1.292344   \n",
       "104524                 1.925492                  1.310779   \n",
       "104525                 1.602966                  1.292344   \n",
       "104526                 1.978758                  1.312686   \n",
       "104527                 1.826674                  1.327307   \n",
       "104528                 1.737664                  1.296794   \n",
       "104529                 1.501088                  1.308872   \n",
       "104530                 1.691001                  1.336842   \n",
       "104531                 1.606973                  1.265646   \n",
       "104532                 1.395739                  1.247847   \n",
       "104533                 1.708684                  1.313322   \n",
       "104534                 1.843393                  1.287895   \n",
       "104535                 1.793910                  1.324764   \n",
       "104536                 1.697926                  1.358456   \n",
       "104537                 1.763519                  1.278359   \n",
       "104538                 1.400094                  1.261196   \n",
       "104539                 1.694765                  1.322222   \n",
       "104540                 1.640440                  1.322222   \n",
       "104541                 1.739998                  1.317772   \n",
       "104542                 2.034539                  1.336842   \n",
       "104543                 1.590457                  1.248482   \n",
       "104544                 1.850227                  1.312051   \n",
       "104545                 1.806604                  1.287259   \n",
       "104546                 1.492679                  1.314593   \n",
       "104547                 1.625115                  1.338749   \n",
       "104548                 1.611227                  1.289166   \n",
       "104549                 1.806831                  1.322222   \n",
       "104550                 4.963756                  3.863048   \n",
       "104551                 1.766703                  1.284081   \n",
       "104552                 1.596865                  1.318408   \n",
       "104553                 1.754727                  1.267553   \n",
       "104554                 1.629963                  1.348920   \n",
       "104555                 1.714419                  1.310144   \n",
       "104556                 1.734164                  1.307601   \n",
       "104557                 1.714042                  1.288530   \n",
       "104558                 1.715688                  1.271367   \n",
       "104559                 1.634026                  1.218605   \n",
       "104560                 1.781842                  1.337478   \n",
       "104561                 1.889784                  1.357184   \n",
       "104562                 1.902122                  1.381976   \n",
       "104563                 1.780028                  1.270096   \n",
       "104564                 1.912860                  1.328578   \n",
       "104565                 1.853666                  1.369262   \n",
       "104566                 1.683705                  1.338114   \n",
       "104567                 1.629398                  1.329850   \n",
       "104568                 1.779799                  1.268189   \n",
       "104569                 1.723042                  1.323493   \n",
       "104570                 1.847019                  1.357820   \n",
       "104571                 1.920965                  1.326671   \n",
       "104572                 1.986342                  1.322222   \n",
       "104573                 1.631396                  1.305694   \n",
       "104574                 1.582418                  1.298066   \n",
       "104575                 1.463347                  1.259289   \n",
       "104576                 5.201676                  3.898647   \n",
       "104577                 5.086373                  3.993364   \n",
       "104578                 1.645641                  1.333028   \n",
       "104579                 1.848515                  1.338749   \n",
       "104580                 1.750783                  1.318408   \n",
       "104581                 1.804904                  1.333028   \n",
       "104582                 1.884120                  1.289166   \n",
       "104583                 1.716258                  1.286623   \n",
       "104584                 1.781047                  1.322857   \n",
       "104585                 1.844698                  1.367991   \n",
       "104586                 1.882248                  1.313958   \n",
       "104587                 3.386867                  2.568797   \n",
       "104588                 3.633755                  2.579604   \n",
       "\n",
       "        product_std_sale_last_12  campaign_avg_sale_last_6   \n",
       "104475                  4.602506                  1.546094  \\\n",
       "104476                  1.638383                  1.546094   \n",
       "104477                  4.806958                  1.546094   \n",
       "104478                  1.566913                  1.546094   \n",
       "104479                  1.534918                  1.546094   \n",
       "104480                  1.530410                  1.546094   \n",
       "104481                  1.644917                  1.546094   \n",
       "104482                  1.576973                  1.546094   \n",
       "104483                  1.603296                  1.546094   \n",
       "104484                  1.523244                  1.546094   \n",
       "104485                  1.694920                  1.546094   \n",
       "104486                  1.657881                  1.546094   \n",
       "104487                  1.365948                  1.546094   \n",
       "104488                  1.584556                  1.546094   \n",
       "104489                  1.623072                  1.546094   \n",
       "104490                  1.559280                  1.546094   \n",
       "104491                  4.824557                  1.546094   \n",
       "104492                  1.582842                  1.546094   \n",
       "104493                  1.566468                  1.546094   \n",
       "104494                  1.564501                  1.546094   \n",
       "104495                  1.729813                  1.546094   \n",
       "104496                  1.563163                  1.546094   \n",
       "104497                  1.572182                  1.546094   \n",
       "104498                  1.464437                  1.546094   \n",
       "104499                  1.723424                  1.546094   \n",
       "104500                  1.571325                  1.546094   \n",
       "104501                  4.816713                  1.546094   \n",
       "104502                  1.577636                  1.546094   \n",
       "104503                  1.553364                  1.546094   \n",
       "104504                  1.544639                  1.546094   \n",
       "104505                  1.502049                  1.546094   \n",
       "104506                  1.496636                  1.546094   \n",
       "104507                  2.887246                  1.546094   \n",
       "104508                  1.574229                  1.546094   \n",
       "104509                  1.655382                  1.546094   \n",
       "104510                  1.527901                  1.546094   \n",
       "104511                  1.717188                  1.546094   \n",
       "104512                  1.445371                  1.546094   \n",
       "104513                  1.520716                  1.546094   \n",
       "104514                  1.544023                  1.546094   \n",
       "104515                  1.670601                  1.546094   \n",
       "104516                  1.546220                  1.546094   \n",
       "104517                  1.482522                  1.546094   \n",
       "104518                  1.581023                  1.546094   \n",
       "104519                  1.474047                  1.546094   \n",
       "104520                  1.542122                  1.546094   \n",
       "104521                  1.484252                  1.546094   \n",
       "104522                  1.463212                  1.546094   \n",
       "104523                  1.493500                  1.546094   \n",
       "104524                  1.592077                  1.546094   \n",
       "104525                  1.507545                  1.546094   \n",
       "104526                  1.604411                  1.546094   \n",
       "104527                  1.556001                  1.546094   \n",
       "104528                  1.482805                  1.546094   \n",
       "104529                  1.531916                  1.546094   \n",
       "104530                  1.568258                  1.546094   \n",
       "104531                  1.443778                  1.546094   \n",
       "104532                  1.500270                  1.546094   \n",
       "104533                  1.574109                  1.546094   \n",
       "104534                  1.503569                  1.546094   \n",
       "104535                  1.596733                  1.546094   \n",
       "104536                  1.608205                  1.546094   \n",
       "104537                  1.428049                  1.546094   \n",
       "104538                  1.417158                  1.546094   \n",
       "104539                  1.532755                  1.546094   \n",
       "104540                  1.572380                  1.546094   \n",
       "104541                  1.595479                  1.546094   \n",
       "104542                  1.644698                  1.546094   \n",
       "104543                  1.394793                  1.546094   \n",
       "104544                  1.599560                  1.546094   \n",
       "104545                  1.541928                  1.546094   \n",
       "104546                  1.577966                  1.546094   \n",
       "104547                  1.629896                  1.546094   \n",
       "104548                  1.476349                  1.546094   \n",
       "104549                  1.625160                  1.546094   \n",
       "104550                  4.499415                  1.546094   \n",
       "104551                  1.561693                  1.546094   \n",
       "104552                  1.583972                  1.546094   \n",
       "104553                  1.531626                  1.546094   \n",
       "104554                  1.588065                  1.546094   \n",
       "104555                  1.481718                  1.546094   \n",
       "104556                  1.602712                  1.546094   \n",
       "104557                  1.477340                  1.546094   \n",
       "104558                  1.504135                  1.546094   \n",
       "104559                  1.336132                  1.546094   \n",
       "104560                  1.593906                  1.546094   \n",
       "104561                  1.669331                  1.546094   \n",
       "104562                  1.752238                  1.546094   \n",
       "104563                  1.442512                  1.546094   \n",
       "104564                  1.615362                  1.546094   \n",
       "104565                  1.632617                  1.546094   \n",
       "104566                  1.612597                  1.546094   \n",
       "104567                  1.565094                  1.546094   \n",
       "104568                  1.501363                  1.546094   \n",
       "104569                  1.555722                  1.546094   \n",
       "104570                  1.631526                  1.546094   \n",
       "104571                  1.656614                  1.546094   \n",
       "104572                  1.550306                  1.546094   \n",
       "104573                  1.571618                  1.546094   \n",
       "104574                  1.584133                  1.546094   \n",
       "104575                  1.449732                  1.546094   \n",
       "104576                  4.652738                  1.546094   \n",
       "104577                  4.758298                  1.546094   \n",
       "104578                  1.583472                  1.546094   \n",
       "104579                  1.634092                  1.546094   \n",
       "104580                  1.610002                  1.546094   \n",
       "104581                  1.611847                  1.546094   \n",
       "104582                  1.559460                  1.546094   \n",
       "104583                  1.588325                  1.546094   \n",
       "104584                  1.539870                  1.546094   \n",
       "104585                  1.633041                  1.546094   \n",
       "104586                  1.578598                  1.546094   \n",
       "104587                  2.948582                  1.546094   \n",
       "104588                  3.090340                  1.546094   \n",
       "\n",
       "        campaign_std_sale_last_6  campaign_avg_sale_last_12   \n",
       "104475                  2.729624                   1.972577  \\\n",
       "104476                  2.729624                   1.972577   \n",
       "104477                  2.729624                   1.972577   \n",
       "104478                  2.729624                   1.972577   \n",
       "104479                  2.729624                   1.972577   \n",
       "104480                  2.729624                   1.972577   \n",
       "104481                  2.729624                   1.972577   \n",
       "104482                  2.729624                   1.972577   \n",
       "104483                  2.729624                   1.972577   \n",
       "104484                  2.729624                   1.972577   \n",
       "104485                  2.729624                   1.972577   \n",
       "104486                  2.729624                   1.972577   \n",
       "104487                  2.729624                   1.972577   \n",
       "104488                  2.729624                   1.972577   \n",
       "104489                  2.729624                   1.972577   \n",
       "104490                  2.729624                   1.972577   \n",
       "104491                  2.729624                   1.972577   \n",
       "104492                  2.729624                   1.972577   \n",
       "104493                  2.729624                   1.972577   \n",
       "104494                  2.729624                   1.972577   \n",
       "104495                  2.729624                   1.972577   \n",
       "104496                  2.729624                   1.972577   \n",
       "104497                  2.729624                   1.972577   \n",
       "104498                  2.729624                   1.972577   \n",
       "104499                  2.729624                   1.972577   \n",
       "104500                  2.729624                   1.972577   \n",
       "104501                  2.729624                   1.972577   \n",
       "104502                  2.729624                   1.972577   \n",
       "104503                  2.729624                   1.972577   \n",
       "104504                  2.729624                   1.972577   \n",
       "104505                  2.729624                   1.972577   \n",
       "104506                  2.729624                   1.972577   \n",
       "104507                  2.729624                   1.972577   \n",
       "104508                  2.729624                   1.972577   \n",
       "104509                  2.729624                   1.972577   \n",
       "104510                  2.729624                   1.972577   \n",
       "104511                  2.729624                   1.972577   \n",
       "104512                  2.729624                   1.972577   \n",
       "104513                  2.729624                   1.972577   \n",
       "104514                  2.729624                   1.972577   \n",
       "104515                  2.729624                   1.972577   \n",
       "104516                  2.729624                   1.972577   \n",
       "104517                  2.729624                   1.972577   \n",
       "104518                  2.729624                   1.972577   \n",
       "104519                  2.729624                   1.972577   \n",
       "104520                  2.729624                   1.972577   \n",
       "104521                  2.729624                   1.972577   \n",
       "104522                  2.729624                   1.972577   \n",
       "104523                  2.729624                   1.972577   \n",
       "104524                  2.729624                   1.972577   \n",
       "104525                  2.729624                   1.972577   \n",
       "104526                  2.729624                   1.972577   \n",
       "104527                  2.729624                   1.972577   \n",
       "104528                  2.729624                   1.972577   \n",
       "104529                  2.729624                   1.972577   \n",
       "104530                  2.729624                   1.972577   \n",
       "104531                  2.729624                   1.972577   \n",
       "104532                  2.729624                   1.972577   \n",
       "104533                  2.729624                   1.972577   \n",
       "104534                  2.729624                   1.972577   \n",
       "104535                  2.729624                   1.972577   \n",
       "104536                  2.729624                   1.972577   \n",
       "104537                  2.729624                   1.972577   \n",
       "104538                  2.729624                   1.972577   \n",
       "104539                  2.729624                   1.972577   \n",
       "104540                  2.729624                   1.972577   \n",
       "104541                  2.729624                   1.972577   \n",
       "104542                  2.729624                   1.972577   \n",
       "104543                  2.729624                   1.972577   \n",
       "104544                  2.729624                   1.972577   \n",
       "104545                  2.729624                   1.972577   \n",
       "104546                  2.729624                   1.972577   \n",
       "104547                  2.729624                   1.972577   \n",
       "104548                  2.729624                   1.972577   \n",
       "104549                  2.729624                   1.972577   \n",
       "104550                  2.729624                   1.972577   \n",
       "104551                  2.729624                   1.972577   \n",
       "104552                  2.729624                   1.972577   \n",
       "104553                  2.729624                   1.972577   \n",
       "104554                  2.729624                   1.972577   \n",
       "104555                  2.729624                   1.972577   \n",
       "104556                  2.729624                   1.972577   \n",
       "104557                  2.729624                   1.972577   \n",
       "104558                  2.729624                   1.972577   \n",
       "104559                  2.729624                   1.972577   \n",
       "104560                  2.729624                   1.972577   \n",
       "104561                  2.729624                   1.972577   \n",
       "104562                  2.729624                   1.972577   \n",
       "104563                  2.729624                   1.972577   \n",
       "104564                  2.729624                   1.972577   \n",
       "104565                  2.729624                   1.972577   \n",
       "104566                  2.729624                   1.972577   \n",
       "104567                  2.729624                   1.972577   \n",
       "104568                  2.729624                   1.972577   \n",
       "104569                  2.729624                   1.972577   \n",
       "104570                  2.729624                   1.972577   \n",
       "104571                  2.729624                   1.972577   \n",
       "104572                  2.729624                   1.972577   \n",
       "104573                  2.729624                   1.972577   \n",
       "104574                  2.729624                   1.972577   \n",
       "104575                  2.729624                   1.972577   \n",
       "104576                  2.729624                   1.972577   \n",
       "104577                  2.729624                   1.972577   \n",
       "104578                  2.729624                   1.972577   \n",
       "104579                  2.729624                   1.972577   \n",
       "104580                  2.729624                   1.972577   \n",
       "104581                  2.729624                   1.972577   \n",
       "104582                  2.729624                   1.972577   \n",
       "104583                  2.729624                   1.972577   \n",
       "104584                  2.729624                   1.972577   \n",
       "104585                  2.729624                   1.972577   \n",
       "104586                  2.729624                   1.972577   \n",
       "104587                  2.729624                   1.972577   \n",
       "104588                  2.729624                   1.972577   \n",
       "\n",
       "        campaign_std_sale_last_12  category_avg_sale_last_12   \n",
       "104475                   2.380269                   0.795596  \\\n",
       "104476                   2.380269                   2.893949   \n",
       "104477                   2.380269                   0.795596   \n",
       "104478                   2.380269                   2.893949   \n",
       "104479                   2.380269                   2.893949   \n",
       "104480                   2.380269                   1.876826   \n",
       "104481                   2.380269                   1.876826   \n",
       "104482                   2.380269                   2.893949   \n",
       "104483                   2.380269                   1.876826   \n",
       "104484                   2.380269                   1.876826   \n",
       "104485                   2.380269                   1.876826   \n",
       "104486                   2.380269                   1.905857   \n",
       "104487                   2.380269                   1.905857   \n",
       "104488                   2.380269                   1.876826   \n",
       "104489                   2.380269                   1.876826   \n",
       "104490                   2.380269                   1.905857   \n",
       "104491                   2.380269                   0.795596   \n",
       "104492                   2.380269                   2.893949   \n",
       "104493                   2.380269                   2.893949   \n",
       "104494                   2.380269                   1.876826   \n",
       "104495                   2.380269                   2.893949   \n",
       "104496                   2.380269                   2.893949   \n",
       "104497                   2.380269                   2.893949   \n",
       "104498                   2.380269                   1.876826   \n",
       "104499                   2.380269                   2.893949   \n",
       "104500                   2.380269                   0.795596   \n",
       "104501                   2.380269                   0.795596   \n",
       "104502                   2.380269                   1.876826   \n",
       "104503                   2.380269                   1.876826   \n",
       "104504                   2.380269                   2.893949   \n",
       "104505                   2.380269                   2.893949   \n",
       "104506                   2.380269                   2.893949   \n",
       "104507                   2.380269                   2.893949   \n",
       "104508                   2.380269                   1.876826   \n",
       "104509                   2.380269                   2.893949   \n",
       "104510                   2.380269                   2.893949   \n",
       "104511                   2.380269                   2.893949   \n",
       "104512                   2.380269                   2.893949   \n",
       "104513                   2.380269                   2.893949   \n",
       "104514                   2.380269                   0.058236   \n",
       "104515                   2.380269                   1.876826   \n",
       "104516                   2.380269                   2.893949   \n",
       "104517                   2.380269                   1.905857   \n",
       "104518                   2.380269                   1.905857   \n",
       "104519                   2.380269                   1.905857   \n",
       "104520                   2.380269                   1.905857   \n",
       "104521                   2.380269                   1.905857   \n",
       "104522                   2.380269                   1.905857   \n",
       "104523                   2.380269                   1.905857   \n",
       "104524                   2.380269                   1.905857   \n",
       "104525                   2.380269                   1.905857   \n",
       "104526                   2.380269                   1.905857   \n",
       "104527                   2.380269                   1.905857   \n",
       "104528                   2.380269                   1.905857   \n",
       "104529                   2.380269                   1.905857   \n",
       "104530                   2.380269                   1.905857   \n",
       "104531                   2.380269                   1.905857   \n",
       "104532                   2.380269                   1.905857   \n",
       "104533                   2.380269                   1.905857   \n",
       "104534                   2.380269                   1.905857   \n",
       "104535                   2.380269                   1.905857   \n",
       "104536                   2.380269                   1.905857   \n",
       "104537                   2.380269                   1.905857   \n",
       "104538                   2.380269                   1.905857   \n",
       "104539                   2.380269                   1.905857   \n",
       "104540                   2.380269                   1.905857   \n",
       "104541                   2.380269                   1.905857   \n",
       "104542                   2.380269                   1.905857   \n",
       "104543                   2.380269                   1.905857   \n",
       "104544                   2.380269                   1.876826   \n",
       "104545                   2.380269                   2.893949   \n",
       "104546                   2.380269                   2.893949   \n",
       "104547                   2.380269                   0.358969   \n",
       "104548                   2.380269                   0.358969   \n",
       "104549                   2.380269                   0.358969   \n",
       "104550                   2.380269                   0.358969   \n",
       "104551                   2.380269                   1.876826   \n",
       "104552                   2.380269                   1.876826   \n",
       "104553                   2.380269                   2.893949   \n",
       "104554                   2.380269                   2.893949   \n",
       "104555                   2.380269                   2.893949   \n",
       "104556                   2.380269                   2.893949   \n",
       "104557                   2.380269                   2.893949   \n",
       "104558                   2.380269                   1.905857   \n",
       "104559                   2.380269                   1.905857   \n",
       "104560                   2.380269                   2.893949   \n",
       "104561                   2.380269                   1.876826   \n",
       "104562                   2.380269                   2.893949   \n",
       "104563                   2.380269                   1.876826   \n",
       "104564                   2.380269                   1.876826   \n",
       "104565                   2.380269                   1.876826   \n",
       "104566                   2.380269                   1.876826   \n",
       "104567                   2.380269                   2.893949   \n",
       "104568                   2.380269                   2.893949   \n",
       "104569                   2.380269                   1.876826   \n",
       "104570                   2.380269                   1.876826   \n",
       "104571                   2.380269                   1.876826   \n",
       "104572                   2.380269                   1.876826   \n",
       "104573                   2.380269                   1.876826   \n",
       "104574                   2.380269                   1.876826   \n",
       "104575                   2.380269                   2.893949   \n",
       "104576                   2.380269                   2.893949   \n",
       "104577                   2.380269                   2.893949   \n",
       "104578                   2.380269                   1.876826   \n",
       "104579                   2.380269                   1.876826   \n",
       "104580                   2.380269                   1.876826   \n",
       "104581                   2.380269                   2.893949   \n",
       "104582                   2.380269                   1.876826   \n",
       "104583                   2.380269                   2.893949   \n",
       "104584                   2.380269                   2.893949   \n",
       "104585                   2.380269                   2.893949   \n",
       "104586                   2.380269                   2.893949   \n",
       "104587                   2.380269                   2.893949   \n",
       "104588                   2.380269                   2.893949   \n",
       "\n",
       "        category_std_sale_last_12  date_avg_item_cnt_lag_1   \n",
       "104475                   0.964616                 106.1875  \\\n",
       "104476                   3.497659                 106.1875   \n",
       "104477                   0.964616                 106.1875   \n",
       "104478                   3.497659                 106.1875   \n",
       "104479                   3.497659                 106.1875   \n",
       "104480                   2.297801                 106.1875   \n",
       "104481                   2.297801                 106.1875   \n",
       "104482                   3.497659                 106.1875   \n",
       "104483                   2.297801                 106.1875   \n",
       "104484                   2.297801                 106.1875   \n",
       "104485                   2.297801                 106.1875   \n",
       "104486                   2.264835                 106.1875   \n",
       "104487                   2.264835                 106.1875   \n",
       "104488                   2.297801                 106.1875   \n",
       "104489                   2.297801                 106.1875   \n",
       "104490                   2.264835                 106.1875   \n",
       "104491                   0.964616                 106.1875   \n",
       "104492                   3.497659                 106.1875   \n",
       "104493                   3.497659                 106.1875   \n",
       "104494                   2.297801                 106.1875   \n",
       "104495                   3.497659                 106.1875   \n",
       "104496                   3.497659                 106.1875   \n",
       "104497                   3.497659                 106.1875   \n",
       "104498                   2.297801                 106.1875   \n",
       "104499                   3.497659                 106.1875   \n",
       "104500                   0.964616                 106.1875   \n",
       "104501                   0.964616                 106.1875   \n",
       "104502                   2.297801                 106.1875   \n",
       "104503                   2.297801                 106.1875   \n",
       "104504                   3.497659                 106.1875   \n",
       "104505                   3.497659                 106.1875   \n",
       "104506                   3.497659                 106.1875   \n",
       "104507                   3.497659                 106.1875   \n",
       "104508                   2.297801                 106.1875   \n",
       "104509                   3.497659                 106.1875   \n",
       "104510                   3.497659                 106.1875   \n",
       "104511                   3.497659                 106.1875   \n",
       "104512                   3.497659                 106.1875   \n",
       "104513                   3.497659                 106.1875   \n",
       "104514                   0.072299                 106.1875   \n",
       "104515                   2.297801                 106.1875   \n",
       "104516                   3.497659                 106.1875   \n",
       "104517                   2.264835                 106.1875   \n",
       "104518                   2.264835                 106.1875   \n",
       "104519                   2.264835                 106.1875   \n",
       "104520                   2.264835                 106.1875   \n",
       "104521                   2.264835                 106.1875   \n",
       "104522                   2.264835                 106.1875   \n",
       "104523                   2.264835                 106.1875   \n",
       "104524                   2.264835                 106.1875   \n",
       "104525                   2.264835                 106.1875   \n",
       "104526                   2.264835                 106.1875   \n",
       "104527                   2.264835                 106.1875   \n",
       "104528                   2.264835                 106.1875   \n",
       "104529                   2.264835                 106.1875   \n",
       "104530                   2.264835                 106.1875   \n",
       "104531                   2.264835                 106.1875   \n",
       "104532                   2.264835                 106.1875   \n",
       "104533                   2.264835                 106.1875   \n",
       "104534                   2.264835                 106.1875   \n",
       "104535                   2.264835                 106.1875   \n",
       "104536                   2.264835                 106.1875   \n",
       "104537                   2.264835                 106.1875   \n",
       "104538                   2.264835                 106.1875   \n",
       "104539                   2.264835                 106.1875   \n",
       "104540                   2.264835                 106.1875   \n",
       "104541                   2.264835                 106.1875   \n",
       "104542                   2.264835                 106.1875   \n",
       "104543                   2.264835                 106.1875   \n",
       "104544                   2.297801                 106.1875   \n",
       "104545                   3.497659                 106.1875   \n",
       "104546                   3.497659                 106.1875   \n",
       "104547                   0.431650                 106.1875   \n",
       "104548                   0.431650                 106.1875   \n",
       "104549                   0.431650                 106.1875   \n",
       "104550                   0.431650                 106.1875   \n",
       "104551                   2.297801                 106.1875   \n",
       "104552                   2.297801                 106.1875   \n",
       "104553                   3.497659                 106.1875   \n",
       "104554                   3.497659                 106.1875   \n",
       "104555                   3.497659                 106.1875   \n",
       "104556                   3.497659                 106.1875   \n",
       "104557                   3.497659                 106.1875   \n",
       "104558                   2.264835                 106.1875   \n",
       "104559                   2.264835                 106.1875   \n",
       "104560                   3.497659                 106.1875   \n",
       "104561                   2.297801                 106.1875   \n",
       "104562                   3.497659                 106.1875   \n",
       "104563                   2.297801                 106.1875   \n",
       "104564                   2.297801                 106.1875   \n",
       "104565                   2.297801                 106.1875   \n",
       "104566                   2.297801                 106.1875   \n",
       "104567                   3.497659                 106.1875   \n",
       "104568                   3.497659                 106.1875   \n",
       "104569                   2.297801                 106.1875   \n",
       "104570                   2.297801                 106.1875   \n",
       "104571                   2.297801                 106.1875   \n",
       "104572                   2.297801                 106.1875   \n",
       "104573                   2.297801                 106.1875   \n",
       "104574                   2.297801                 106.1875   \n",
       "104575                   3.497659                 106.1875   \n",
       "104576                   3.497659                 106.1875   \n",
       "104577                   3.497659                 106.1875   \n",
       "104578                   2.297801                 106.1875   \n",
       "104579                   2.297801                 106.1875   \n",
       "104580                   2.297801                 106.1875   \n",
       "104581                   3.497659                 106.1875   \n",
       "104582                   2.297801                 106.1875   \n",
       "104583                   3.497659                 106.1875   \n",
       "104584                   3.497659                 106.1875   \n",
       "104585                   3.497659                 106.1875   \n",
       "104586                   3.497659                 106.1875   \n",
       "104587                   3.497659                 106.1875   \n",
       "104588                   3.497659                 106.1875   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_1  date_campaign_avg_item_cnt_lag_2   \n",
       "104475                           35.6875                           34.9375  \\\n",
       "104476                           35.6875                           34.9375   \n",
       "104477                           35.6875                           34.9375   \n",
       "104478                           35.6875                           34.9375   \n",
       "104479                           35.6875                           34.9375   \n",
       "104480                           35.6875                           34.9375   \n",
       "104481                           35.6875                           34.9375   \n",
       "104482                           35.6875                           34.9375   \n",
       "104483                           35.6875                           34.9375   \n",
       "104484                           35.6875                           34.9375   \n",
       "104485                           35.6875                           34.9375   \n",
       "104486                           35.6875                           34.9375   \n",
       "104487                           35.6875                           34.9375   \n",
       "104488                           35.6875                           34.9375   \n",
       "104489                           35.6875                           34.9375   \n",
       "104490                           35.6875                           34.9375   \n",
       "104491                           35.6875                           34.9375   \n",
       "104492                           35.6875                           34.9375   \n",
       "104493                           35.6875                           34.9375   \n",
       "104494                           35.6875                           34.9375   \n",
       "104495                           35.6875                           34.9375   \n",
       "104496                           35.6875                           34.9375   \n",
       "104497                           35.6875                           34.9375   \n",
       "104498                           35.6875                           34.9375   \n",
       "104499                           35.6875                           34.9375   \n",
       "104500                           35.6875                           34.9375   \n",
       "104501                           35.6875                           34.9375   \n",
       "104502                           35.6875                           34.9375   \n",
       "104503                           35.6875                           34.9375   \n",
       "104504                           35.6875                           34.9375   \n",
       "104505                           35.6875                           34.9375   \n",
       "104506                           35.6875                           34.9375   \n",
       "104507                           35.6875                           34.9375   \n",
       "104508                           35.6875                           34.9375   \n",
       "104509                           35.6875                           34.9375   \n",
       "104510                           35.6875                           34.9375   \n",
       "104511                           35.6875                           34.9375   \n",
       "104512                           35.6875                           34.9375   \n",
       "104513                           35.6875                           34.9375   \n",
       "104514                           35.6875                           34.9375   \n",
       "104515                           35.6875                           34.9375   \n",
       "104516                           35.6875                           34.9375   \n",
       "104517                           35.6875                           34.9375   \n",
       "104518                           35.6875                           34.9375   \n",
       "104519                           35.6875                           34.9375   \n",
       "104520                           35.6875                           34.9375   \n",
       "104521                           35.6875                           34.9375   \n",
       "104522                           35.6875                           34.9375   \n",
       "104523                           35.6875                           34.9375   \n",
       "104524                           35.6875                           34.9375   \n",
       "104525                           35.6875                           34.9375   \n",
       "104526                           35.6875                           34.9375   \n",
       "104527                           35.6875                           34.9375   \n",
       "104528                           35.6875                           34.9375   \n",
       "104529                           35.6875                           34.9375   \n",
       "104530                           35.6875                           34.9375   \n",
       "104531                           35.6875                           34.9375   \n",
       "104532                           35.6875                           34.9375   \n",
       "104533                           35.6875                           34.9375   \n",
       "104534                           35.6875                           34.9375   \n",
       "104535                           35.6875                           34.9375   \n",
       "104536                           35.6875                           34.9375   \n",
       "104537                           35.6875                           34.9375   \n",
       "104538                           35.6875                           34.9375   \n",
       "104539                           35.6875                           34.9375   \n",
       "104540                           35.6875                           34.9375   \n",
       "104541                           35.6875                           34.9375   \n",
       "104542                           35.6875                           34.9375   \n",
       "104543                           35.6875                           34.9375   \n",
       "104544                           35.6875                           34.9375   \n",
       "104545                           35.6875                           34.9375   \n",
       "104546                           35.6875                           34.9375   \n",
       "104547                           35.6875                           34.9375   \n",
       "104548                           35.6875                           34.9375   \n",
       "104549                           35.6875                           34.9375   \n",
       "104550                           35.6875                           34.9375   \n",
       "104551                           35.6875                           34.9375   \n",
       "104552                           35.6875                           34.9375   \n",
       "104553                           35.6875                           34.9375   \n",
       "104554                           35.6875                           34.9375   \n",
       "104555                           35.6875                           34.9375   \n",
       "104556                           35.6875                           34.9375   \n",
       "104557                           35.6875                           34.9375   \n",
       "104558                           35.6875                           34.9375   \n",
       "104559                           35.6875                           34.9375   \n",
       "104560                           35.6875                           34.9375   \n",
       "104561                           35.6875                           34.9375   \n",
       "104562                           35.6875                           34.9375   \n",
       "104563                           35.6875                           34.9375   \n",
       "104564                           35.6875                           34.9375   \n",
       "104565                           35.6875                           34.9375   \n",
       "104566                           35.6875                           34.9375   \n",
       "104567                           35.6875                           34.9375   \n",
       "104568                           35.6875                           34.9375   \n",
       "104569                           35.6875                           34.9375   \n",
       "104570                           35.6875                           34.9375   \n",
       "104571                           35.6875                           34.9375   \n",
       "104572                           35.6875                           34.9375   \n",
       "104573                           35.6875                           34.9375   \n",
       "104574                           35.6875                           34.9375   \n",
       "104575                           35.6875                           34.9375   \n",
       "104576                           35.6875                           34.9375   \n",
       "104577                           35.6875                           34.9375   \n",
       "104578                           35.6875                           34.9375   \n",
       "104579                           35.6875                           34.9375   \n",
       "104580                           35.6875                           34.9375   \n",
       "104581                           35.6875                           34.9375   \n",
       "104582                           35.6875                           34.9375   \n",
       "104583                           35.6875                           34.9375   \n",
       "104584                           35.6875                           34.9375   \n",
       "104585                           35.6875                           34.9375   \n",
       "104586                           35.6875                           34.9375   \n",
       "104587                           35.6875                           34.9375   \n",
       "104588                           35.6875                           34.9375   \n",
       "\n",
       "        date_campaign_avg_item_cnt_lag_3  date_campaign_avg_item_cnt_lag_6   \n",
       "104475                             34.25                             160.5  \\\n",
       "104476                             34.25                             160.5   \n",
       "104477                             34.25                             160.5   \n",
       "104478                             34.25                             160.5   \n",
       "104479                             34.25                             160.5   \n",
       "104480                             34.25                             160.5   \n",
       "104481                             34.25                             160.5   \n",
       "104482                             34.25                             160.5   \n",
       "104483                             34.25                             160.5   \n",
       "104484                             34.25                             160.5   \n",
       "104485                             34.25                             160.5   \n",
       "104486                             34.25                             160.5   \n",
       "104487                             34.25                             160.5   \n",
       "104488                             34.25                             160.5   \n",
       "104489                             34.25                             160.5   \n",
       "104490                             34.25                             160.5   \n",
       "104491                             34.25                             160.5   \n",
       "104492                             34.25                             160.5   \n",
       "104493                             34.25                             160.5   \n",
       "104494                             34.25                             160.5   \n",
       "104495                             34.25                             160.5   \n",
       "104496                             34.25                             160.5   \n",
       "104497                             34.25                             160.5   \n",
       "104498                             34.25                             160.5   \n",
       "104499                             34.25                             160.5   \n",
       "104500                             34.25                             160.5   \n",
       "104501                             34.25                             160.5   \n",
       "104502                             34.25                             160.5   \n",
       "104503                             34.25                             160.5   \n",
       "104504                             34.25                             160.5   \n",
       "104505                             34.25                             160.5   \n",
       "104506                             34.25                             160.5   \n",
       "104507                             34.25                             160.5   \n",
       "104508                             34.25                             160.5   \n",
       "104509                             34.25                             160.5   \n",
       "104510                             34.25                             160.5   \n",
       "104511                             34.25                             160.5   \n",
       "104512                             34.25                             160.5   \n",
       "104513                             34.25                             160.5   \n",
       "104514                             34.25                             160.5   \n",
       "104515                             34.25                             160.5   \n",
       "104516                             34.25                             160.5   \n",
       "104517                             34.25                             160.5   \n",
       "104518                             34.25                             160.5   \n",
       "104519                             34.25                             160.5   \n",
       "104520                             34.25                             160.5   \n",
       "104521                             34.25                             160.5   \n",
       "104522                             34.25                             160.5   \n",
       "104523                             34.25                             160.5   \n",
       "104524                             34.25                             160.5   \n",
       "104525                             34.25                             160.5   \n",
       "104526                             34.25                             160.5   \n",
       "104527                             34.25                             160.5   \n",
       "104528                             34.25                             160.5   \n",
       "104529                             34.25                             160.5   \n",
       "104530                             34.25                             160.5   \n",
       "104531                             34.25                             160.5   \n",
       "104532                             34.25                             160.5   \n",
       "104533                             34.25                             160.5   \n",
       "104534                             34.25                             160.5   \n",
       "104535                             34.25                             160.5   \n",
       "104536                             34.25                             160.5   \n",
       "104537                             34.25                             160.5   \n",
       "104538                             34.25                             160.5   \n",
       "104539                             34.25                             160.5   \n",
       "104540                             34.25                             160.5   \n",
       "104541                             34.25                             160.5   \n",
       "104542                             34.25                             160.5   \n",
       "104543                             34.25                             160.5   \n",
       "104544                             34.25                             160.5   \n",
       "104545                             34.25                             160.5   \n",
       "104546                             34.25                             160.5   \n",
       "104547                             34.25                             160.5   \n",
       "104548                             34.25                             160.5   \n",
       "104549                             34.25                             160.5   \n",
       "104550                             34.25                             160.5   \n",
       "104551                             34.25                             160.5   \n",
       "104552                             34.25                             160.5   \n",
       "104553                             34.25                             160.5   \n",
       "104554                             34.25                             160.5   \n",
       "104555                             34.25                             160.5   \n",
       "104556                             34.25                             160.5   \n",
       "104557                             34.25                             160.5   \n",
       "104558                             34.25                             160.5   \n",
       "104559                             34.25                             160.5   \n",
       "104560                             34.25                             160.5   \n",
       "104561                             34.25                             160.5   \n",
       "104562                             34.25                             160.5   \n",
       "104563                             34.25                             160.5   \n",
       "104564                             34.25                             160.5   \n",
       "104565                             34.25                             160.5   \n",
       "104566                             34.25                             160.5   \n",
       "104567                             34.25                             160.5   \n",
       "104568                             34.25                             160.5   \n",
       "104569                             34.25                             160.5   \n",
       "104570                             34.25                             160.5   \n",
       "104571                             34.25                             160.5   \n",
       "104572                             34.25                             160.5   \n",
       "104573                             34.25                             160.5   \n",
       "104574                             34.25                             160.5   \n",
       "104575                             34.25                             160.5   \n",
       "104576                             34.25                             160.5   \n",
       "104577                             34.25                             160.5   \n",
       "104578                             34.25                             160.5   \n",
       "104579                             34.25                             160.5   \n",
       "104580                             34.25                             160.5   \n",
       "104581                             34.25                             160.5   \n",
       "104582                             34.25                             160.5   \n",
       "104583                             34.25                             160.5   \n",
       "104584                             34.25                             160.5   \n",
       "104585                             34.25                             160.5   \n",
       "104586                             34.25                             160.5   \n",
       "104587                             34.25                             160.5   \n",
       "104588                             34.25                             160.5   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_1  date_item_avg_item_cnt_lag_2   \n",
       "104475                     99.000000                     87.687500  \\\n",
       "104476                     35.656250                     32.000000   \n",
       "104477                     93.312500                     89.312500   \n",
       "104478                     28.000000                     29.328125   \n",
       "104479                     30.328125                     34.000000   \n",
       "104480                     27.671875                     31.000000   \n",
       "104481                     32.000000                     32.000000   \n",
       "104482                     36.343750                     29.000000   \n",
       "104483                     26.671875                     30.000000   \n",
       "104484                     26.328125                     29.000000   \n",
       "104485                     33.000000                     35.656250   \n",
       "104486                     33.343750                     31.000000   \n",
       "104487                     33.656250                     32.656250   \n",
       "104488                     30.328125                     28.671875   \n",
       "104489                     30.000000                     30.328125   \n",
       "104490                     30.671875                     31.000000   \n",
       "104491                     92.312500                     98.312500   \n",
       "104492                     30.671875                     27.671875   \n",
       "104493                     29.000000                     27.671875   \n",
       "104494                     30.671875                     33.656250   \n",
       "104495                     26.000000                     27.000000   \n",
       "104496                     28.328125                     33.656250   \n",
       "104497                     34.343750                     25.328125   \n",
       "104498                     28.671875                     33.343750   \n",
       "104499                     32.656250                     33.000000   \n",
       "104500                     29.671875                     32.343750   \n",
       "104501                     96.312500                     96.312500   \n",
       "104502                     32.656250                     28.328125   \n",
       "104503                     29.328125                     29.328125   \n",
       "104504                     34.000000                     30.000000   \n",
       "104505                     34.656250                     33.343750   \n",
       "104506                     33.000000                     31.671875   \n",
       "104507                     62.000000                     60.000000   \n",
       "104508                     31.671875                     28.328125   \n",
       "104509                     30.328125                     27.671875   \n",
       "104510                     29.328125                     29.328125   \n",
       "104511                     35.656250                     29.328125   \n",
       "104512                     31.328125                     28.328125   \n",
       "104513                     31.000000                     28.671875   \n",
       "104514                     29.328125                     28.328125   \n",
       "104515                     33.000000                     25.671875   \n",
       "104516                     29.000000                     28.328125   \n",
       "104517                     33.000000                     34.656250   \n",
       "104518                     33.000000                     32.343750   \n",
       "104519                     33.000000                     29.671875   \n",
       "104520                     29.000000                     27.671875   \n",
       "104521                     31.671875                     32.343750   \n",
       "104522                     28.000000                     34.656250   \n",
       "104523                     31.000000                     28.000000   \n",
       "104524                     35.000000                     30.000000   \n",
       "104525                     33.000000                     30.328125   \n",
       "104526                     30.671875                     33.343750   \n",
       "104527                     32.656250                     32.000000   \n",
       "104528                     30.000000                     34.656250   \n",
       "104529                     35.000000                     34.000000   \n",
       "104530                     33.343750                     33.343750   \n",
       "104531                     28.000000                     34.000000   \n",
       "104532                     31.671875                     27.671875   \n",
       "104533                     30.328125                     33.000000   \n",
       "104534                     33.343750                     28.328125   \n",
       "104535                     32.656250                     30.000000   \n",
       "104536                     31.000000                     35.343750   \n",
       "104537                     31.328125                     27.000000   \n",
       "104538                     28.000000                     32.000000   \n",
       "104539                     29.328125                     34.343750   \n",
       "104540                     31.000000                     29.000000   \n",
       "104541                     28.328125                     31.000000   \n",
       "104542                     33.656250                     31.000000   \n",
       "104543                     29.671875                     32.000000   \n",
       "104544                     29.328125                     29.000000   \n",
       "104545                     30.328125                     28.671875   \n",
       "104546                     33.000000                     32.656250   \n",
       "104547                     32.656250                     33.656250   \n",
       "104548                     33.343750                     33.000000   \n",
       "104549                     32.343750                     26.328125   \n",
       "104550                     90.312500                     89.687500   \n",
       "104551                     27.671875                     31.000000   \n",
       "104552                     29.671875                     32.656250   \n",
       "104553                     29.671875                     30.000000   \n",
       "104554                     37.656250                     31.328125   \n",
       "104555                     32.343750                     32.000000   \n",
       "104556                     32.343750                     28.671875   \n",
       "104557                     32.656250                     31.328125   \n",
       "104558                     34.656250                     27.000000   \n",
       "104559                     34.343750                     31.671875   \n",
       "104560                     31.328125                     32.000000   \n",
       "104561                     28.000000                     30.328125   \n",
       "104562                     34.343750                     30.000000   \n",
       "104563                     25.671875                     30.671875   \n",
       "104564                     31.000000                     28.671875   \n",
       "104565                     30.000000                     31.671875   \n",
       "104566                     28.328125                     38.343750   \n",
       "104567                     36.656250                     30.000000   \n",
       "104568                     28.000000                     29.671875   \n",
       "104569                     33.000000                     29.671875   \n",
       "104570                     29.000000                     32.656250   \n",
       "104571                     33.656250                     31.328125   \n",
       "104572                     29.671875                     32.343750   \n",
       "104573                     31.671875                     27.671875   \n",
       "104574                     29.328125                     34.656250   \n",
       "104575                     31.328125                     31.000000   \n",
       "104576                     91.000000                     90.000000   \n",
       "104577                     87.687500                     85.687500   \n",
       "104578                     32.656250                     32.000000   \n",
       "104579                     29.671875                     30.328125   \n",
       "104580                     32.343750                     27.671875   \n",
       "104581                     29.671875                     27.000000   \n",
       "104582                     29.328125                     31.328125   \n",
       "104583                     29.328125                     29.000000   \n",
       "104584                     33.656250                     34.343750   \n",
       "104585                     34.343750                     35.656250   \n",
       "104586                     29.000000                     29.671875   \n",
       "104587                     60.343750                     66.312500   \n",
       "104588                     59.000000                     61.656250   \n",
       "\n",
       "        date_item_avg_item_cnt_lag_3  date_item_avg_item_cnt_lag_6   \n",
       "104475                    102.312500                      418.0000  \\\n",
       "104476                     29.671875                      140.0000   \n",
       "104477                     97.687500                      438.2500   \n",
       "104478                     27.671875                      138.0000   \n",
       "104479                     33.000000                      130.0000   \n",
       "104480                     31.671875                      143.3750   \n",
       "104481                     27.000000                      142.6250   \n",
       "104482                     35.000000                      137.6250   \n",
       "104483                     26.671875                      147.0000   \n",
       "104484                     32.000000                      139.0000   \n",
       "104485                     28.671875                      154.6250   \n",
       "104486                     24.671875                      141.6250   \n",
       "104487                     31.671875                      134.3750   \n",
       "104488                     28.671875                      143.6250   \n",
       "104489                     32.656250                      141.0000   \n",
       "104490                     27.000000                      129.0000   \n",
       "104491                     94.687500                      432.2500   \n",
       "104492                     27.000000                      143.3750   \n",
       "104493                     33.656250                      126.0000   \n",
       "104494                     33.656250                      154.0000   \n",
       "104495                     30.671875                      164.6250   \n",
       "104496                     30.671875                      130.0000   \n",
       "104497                     28.000000                      129.6250   \n",
       "104498                     25.000000                      126.3125   \n",
       "104499                     30.328125                      155.0000   \n",
       "104500                     31.000000                      149.3750   \n",
       "104501                     97.000000                      406.2500   \n",
       "104502                     29.328125                      152.3750   \n",
       "104503                     30.328125                      131.6250   \n",
       "104504                     30.328125                      145.0000   \n",
       "104505                     28.671875                      129.0000   \n",
       "104506                     32.343750                      139.3750   \n",
       "104507                     64.687500                      260.7500   \n",
       "104508                     25.328125                      127.3125   \n",
       "104509                     29.000000                      140.0000   \n",
       "104510                     32.000000                      144.0000   \n",
       "104511                     23.671875                      143.6250   \n",
       "104512                     30.671875                      125.6875   \n",
       "104513                     27.000000                      144.3750   \n",
       "104514                     26.328125                      138.0000   \n",
       "104515                     27.671875                      157.0000   \n",
       "104516                     31.000000                      147.6250   \n",
       "104517                     30.671875                      127.6875   \n",
       "104518                     27.671875                      136.3750   \n",
       "104519                     34.000000                      147.6250   \n",
       "104520                     24.328125                      144.0000   \n",
       "104521                     32.343750                      122.6875   \n",
       "104522                     26.000000                      122.0000   \n",
       "104523                     33.343750                      125.6875   \n",
       "104524                     29.671875                      150.0000   \n",
       "104525                     28.671875                      131.0000   \n",
       "104526                     27.328125                      153.0000   \n",
       "104527                     25.000000                      145.0000   \n",
       "104528                     28.328125                      139.3750   \n",
       "104529                     32.343750                      126.0000   \n",
       "104530                     32.000000                      139.0000   \n",
       "104531                     28.671875                      129.6250   \n",
       "104532                     28.671875                      117.0000   \n",
       "104533                     30.328125                      136.6250   \n",
       "104534                     29.671875                      145.6250   \n",
       "104535                     26.328125                      142.3750   \n",
       "104536                     31.671875                      139.0000   \n",
       "104537                     37.343750                      141.6250   \n",
       "104538                     32.656250                      119.0000   \n",
       "104539                     31.671875                      137.0000   \n",
       "104540                     25.328125                      133.0000   \n",
       "104541                     34.343750                      139.0000   \n",
       "104542                     29.000000                      158.6250   \n",
       "104543                     32.343750                      131.0000   \n",
       "104544                     29.328125                      145.3750   \n",
       "104545                     25.671875                      142.3750   \n",
       "104546                     34.656250                      125.6875   \n",
       "104547                     25.000000                      132.6250   \n",
       "104548                     29.000000                      132.6250   \n",
       "104549                     31.328125                      142.3750   \n",
       "104550                     97.000000                      402.2500   \n",
       "104551                     26.328125                      139.0000   \n",
       "104552                     28.000000                      131.0000   \n",
       "104553                     27.671875                      139.6250   \n",
       "104554                     28.671875                      134.3750   \n",
       "104555                     34.000000                      140.3750   \n",
       "104556                     26.000000                      138.0000   \n",
       "104557                     35.343750                      139.0000   \n",
       "104558                     30.328125                      137.3750   \n",
       "104559                     28.000000                      132.3750   \n",
       "104560                     33.000000                      143.3750   \n",
       "104561                     30.671875                      148.3750   \n",
       "104562                     29.671875                      148.3750   \n",
       "104563                     29.328125                      141.3750   \n",
       "104564                     32.000000                      150.3750   \n",
       "104565                     31.328125                      146.6250   \n",
       "104566                     26.671875                      135.6250   \n",
       "104567                     29.000000                      133.0000   \n",
       "104568                     28.000000                      139.6250   \n",
       "104569                     32.656250                      140.0000   \n",
       "104570                     34.000000                      147.3750   \n",
       "104571                     30.328125                      149.6250   \n",
       "104572                     31.328125                      153.6250   \n",
       "104573                     28.328125                      132.0000   \n",
       "104574                     23.328125                      128.3750   \n",
       "104575                     30.671875                      122.0000   \n",
       "104576                     94.000000                      417.0000   \n",
       "104577                     97.312500                      411.2500   \n",
       "104578                     28.671875                      134.6250   \n",
       "104579                     26.671875                      145.3750   \n",
       "104580                     28.328125                      139.3750   \n",
       "104581                     31.671875                      142.3750   \n",
       "104582                     31.328125                      148.3750   \n",
       "104583                     27.328125                      136.0000   \n",
       "104584                     30.671875                      143.3750   \n",
       "104585                     25.328125                      146.6250   \n",
       "104586                     28.671875                      148.0000   \n",
       "104587                     62.000000                      274.2500   \n",
       "104588                     59.656250                      286.7500   \n",
       "\n",
       "        date_campaign_cat_avg_item_cnt_lag_1  delta_price_lag   \n",
       "104475                               84.6250         0.000000  \\\n",
       "104476                               37.8750         0.000000   \n",
       "104477                               84.6250         0.000000   \n",
       "104478                               37.8750         0.000000   \n",
       "104479                               37.8750         0.099731   \n",
       "104480                               28.9375         0.000000   \n",
       "104481                               28.9375         0.000000   \n",
       "104482                               37.8750         0.002853   \n",
       "104483                               28.9375         0.000000   \n",
       "104484                               28.9375         0.000000   \n",
       "104485                               28.9375         0.000000   \n",
       "104486                               30.9375         0.000000   \n",
       "104487                               30.9375         0.000000   \n",
       "104488                               28.9375         0.000000   \n",
       "104489                               28.9375         0.000000   \n",
       "104490                               30.9375         0.000000   \n",
       "104491                               84.6250         0.000000   \n",
       "104492                               37.8750         0.000000   \n",
       "104493                               37.8750         0.000000   \n",
       "104494                               28.9375        -0.000010   \n",
       "104495                               37.8750         0.000000   \n",
       "104496                               37.8750         0.000000   \n",
       "104497                               37.8750         0.000000   \n",
       "104498                               28.9375         0.000000   \n",
       "104499                               37.8750         0.000000   \n",
       "104500                               84.6250         0.000000   \n",
       "104501                               84.6250         0.000000   \n",
       "104502                               28.9375         0.000000   \n",
       "104503                               28.9375         0.000000   \n",
       "104504                               37.8750         0.020782   \n",
       "104505                               37.8750         0.050842   \n",
       "104506                               37.8750         0.000000   \n",
       "104507                               37.8750         0.006634   \n",
       "104508                               28.9375         0.000000   \n",
       "104509                               37.8750         0.000000   \n",
       "104510                               37.8750         0.000000   \n",
       "104511                               37.8750         0.000000   \n",
       "104512                               37.8750         0.000000   \n",
       "104513                               37.8750         0.000000   \n",
       "104514                               22.0000         0.032013   \n",
       "104515                               28.9375         0.000000   \n",
       "104516                               37.8750         0.000000   \n",
       "104517                               30.9375         0.000000   \n",
       "104518                               30.9375         0.000000   \n",
       "104519                               30.9375         0.000000   \n",
       "104520                               30.9375         0.000000   \n",
       "104521                               30.9375         0.000000   \n",
       "104522                               30.9375         0.000000   \n",
       "104523                               30.9375         0.000000   \n",
       "104524                               30.9375         0.000000   \n",
       "104525                               30.9375         0.000000   \n",
       "104526                               30.9375         0.000000   \n",
       "104527                               30.9375         0.000000   \n",
       "104528                               30.9375         0.000000   \n",
       "104529                               30.9375         0.000000   \n",
       "104530                               30.9375         0.000000   \n",
       "104531                               30.9375         0.000000   \n",
       "104532                               30.9375         0.000000   \n",
       "104533                               30.9375         0.000000   \n",
       "104534                               30.9375         0.000000   \n",
       "104535                               30.9375         0.000000   \n",
       "104536                               30.9375         0.000000   \n",
       "104537                               30.9375         0.000000   \n",
       "104538                               30.9375         0.000000   \n",
       "104539                               30.9375         0.000000   \n",
       "104540                               30.9375         0.000000   \n",
       "104541                               30.9375         0.000000   \n",
       "104542                               30.9375         0.000000   \n",
       "104543                               30.9375         0.000000   \n",
       "104544                               28.9375         0.000000   \n",
       "104545                               37.8750         0.000000   \n",
       "104546                               37.8750         0.000000   \n",
       "104547                               45.5000        -0.022202   \n",
       "104548                               45.5000         0.000000   \n",
       "104549                               45.5000         0.000000   \n",
       "104550                               45.5000         0.000000   \n",
       "104551                               28.9375         0.000000   \n",
       "104552                               28.9375         0.000000   \n",
       "104553                               37.8750         0.000000   \n",
       "104554                               37.8750         0.000000   \n",
       "104555                               37.8750         0.000000   \n",
       "104556                               37.8750         0.000000   \n",
       "104557                               37.8750         0.000000   \n",
       "104558                               30.9375         0.000000   \n",
       "104559                               30.9375         0.000000   \n",
       "104560                               37.8750         0.000000   \n",
       "104561                               28.9375         0.000000   \n",
       "104562                               37.8750         0.000000   \n",
       "104563                               28.9375         0.000000   \n",
       "104564                               28.9375         0.000000   \n",
       "104565                               28.9375         0.000000   \n",
       "104566                               28.9375         0.000000   \n",
       "104567                               37.8750         0.000000   \n",
       "104568                               37.8750         0.000000   \n",
       "104569                               28.9375         0.000000   \n",
       "104570                               28.9375         0.000000   \n",
       "104571                               28.9375         0.000000   \n",
       "104572                               28.9375         0.000000   \n",
       "104573                               28.9375         0.000000   \n",
       "104574                               28.9375         0.000000   \n",
       "104575                               37.8750         0.000000   \n",
       "104576                               37.8750         0.000000   \n",
       "104577                               37.8750         0.000000   \n",
       "104578                               28.9375         0.000000   \n",
       "104579                               28.9375         0.000000   \n",
       "104580                               28.9375         0.000000   \n",
       "104581                               37.8750        -0.017807   \n",
       "104582                               28.9375         0.000000   \n",
       "104583                               37.8750         0.000000   \n",
       "104584                               37.8750         0.000000   \n",
       "104585                               37.8750         0.000000   \n",
       "104586                               37.8750         0.000000   \n",
       "104587                               37.8750         0.000000   \n",
       "104588                               37.8750         0.000000   \n",
       "\n",
       "        delta_revenue_lag_1  month  year  item_last_sale   \n",
       "104475             0.098877      2     2            25.0  \\\n",
       "104476             0.098877      2     2            25.0   \n",
       "104477             0.098877      2     2            25.0   \n",
       "104478             0.098877      2     2            25.0   \n",
       "104479             0.098877      2     2            25.0   \n",
       "104480             0.098877      2     2            25.0   \n",
       "104481             0.098877      2     2            25.0   \n",
       "104482             0.098877      2     2            25.0   \n",
       "104483             0.098877      2     2            25.0   \n",
       "104484             0.098877      2     2            25.0   \n",
       "104485             0.098877      2     2            25.0   \n",
       "104486             0.098877      2     2            25.0   \n",
       "104487             0.098877      2     2            25.0   \n",
       "104488             0.098877      2     2            25.0   \n",
       "104489             0.098877      2     2            25.0   \n",
       "104490             0.098877      2     2            25.0   \n",
       "104491             0.098877      2     2            25.0   \n",
       "104492             0.098877      2     2            25.0   \n",
       "104493             0.098877      2     2            25.0   \n",
       "104494             0.098877      2     2            25.0   \n",
       "104495             0.098877      2     2            25.0   \n",
       "104496             0.098877      2     2            25.0   \n",
       "104497             0.098877      2     2            25.0   \n",
       "104498             0.098877      2     2            25.0   \n",
       "104499             0.098877      2     2            25.0   \n",
       "104500             0.098877      2     2            25.0   \n",
       "104501             0.098877      2     2            25.0   \n",
       "104502             0.098877      2     2            25.0   \n",
       "104503             0.098877      2     2            25.0   \n",
       "104504             0.098877      2     2            25.0   \n",
       "104505             0.098877      2     2            25.0   \n",
       "104506             0.098877      2     2            25.0   \n",
       "104507             0.098877      2     2            25.0   \n",
       "104508             0.098877      2     2            25.0   \n",
       "104509             0.098877      2     2            25.0   \n",
       "104510             0.098877      2     2            25.0   \n",
       "104511             0.098877      2     2            25.0   \n",
       "104512             0.098877      2     2            25.0   \n",
       "104513             0.098877      2     2            25.0   \n",
       "104514             0.098877      2     2            25.0   \n",
       "104515             0.098877      2     2            25.0   \n",
       "104516             0.098877      2     2            25.0   \n",
       "104517             0.098877      2     2            25.0   \n",
       "104518             0.098877      2     2            25.0   \n",
       "104519             0.098877      2     2            25.0   \n",
       "104520             0.098877      2     2            25.0   \n",
       "104521             0.098877      2     2            25.0   \n",
       "104522             0.098877      2     2            25.0   \n",
       "104523             0.098877      2     2            25.0   \n",
       "104524             0.098877      2     2            25.0   \n",
       "104525             0.098877      2     2            25.0   \n",
       "104526             0.098877      2     2            25.0   \n",
       "104527             0.098877      2     2            25.0   \n",
       "104528             0.098877      2     2            25.0   \n",
       "104529             0.098877      2     2            25.0   \n",
       "104530             0.098877      2     2            25.0   \n",
       "104531             0.098877      2     2            25.0   \n",
       "104532             0.098877      2     2            25.0   \n",
       "104533             0.098877      2     2            25.0   \n",
       "104534             0.098877      2     2            25.0   \n",
       "104535             0.098877      2     2            25.0   \n",
       "104536             0.098877      2     2            25.0   \n",
       "104537             0.098877      2     2            25.0   \n",
       "104538             0.098877      2     2            25.0   \n",
       "104539             0.098877      2     2            25.0   \n",
       "104540             0.098877      2     2            25.0   \n",
       "104541             0.098877      2     2            25.0   \n",
       "104542             0.098877      2     2            25.0   \n",
       "104543             0.098877      2     2            25.0   \n",
       "104544             0.098877      2     2            25.0   \n",
       "104545             0.098877      2     2            25.0   \n",
       "104546             0.098877      2     2            25.0   \n",
       "104547             0.098877      2     2            25.0   \n",
       "104548             0.098877      2     2            25.0   \n",
       "104549             0.098877      2     2            25.0   \n",
       "104550             0.098877      2     2            25.0   \n",
       "104551             0.098877      2     2            25.0   \n",
       "104552             0.098877      2     2            25.0   \n",
       "104553             0.098877      2     2            25.0   \n",
       "104554             0.098877      2     2            25.0   \n",
       "104555             0.098877      2     2            25.0   \n",
       "104556             0.098877      2     2            25.0   \n",
       "104557             0.098877      2     2            25.0   \n",
       "104558             0.098877      2     2            25.0   \n",
       "104559             0.098877      2     2            25.0   \n",
       "104560             0.098877      2     2            25.0   \n",
       "104561             0.098877      2     2            25.0   \n",
       "104562             0.098877      2     2            25.0   \n",
       "104563             0.098877      2     2            25.0   \n",
       "104564             0.098877      2     2            25.0   \n",
       "104565             0.098877      2     2            25.0   \n",
       "104566             0.098877      2     2            25.0   \n",
       "104567             0.098877      2     2            25.0   \n",
       "104568             0.098877      2     2            25.0   \n",
       "104569             0.098877      2     2            25.0   \n",
       "104570             0.098877      2     2            25.0   \n",
       "104571             0.098877      2     2            25.0   \n",
       "104572             0.098877      2     2            25.0   \n",
       "104573             0.098877      2     2            25.0   \n",
       "104574             0.098877      2     2            25.0   \n",
       "104575             0.098877      2     2            25.0   \n",
       "104576             0.098877      2     2            25.0   \n",
       "104577             0.098877      2     2            25.0   \n",
       "104578             0.098877      2     2            25.0   \n",
       "104579             0.098877      2     2            25.0   \n",
       "104580             0.098877      2     2            25.0   \n",
       "104581             0.098877      2     2            25.0   \n",
       "104582             0.098877      2     2            25.0   \n",
       "104583             0.098877      2     2            25.0   \n",
       "104584             0.098877      2     2            25.0   \n",
       "104585             0.098877      2     2            25.0   \n",
       "104586             0.098877      2     2            25.0   \n",
       "104587             0.098877      2     2            25.0   \n",
       "104588             0.098877      2     2            25.0   \n",
       "\n",
       "        item_campaign_first_sale  item_first_sale  item_cnt_month_forecast   \n",
       "104475                        25               25               130.760559  \\\n",
       "104476                        25               25                29.155863   \n",
       "104477                        25               25               111.033714   \n",
       "104478                        25               25                29.155863   \n",
       "104479                        25               25                29.155863   \n",
       "104480                        25               25                23.381355   \n",
       "104481                        25               25                28.764393   \n",
       "104482                        25               25                31.772161   \n",
       "104483                        25               25                23.816744   \n",
       "104484                        25               25                28.404255   \n",
       "104485                        25               25                22.628382   \n",
       "104486                        25               25                22.270302   \n",
       "104487                        25               25                32.565147   \n",
       "104488                        25               25                28.764393   \n",
       "104489                        25               25                27.620361   \n",
       "104490                        25               25                23.903709   \n",
       "104491                        25               25               109.038353   \n",
       "104492                        25               25                23.903709   \n",
       "104493                        25               25                28.648529   \n",
       "104494                        25               25                29.685987   \n",
       "104495                        25               25                24.572157   \n",
       "104496                        25               25                32.697990   \n",
       "104497                        25               25                28.404255   \n",
       "104498                        25               25                27.620361   \n",
       "104499                        25               25                31.551554   \n",
       "104500                        25               25                28.764393   \n",
       "104501                        25               25               130.760559   \n",
       "104502                        25               25                27.620361   \n",
       "104503                        25               25                33.618092   \n",
       "104504                        25               25                32.565147   \n",
       "104505                        25               25                22.628382   \n",
       "104506                        25               25                28.764393   \n",
       "104507                        25               25                78.766464   \n",
       "104508                        25               25                22.628382   \n",
       "104509                        25               25                28.764393   \n",
       "104510                        25               25                27.986921   \n",
       "104511                        25               25                24.659122   \n",
       "104512                        25               25                28.421078   \n",
       "104513                        25               25                29.248323   \n",
       "104514                        25               25                28.492908   \n",
       "104515                        25               25                28.492908   \n",
       "104516                        25               25                27.620361   \n",
       "104517                        25               25                23.816744   \n",
       "104518                        25               25                32.565147   \n",
       "104519                        25               25                31.772161   \n",
       "104520                        25               25                28.648529   \n",
       "104521                        25               25                28.764393   \n",
       "104522                        25               25                28.764393   \n",
       "104523                        25               25                23.816744   \n",
       "104524                        25               25                25.362192   \n",
       "104525                        25               25                32.697990   \n",
       "104526                        25               25                28.421078   \n",
       "104527                        25               25                28.375776   \n",
       "104528                        25               25                29.248323   \n",
       "104529                        25               25                34.101162   \n",
       "104530                        25               25                32.697990   \n",
       "104531                        25               25                24.659122   \n",
       "104532                        25               25                32.989952   \n",
       "104533                        25               25                29.685987   \n",
       "104534                        25               25                28.764393   \n",
       "104535                        25               25                23.816744   \n",
       "104536                        25               25                27.967983   \n",
       "104537                        25               25                28.492908   \n",
       "104538                        25               25                28.764393   \n",
       "104539                        25               25                28.492908   \n",
       "104540                        25               25                29.155863   \n",
       "104541                        25               25                28.404255   \n",
       "104542                        25               25                27.620361   \n",
       "104543                        25               25                22.270302   \n",
       "104544                        25               25                27.620361   \n",
       "104545                        25               25                28.723398   \n",
       "104546                        25               25                22.628382   \n",
       "104547                        25               25                29.685987   \n",
       "104548                        25               25                28.764393   \n",
       "104549                        25               25                27.967983   \n",
       "104550                        25               25               109.860443   \n",
       "104551                        25               25                29.155863   \n",
       "104552                        25               25                23.903709   \n",
       "104553                        25               25                23.816744   \n",
       "104554                        25               25                27.967983   \n",
       "104555                        25               25                28.404255   \n",
       "104556                        25               25                28.662470   \n",
       "104557                        25               25                28.404255   \n",
       "104558                        25               25                22.270302   \n",
       "104559                        25               25                28.723398   \n",
       "104560                        25               25                28.764393   \n",
       "104561                        25               25                27.967983   \n",
       "104562                        25               25                28.421078   \n",
       "104563                        25               25                27.967983   \n",
       "104564                        25               25                27.620361   \n",
       "104565                        25               25                33.618092   \n",
       "104566                        25               25                27.967983   \n",
       "104567                        25               25                28.404255   \n",
       "104568                        25               25                29.685987   \n",
       "104569                        25               25                22.628382   \n",
       "104570                        25               25                28.404255   \n",
       "104571                        25               25                31.551554   \n",
       "104572                        25               25                31.551554   \n",
       "104573                        25               25                33.861290   \n",
       "104574                        25               25                31.551554   \n",
       "104575                        25               25                22.628382   \n",
       "104576                        25               25               122.366272   \n",
       "104577                        25               25               180.285431   \n",
       "104578                        25               25                29.155863   \n",
       "104579                        25               25                28.421078   \n",
       "104580                        25               25                31.551554   \n",
       "104581                        25               25                23.381355   \n",
       "104582                        25               25                32.883377   \n",
       "104583                        25               25                24.659122   \n",
       "104584                        25               25                28.492908   \n",
       "104585                        25               25                29.248323   \n",
       "104586                        25               25                29.053940   \n",
       "104587                        25               25                96.577728   \n",
       "104588                        25               25                99.945000   \n",
       "\n",
       "        item_cnt_month  target     salary  \n",
       "104475              86      76   215000.0  \n",
       "104476              31      38    77500.0  \n",
       "104477              74      80  1850000.0  \n",
       "104478              31      25    77500.0  \n",
       "104479              31      22    77500.0  \n",
       "104480              22      24    55000.0  \n",
       "104481              30      21    75000.0  \n",
       "104482              34      38    85000.0  \n",
       "104483              23      28    57500.0  \n",
       "104484              29      48    72500.0  \n",
       "104485              21      34    52500.0  \n",
       "104486              20      38    50000.0  \n",
       "104487              35      35    87500.0  \n",
       "104488              30      35    75000.0  \n",
       "104489              28      39    70000.0  \n",
       "104490              24      10    60000.0  \n",
       "104491              78      76   195000.0  \n",
       "104492              24      27    60000.0  \n",
       "104493              25      12    62500.0  \n",
       "104494              32      28    80000.0  \n",
       "104495              20      23   500000.0  \n",
       "104496              33      25   825000.0  \n",
       "104497              29      43    72500.0  \n",
       "104498              28      39    70000.0  \n",
       "104499              33      23    82500.0  \n",
       "104500              30      38    75000.0  \n",
       "104501              85      96   212500.0  \n",
       "104502              28      30    70000.0  \n",
       "104503              37      28    92500.0  \n",
       "104504              35      46    87500.0  \n",
       "104505              21      36    52500.0  \n",
       "104506              30      50    75000.0  \n",
       "104507              48      51   120000.0  \n",
       "104508              21      37    52500.0  \n",
       "104509              30      43   750000.0  \n",
       "104510              25      42   625000.0  \n",
       "104511              22      18   550000.0  \n",
       "104512              29      31   725000.0  \n",
       "104513              26      38   650000.0  \n",
       "104514              26      17    65000.0  \n",
       "104515              27      37    67500.0  \n",
       "104516              28      16    70000.0  \n",
       "104517              23      29    57500.0  \n",
       "104518              35      32    87500.0  \n",
       "104519              34      46    85000.0  \n",
       "104520              25      26    62500.0  \n",
       "104521              30      31    75000.0  \n",
       "104522              30      27   750000.0  \n",
       "104523              23      23    57500.0  \n",
       "104524              24      40   600000.0  \n",
       "104525              33      46   825000.0  \n",
       "104526              29      40   725000.0  \n",
       "104527              28      40   700000.0  \n",
       "104528              26      28   650000.0  \n",
       "104529              36      41   900000.0  \n",
       "104530              33      46   825000.0  \n",
       "104531              20      17   500000.0  \n",
       "104532              36      43    90000.0  \n",
       "104533              32      27    80000.0  \n",
       "104534              30      42    75000.0  \n",
       "104535              23       9    57500.0  \n",
       "104536              27      16    67500.0  \n",
       "104537              26      19    65000.0  \n",
       "104538              30      21    75000.0  \n",
       "104539              26      31    65000.0  \n",
       "104540              31      41    77500.0  \n",
       "104541              29      29    72500.0  \n",
       "104542              29      21    72500.0  \n",
       "104543              20      12    50000.0  \n",
       "104544              28      17    70000.0  \n",
       "104545              27      37   675000.0  \n",
       "104546              21      19    52500.0  \n",
       "104547              32      45    80000.0  \n",
       "104548              30      39    75000.0  \n",
       "104549              27      28    67500.0  \n",
       "104550              85      88   212500.0  \n",
       "104551              31      23    77500.0  \n",
       "104552              24      33    60000.0  \n",
       "104553              23      10    57500.0  \n",
       "104554              27      33    67500.0  \n",
       "104555              29      30    72500.0  \n",
       "104556              30      19    75000.0  \n",
       "104557              29      18    72500.0  \n",
       "104558              19      31    47500.0  \n",
       "104559              27      44   675000.0  \n",
       "104560              30      17    75000.0  \n",
       "104561              27      31    67500.0  \n",
       "104562              29      40   725000.0  \n",
       "104563              27      17    67500.0  \n",
       "104564              28      19    70000.0  \n",
       "104565              37      40    92500.0  \n",
       "104566              27      42    67500.0  \n",
       "104567              29      48    72500.0  \n",
       "104568              32      42    80000.0  \n",
       "104569              21      22    52500.0  \n",
       "104570              29      14    72500.0  \n",
       "104571              33      29    82500.0  \n",
       "104572              33      51    82500.0  \n",
       "104573              19      11   475000.0  \n",
       "104574              33      23    82500.0  \n",
       "104575              21       7    52500.0  \n",
       "104576              79      64   197500.0  \n",
       "104577              83      77  2075000.0  \n",
       "104578              31      32    77500.0  \n",
       "104579              29      49   725000.0  \n",
       "104580              33      40    82500.0  \n",
       "104581              22      18    55000.0  \n",
       "104582              34      45   850000.0  \n",
       "104583              24      10   600000.0  \n",
       "104584              26      26    65000.0  \n",
       "104585              26      20   650000.0  \n",
       "104586              31      31   775000.0  \n",
       "104587              66      77   165000.0  \n",
       "104588              56      55  1400000.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_campaign_1 = df.loc[df['campaign_id'] == 1]\n",
    "df_campaign_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "91eff726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAIjCAYAAAAJLyrXAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd5gT5doG8HvSN2UbbUHpoGBDQEVEBJGu2FAONsACFsTCsRw8RwG7HrH381nQYz92RaWIDRCxgKJIkyLSy5bspme+P95M2mZ3k91NMpncv+vaazeTSfbNzGRmnnme9x1JlmUZRERERERElDRdthtARERERESUaxhIERERERERpYiBFBERERERUYoYSBEREREREaWIgRQREREREVGKGEgRERERERGliIEUERERERFRihhIERERERERpYiBFBERERERUYoYSBFl2ObNmyFJEl588cVsN6VeSjsfeOCBbDelyV5++WX06NEDRqMRxcXF2W4OEeWYSZMmwW63Z7sZjdapUydMmjQp280g0hwGUkTN6MUXX4QkSfj++++z3ZR6rVy5EhdeeCHat28Ps9mM0tJSDB06FC+88AICgUBW2jRv3jzMmjWr2d/3999/x6RJk9C1a1f85z//wbPPPlvnvLNmzYIkSdi7d2942quvvoqHH3642dvVVMq2lujnH//4R7abl3bNsV5yaX0rNm7ciMsvvxxdunSBxWJBYWEhBgwYgEceeQQulyvbzQMAPPnkkyldKFK228suuyzh8//85z/D80Svq+ZWU1ODWbNm4Ysvvkjb/1CrSZMm1bk/if5RWzCWz+uM1MGQ7QYQ5ZuOHTvC5XLBaDRm5f//3//9H6644gq0adMGF110Ebp3746qqiosWrQIl156KXbs2IFbbrkl4+2aN28ennjiiWYPpr744gsEg0E88sgj6NatW8qvf/XVV7F69Wpcd911zdqu5nL77bejc+fOMdOOOOKILLUmc9K1XtS8vj/++GOce+65MJvNmDBhAo444gh4vV588803uPHGG/Hrr7/We6EgU5588km0bNkypZNui8WCt99+G08++SRMJlPMc6+99hosFgvcbncztzRWTU0NZs+eDQAYPHhwWv+X2lx++eUYOnRo+PGmTZtw2223YcqUKRg4cGB4eteuXbPRvDrl8zojdWAgRZRhkiTBYrFk5X9/++23uOKKK9C/f3/MmzcPDocj/Nx1112H77//HqtXr85om6qrq2Gz2dL2/rt37wYAzZb0jRo1Csccc0yzv2+61wulZtOmTRg/fjw6duyIzz//HG3btg0/N3XqVGzYsAEff/xxFlvYNCNHjsQHH3yATz75BGeccUZ4+tKlS7Fp0yaMHTsWb7/9dhZbqG39+/dH//79w4+///573Hbbbejfvz8uvPDCJr8/9yekVSztI8qwRH2klPr7v/76C2eeeSbsdjtatWqFG264oVapXTAYxMMPP4zDDz8cFosFbdq0weWXX44DBw40+L9nz54NSZLwyiuvxARRimOOOSbhVeRnn30WXbt2hdlsxrHHHosVK1bEPP/zzz9j0qRJ4XKjsrIyXHLJJdi3b1/MfEop1W+//Ybzzz8fJSUlOPHEEzFp0iQ88cQTABBTRtKQJ598EocffjjMZjPatWuHqVOnory8PPx8p06dMHPmTABAq1atIElSShmvwYMH4+OPP8aWLVvCberUqVP4eY/Hg5kzZ6Jbt24wm81o3749brrpJng8npj3kSQJV199Nd566y0cdthhKCgoQP/+/fHLL78AAJ555hl069YNFosFgwcPxubNm5NuY0M+//xzDBw4EDabDcXFxTjjjDOwZs2amHnqWi+K//73v+jbty8KCgpQWlqK8ePH488//6z1v5YvX47Ro0ejpKQENpsNRx11FB555JHw88luJ1VVVbjuuuvQqVMnmM1mtG7dGsOGDcOPP/4IoOH10lhqXt/3338/nE4nnnvuuZggStGtWzdce+214cd+vx933HFH+HvbqVMn3HLLLQnbmug7Ed+nRiklXbJkCaZPn45WrVrBZrPhrLPOwp49e2Je9+uvv+LLL78ML8NkMgUHHXQQTjrpJLz66qsx01955RUceeSRdWZZ33rrrfC22bJlS1x44YX466+/YuZJZv+6efNmtGrVCkBkP5lo2SSzj07Wli1bcNVVV+HQQw9FQUEBWrRogXPPPbfW9pDssgcAWZZx55134uCDD4bVasXJJ5+MX3/9tVHti9fU/Twgjl+zZs1Cu3btwu377bffEvbhKi8vx3XXXRcuQe/WrRvuu+8+BINBAMmvM6J0YkaKSCUCgQBGjBiBfv364YEHHsDChQsxZ84cdO3aFVdeeWV4vssvvxwvvvgiLr74YlxzzTXYtGkTHn/8cfz0009YsmRJnSWDNTU1WLRoEU466SR06NAh6Xa9+uqrqKqqwuWXXw5JknD//ffj7LPPxh9//BH+XwsWLMAff/yBiy++GGVlZeESo19//RXffvttraDo3HPPRffu3XH33XdDlmX07t0b27dvx4IFC/Dyyy8n1a5Zs2Zh9uzZGDp0KK688kqsXbsWTz31FFasWBFeDg8//DBeeuklvPvuu3jqqadgt9tx1FFHJf3Z//nPf6KiogLbtm3DQw89BADhDufBYBCnn346vvnmG0yZMgU9e/bEL7/8goceegjr1q3De++9F/NeX3/9NT744ANMnToVAHDPPffgtNNOw0033YQnn3wSV111FQ4cOID7778fl1xyCT7//POk2lhRUVGr30jLli0BAAsXLsSoUaPQpUsXzJo1Cy6XC4899hgGDBiAH3/8sVbwEb9eAOCuu+7CrbfeinHjxuGyyy7Dnj178Nhjj+Gkk07CTz/9FM70LViwAKeddhratm2La6+9FmVlZVizZg0++uij8Al+stvJFVdcgf/973+4+uqrcdhhh2Hfvn345ptvsGbNGvTp06fe9dIUal7fH374Ibp06YITTjghqc9y2WWXYe7cuTjnnHPw97//HcuXL8c999yDNWvW4N13301xyURMmzYNJSUlmDlzJjZv3oyHH34YV199Nd544w0AwMMPP4xp06bBbrfjn//8JwCgTZs2Sb33+eefj2uvvRZOpxN2ux1+vx9vvfUWpk+fnrCsT9kPHnvssbjnnnuwa9cuPPLII1iyZEnMtgk0vH9t1aoVnnrqKVx55ZU466yzcPbZZwNAzP4i2X10slasWIGlS5di/PjxOPjgg7F582Y89dRTGDx4MH777TdYrdaY+Rta9gBw22234c4778To0aMxevRo/Pjjjxg+fDi8Xm/K7YvX1P08AMyYMQP3338/xowZgxEjRmDVqlUYMWJErfVbU1ODQYMG4a+//sLll1+ODh06YOnSpZgxYwZ27NiBhx9+OKl1RpR2MhE1mxdeeEEGIK9YsaLOeTZt2iQDkF944YXwtIkTJ8oA5Ntvvz1m3t69e8t9+/YNP/76669lAPIrr7wSM9+nn36acHq0VatWyQDka6+9NqnPorSzRYsW8v79+8PT33//fRmA/OGHH4an1dTU1Hr9a6+9JgOQv/rqq/C0mTNnygDk8847r9b8U6dOlZPdJe3evVs2mUzy8OHD5UAgEJ7++OOPywDk559/vtb/3LNnT4Pvm2jeU089Ve7YsWOteV9++WVZp9PJX3/9dcz0p59+WgYgL1myJDwNgGw2m+VNmzaFpz3zzDMyALmsrEyurKwMT58xY4YMIGbeRJRtLdGP4uijj5Zbt24t79u3Lzxt1apVsk6nkydMmFDrc8evl82bN8t6vV6+6667Yqb/8ssvssFgCE/3+/1y586d5Y4dO8oHDhyImTcYDIb/TnY7KSoqkqdOnVrv569rvaQiV9Z3RUWFDEA+44wzkvpcK1eulAHIl112Wcz0G264QQYgf/755zFtnTlzZq336Nixozxx4sTwY2V7Gzp0aMw6vf7662W9Xi+Xl5eHpx1++OHyoEGDkmqr0oapU6fK+/fvl00mk/zyyy/LsizLH3/8sSxJkrx58+Za68rr9cqtW7eWjzjiCNnlcoXf66OPPpIByLfddlt4WrL71z179tS5PJJ9j1Qk+j4sW7ZMBiC/9NJL4WnJLntlv3jqqafGzHfLLbfIAGLWZ0NWrFhR6zjV1P38zp07ZYPBIJ955pkx02fNmlWrfXfccYdss9nkdevWxcz7j3/8Q9br9fLWrVtlWa5/nRFlAkv7iFTkiiuuiHk8cOBA/PHHH+HHb731FoqKijBs2DDs3bs3/NO3b1/Y7XYsXry4zveurKwEgIQlffX529/+hpKSkpg2AYhpV0FBQfhvt9uNvXv34vjjjweAcDlWtPjPmaqFCxfC6/Xiuuuug04X2Y1NnjwZhYWFGekr8tZbb6Fnz57o0aNHzLoYMmQIANRaF6ecckpMBqhfv34AgLFjx8asE2V69PKtzxNPPIEFCxbE/ADAjh07sHLlSkyaNAmlpaXh+Y866igMGzYM8+bNq/Ve8evlnXfeQTAYxLhx42I+Y1lZGbp37x7+jD/99BM2bdqE6667rlZftOir1MluJ8XFxVi+fDm2b9+e1DLIhGyu71S/u8q6nT59esz0v//97wDQpO/HlClTYtbpwIEDEQgEsGXLlka/p6KkpAQjR47Ea6+9BkBkw0844QR07Nix1rzff/89du/ejauuuiqmz+mpp56KHj16JPyMDe1fk9Ec76GI/j74fD7s27cP3bp1Q3FxccL9ZkPLXtkvTps2LWa+5ho4pan7+UWLFsHv9+Oqq66KmT5t2rRar33rrbcwcOBAlJSUxHzfhg4dikAggK+++qo5PhJRk7G0j0glLBZLuN5bUVJSEtP3af369aioqEDr1q0TvocysEIihYWFAET/k1TElwEqQVV0u/bv34/Zs2fj9ddfr9WGioqKWu8ZP8pcqpQTh0MPPTRmuslkQpcuXZrlpK4h69evx5o1a2qtM0X8cohfjkVFRQCA9u3bJ5yeTJ83ADjuuOMSDjZR1zICgJ49e+Kzzz6r1QE8fr2sX78esiyje/fuCf+3Utq5ceNGAA2PFpjsdnL//fdj4sSJaN++Pfr27YvRo0djwoQJ6NKlS73vn07ZXN+pfne3bNkCnU5Xa5TKsrIyFBcXN+n7kcz+oCnOP/98XHTRRdi6dSvee+893H///Qnnq2/77tGjB7755puYacnsXxvSHO8RzeVy4Z577sELL7yAv/76K1z+BiTebza07JVlEv99bdWqVczFsMZq6n5eaV/8dllaWlqrfevXr8fPP/+c9PeNKFsYSBGphF6vb3CeYDCI1q1b45VXXkn4fF0HHUAcvAwGQ7jDe1PbFX3QHzduHJYuXYobb7wRRx99NOx2O4LBIEaOHBnuGBwt+spmrgoGgzjyyCPx4IMPJnw+/oS5ruWYzPLNlPj1EgwGIUkSPvnkk4TtTLVfUrLbybhx4zBw4EC8++67mD9/Pv7973/jvvvuwzvvvINRo0Y17sM1UTbXd2FhIdq1a5fyiJrJDNhSl7oGUEj39nr66afDbDZj4sSJ8Hg8GDduXLO8bzL710y8R7Rp06bhhRdewHXXXYf+/fujqKgIkiRh/PjxCfeb2d5XZHI/HwwGMWzYMNx0000Jnz/kkEMa/d5EzYmBFFEO6dq1KxYuXIgBAwakfJCyWq0YMmQIPv/8c/z555+1Tvwa68CBA1i0aBFmz56N2267LTx9/fr1Kb1PKid9SqnP2rVrY7IUXq8XmzZtirkfSlPV1a6uXbti1apVOOWUU5p0wpou0cso3u+//46WLVs2OBxx165dIcsyOnfuXO+Ji3JvmdWrV9e57FPdTtq2bYurrroKV111FXbv3o0+ffrgrrvuCgdS6Vrmal3fp512Gp599lksW7YsZpjqRDp27IhgMIj169ejZ8+e4em7du1CeXl5TKlcSUlJzEiXgPge7dixo9FtbcryKSgowJlnnon//ve/GDVqVHjglHjR27dSXqlYu3ZtwnLAhmR6vf7vf//DxIkTMWfOnPA0t9tda30kS/nM69evj9kv7tmzp8kZw+bYzyvt27BhQ0y2at++fbXa17VrVzidzgb35Wrc91J+YR8pohwybtw4BAIB3HHHHbWe8/v9DR6AZ86cCVmWcdFFF8HpdNZ6/ocffsDcuXNTapNylTT+qujDDz+c0vsoJ/XJnEQMHToUJpMJjz76aMz/fe6551BRUYFTTz01pf/dULsSla2MGzcOf/31F/7zn//Ues7lcqG6urrZ2tAYbdu2xdFHH425c+fGLNPVq1dj/vz5GD16dIPvcfbZZ0Ov12P27Nm11q8sy+Fhj/v06YPOnTvj4YcfrrX+lNclu50EAoFay7t169Zo165dzNDdda2XplLr+r7ppptgs9lw2WWXYdeuXbWe37hxY3ioeWXdxi9bJZsW/f3o2rVrrf4mzz77bKOH9AbEMmxsMAAAN9xwA2bOnIlbb721znmOOeYYtG7dGk8//XTMdvHJJ59gzZo1jdoHKKPkNaXtqdDr9bW+D4899lijl/3QoUNhNBrx2GOPxbxvqvviRJpjP3/KKafAYDDgqaeeipn++OOP15p33LhxWLZsGT777LNaz5WXl8Pv9wPI/DojiseMFFEaPP/88/j0009rTY++z0tjDBo0CJdffjnuuecerFy5EsOHD4fRaMT69evx1ltv4ZFHHsE555xT5+tPOOEEPPHEE7jqqqvQo0cPXHTRRejevTuqqqrwxRdf4IMPPsCdd96ZUpsKCwtx0kkn4f7774fP58NBBx2E+fPnY9OmTSm9T9++fQEA11xzDUaMGAG9Xo/x48cnnLdVq1aYMWMGZs+ejZEjR+L000/H2rVr8eSTT+LYY49tlhtIRrfrjTfewPTp03HsscfCbrdjzJgxuOiii/Dmm2/iiiuuwOLFizFgwAAEAgH8/vvvePPNN/HZZ5+l5Ua5qfj3v/+NUaNGoX///rj00kvDw58XFRUlda+Vrl274s4778SMGTOwefNmnHnmmXA4HNi0aRPeffddTJkyBTfccAN0Oh2eeuopjBkzBkcffTQuvvhitG3bFr///jt+/fVXfPbZZ0lvJ1VVVTj44INxzjnnoFevXrDb7Vi4cCFWrFgRc+W+rvUCiPtBffnll40qeVLr+u7atSteffVV/O1vf0PPnj0xYcIEHHHEEfB6vVi6dCneeuut8H14evXqhYkTJ+LZZ59FeXk5Bg0ahO+++w5z587FmWeeiZNPPjn8vpdddhmuuOIKjB07FsOGDcOqVavw2Wef1ZkJSkbfvn3x1FNP4c4770S3bt3QunXrWlmj+vTq1Qu9evWqdx6j0Yj77rsPF198MQYNGoTzzjsvPPx5p06dcP3116fc7oKCAhx22GF44403cMghh6C0tBRHHHFEg33/4k2aNAlz587Fpk2b6r2/2WmnnYaXX34ZRUVFOOyww7Bs2TIsXLgQLVq0SLntAML3tVKG2h89ejR++uknfPLJJ01an0Dz7OfbtGmDa6+9FnPmzMHpp5+OkSNHYtWqVeH2RWeXbrzxRnzwwQc47bTTMGnSJPTt2xfV1dX45Zdf8L///Q+bN29Gy5Ytm22dETVahkcJJNK0+oakBiD/+eefdQ5/brPZar2fMoxsvGeffVbu27evXFBQIDscDvnII4+Ub7rpJnn79u1JtfOHH36Qzz//fLldu3ay0WiUS0pK5FNOOUWeO3dueDhxpZ3//ve/a70eccPNbtu2TT7rrLPk4uJiuaioSD733HPl7du315qvvqHI/X6/PG3aNLlVq1ayJElJDYX++OOPyz169JCNRqPcpk0b+corr6w1/HZThz93Op3y+eefLxcXF8sAYobG9nq98n333ScffvjhstlslktKSuS+ffvKs2fPlisqKmKWV/xw3nUt38WLF8sA5LfeeqvetiYz1L4sy/LChQvlAQMGyAUFBXJhYaE8ZswY+bfffmvwc0d7++235RNPPFG22WyyzWaTe/ToIU+dOlVeu3ZtzHzffPONPGzYMNnhcMg2m00+6qij5Mceeyz8fDLbicfjkW+88Ua5V69e4ffp1auX/OSTT8b8r/rWS9++feWysrJ6l0tdn1ut61uxbt06efLkyXKnTp1kk8kkOxwOecCAAfJjjz0mu93u8Hw+n0+ePXu23LlzZ9loNMrt27eXZ8yYETOPLMtyIBCQb775Zrlly5ay1WqVR4wYIW/YsKHO4c/jtzel/YsXLw5P27lzp3zqqafKDodDBtDgUOiJlle8urbRN954Q+7du7dsNpvl0tJS+YILLpC3bdsWM08q+9elS5fKffv2lU0mU8x2mcp7jB07Vi4oKKi1L4p34MAB+eKLL5Zbtmwp2+12ecSIEfLvv//epGUfCATk2bNny23btpULCgrkwYMHy6tXr671ng1JNPx5c+3nb731VrmsrEwuKCiQhwwZIq9Zs0Zu0aKFfMUVV8TMW1VVJc+YMUPu1q2bbDKZ5JYtW8onnHCC/MADD8herzc8X13rjCgTJFnOQo9mIiKiNKiqqkJpaSkefvjh8M1wiTKpTZs2mDBhAv79739nuyk5oby8HCUlJbjzzjvDN3EmyhXsI0VERJrx1Vdf4aCDDsLkyZOz3RTKQ7/++itcLhduvvnmbDdFlVwuV61pSj+rwYMHZ7YxRM2AGSkiIiIiSrsXX3wRL774IkaPHg273Y5vvvkGr732GoYPH55wYAkiteNgE0RERESUdkcddRQMBgPuv/9+VFZWhgegSHWQIyK1YEaKiIiIiIgoRewjRURERERElCIGUkRERERERCliHykAwWAQ27dvh8PhiLkhHBERERER5RdZllFVVYV27dpBp6s778RACsD27dvRvn37bDeDiIiIiIhU4s8//8TBBx9c5/MMpAA4HA4AYmEVFhZmuTXZ4/P5MH/+fAwfPhxGozHbzaEM4DrPL1zf+YfrPP9wnecfrvPmV1lZifbt24djhLowkALC5XyFhYV5H0hZrVYUFhbyi5gnuM7zC9d3/uE6zz9c5/mH6zx9Guryw8EmiIiIiIiIUsRAioiIiIiIKEUMpIiIiIiIiFLEPlJJCgQC8Pl82W5GWvl8PhgMBrjdbgQCgWw3h5pAr9fDYDBwOH8iIiKiNGEglQSn04lt27ZBluVsNyWtZFlGWVkZ/vzzT56Aa4DVakXbtm1hMpmy3RQiIiIizWEg1YBAIIBt27bBarWiVatWmg4wgsEgnE4n7HZ7vTcfI3WTZRlerxd79uzBpk2b0L17d65PIiIiombGQKoBPp8PsiyjVatWKCgoyHZz0ioYDMLr9cJisfDEO8cVFBTAaDRiy5Yt4XVKRERERM2HZ8tJ0nImirSJwTARERFR+vBMi4iIiIiIKEUMpIiIiIiIiFLEQIqIiIiIiChFDKQ0atKkSTjzzDPDjwcPHozrrrsua+0ZPHgwJEmq9eP3+7PWpqaaNWsWjj766Gw3g4iIiIiygIEUZczkyZOxY8eOmB+DoXEDR3q93mZuHRERERFR8hhIpUiWgerq7Pw09n7AkyZNwpdffolHHnkknAnavHkzAGD16tUYNWoU7HY72rZti8svvxx79+4Nv3bw4MGYNm0arrvuOpSUlKBNmzb4z3/+g+rqalx88cVwOBzo1q0bPvnkkwbbYbVaUVZWFvOjePvtt3H44YfDbDajU6dOmDNnTsxrO3XqhDvuuAMTJkxAYWEhpkyZAgD45ptvMHDgQBQUFKB9+/a45pprUF1dHX6dx+PBzTffjPbt28NsNqNbt2547rnnAIh7hF166aXo3LkzCgoKcOihh+KRRx6J+b9ffPEFjjvuONhsNhQXF2PAgAHYsmULXnzxRcyePRurVq0KL9MXX3wxpfVCRERERLmLgVSKamoAuz07PzU1jWvzI488gv79+8dkhNq3b4/y8nIMGTIEvXv3xvfff4958+Zhz549GD9+fMzr586di5YtW+K7777DtGnTcOWVV+Lcc8/FCSecgB9//BHDhw/HRRddhJpGNvCHH37AuHHjMH78ePzyyy+YNWsWbr311lqByQMPPIBevXrhp59+wq233oqNGzdi5MiRGDt2LH7++We88cYb+Oabb3D11VeHXzNhwgS89tprePTRR7FmzRo888wzsNvtAMR9sw4++GC89dZb+O2333DbbbfhlltuwZtvvgkA8Pv9OPPMMzFo0CD8/PPPWLZsGaZMmQJJkvC3v/0Nf//733H44YeHl+nf/va3Rn1+IiIiIspBMskVFRUyALmioqLWcy6XS/7tt99kl8sly7IsO52yLHJDmf9xOpP/TBMnTpTPOOOM8ONBgwbJ1157bcw8d9xxhzx8+PDw40AgIK9evVoGIK9duzb8uhNPPDE8j9/vl202m3zRRReFp+3YsUMGIC9btqzO9gwaNEg2Go2yzWYL/0yfPl2WZVk+//zz5WHDhsXMf+ONN8qHHXZY+HHHjh3lM888M2aeSy+9VJ4yZUrMtK+//lrW6XSyy+WS165dKwOQFyxYUGe74k2dOlUeO3asLMuyvG/fPhmA/MUXXyScd+bMmXKvXr2Sfu9Mi992E/F6vfJ7770ne73eDLaMsoXrO/9wnecfrvP8w3Xe/OqLDaI1roNKHrNaAacze/+7Oa1atQqLFy8OZ2iibdy4EYcccggA4KijjgpP1+v1aNGiBY488sjwtDZt2gAAdu/eXe//u+CCC/DPf/4z/Li4uBgAsGbNGpxxxhkx8w4YMAAPP/wwAoEA9Ho9AOCYY46p1f6ff/4Zr7zySniaLMsIBoPYtGkTfvnlF+j1egwaNKjONj3xxBN4/vnnsXXrVrhcLni93vAAEqWlpZg0aRJGjBiBYcOGYejQoRg3bhzatm1b7+ckIiIiqs8vvwBlZUCrVtluCTUFA6kUSRJgs2W7Fc3D6XRizJgxuO+++wCIUjen0wm73Y6DDjooPJ/RaIx5nSRJMdMkSQq/vj5FRUXo1q1bo9tri1vwTqcTl19+Oa655ppa83bo0AEbNmyo9/1ef/113HDDDZgzZw769+8Ph8OBf//731i+fHl4nhdeeAHXXHMNPv30U7zxxhv417/+hQULFuD4449v9OcgIiKi/PXnn8DRRwPHHgt8+222W0NNwUAqT5hMJgQCgZhpffr0wdtvv41OnTrBYDAgGAyisrIShYWF0Oky132uZ8+eWLJkScy0JUuW4JBDDglnoxLp06cPfvvttzqDsyOPPBLBYBBffvklhg4dWuv5JUuW4IQTTsBVV10VnrZx48Za8/Xu3Ru9e/fGjBkz0L9/f7z66qs4/vjjEy5TIiIiovps3gwEgyKgotzGwSbyRKdOnbB8+XJs3rwZe/fuRTAYxNSpU7F//36cd955WLFiBTZu3IhFixbhkksuyWiA8Pe//x2LFi3CHXfcgXXr1mHu3Ll4/PHHccMNN9T7uptvvhlLly7F1VdfjZUrV2L9+vV4//33w4NNdOrUCRMnTsQll1yC9957D5s2bcIXX3wRHkyie/fu+P777/HZZ59h3bp1uPXWW7FixYrw+2/atAkzZszAsmXLsGXLFsyfPx/r169Hz549w++/adMmrFy5Env37oXH40nTEiIiIiKtULqI+HzZbQc1HQOpPHHDDTdAr9fjsMMOQ6tWrbB161a0a9cOS5YsQSAQwPDhw9GrVy/ccsstKC4uzmhGqk+fPnjzzTfx+uuv44gjjsBtt92G22+/HZMmTar3dUcddRS+/PJLrFu3DgMHDkTv3r1x2223oV27duF5nnrqKZxzzjm46qqr0KNHD0yePDk8PPrll1+Os88+G3/729/Qr18/7Nu3LyY7ZbVa8fvvv2Ps2LE45JBDMGXKFEydOhWXX345AGDs2LEYOXIkTj75ZLRq1QqvvfZa8y8cIiIi0hTlLi1+f3bbQU0nyXJj706kHZWVlSgqKkJFRQUKCwtjnnO73di0aRM6d+4Mi8WSpRZmRrZK+yg9ktl2fT4f5s2bh9GjR9fqC0faw/Wdf7jO8w/XufrNnQtMmgQ4HEBlZdPfj+u8+dUXG0Tj2TIRERERUYawtE87GEgREREREWUIS/u0g4EUEREREVGGRAdS7GCT2xhIERERERFliFLaBwC8i0puYyBFRERERJQhSkYKYHlfrstqIPXVV19hzJgxaNeuHSRJwnvvvRfzvCRJCX/+/e9/h+fp1KlTrefvvffeDH8SIiIiIqKGRWekGEjltqwGUtXV1ejVqxeeeOKJhM/v2LEj5uf555+HJEkYO3ZszHy33357zHzTpk3LRPOJiIiIiFISnZHiyH25zZDNfz5q1CiMGjWqzufLyspiHr///vs4+eST0aVLl5jpDoej1rxERERERGrD0j7tyGoglYpdu3bh448/xty5c2s9d++99+KOO+5Ahw4dcP755+P666+HwVD3R/N4PPB4POHHlaG7ofl8PvjiLg34fD7IsoxgMIhgMNhMn0adlHszK5+XclswGIQsy/D5fNDr9QnnUbb3+O2etInrO/9wnecfrnP1q6rSQykKc7l8Tc5KcZ03v2SXZc4EUnPnzoXD4cDZZ58dM/2aa65Bnz59UFpaiqVLl2LGjBnYsWMHHnzwwTrf65577sHs2bNrTZ8/fz6sVmvMNIPBgLKyMjidTni93ub5MCpXVVWV7SZQnHvvvRcff/wxvv7666Rf4/V64XK58NVXX8HfwCWvBQsWNLWJlEO4vvMP13n+4TpXr507BwMoAgDMn78YrVq5muV9uc6bT01NTVLzSbKsjhHsJUnCu+++izPPPDPh8z169MCwYcPw2GOP1fs+zz//PC6//HI4nU6YzeaE8yTKSLVv3x579+5FYWFhzLxutxt//vknOnXqBIvFktqHyqKLL74Y5eXlePfddwEAQ4YMQa9evfDQQw/V+RpZllFVVQWHwwFJkpq9TRs2bMDdd9+NhQsXYs+ePWjXrh369euH6dOn45hjjmn2/1eXzZs3o2vXrvjhhx9w9NFHNzifTqfD5s2bcdBBB4Wf27FjBzp27IhAIICNGzeiU6dOzdY+vV6Pt99+O+a7MHv2bLz//vv48ccfk34ft9uNzZs3o3379nVuuz6fDwsWLMCwYcNgNBqb2nRSOa7v/MN1nn+4ztWvZ08DNm4U51m//+5DXI+VlHGdN7/Kykq0bNkSFRUVtWKDaDmRkfr666+xdu1avPHGGw3O269fP/j9fmzevBmHHnpownnMZnPCIMtoNNbaAAOBACRJgk6ng06XO6PFKyMYRrc5/nE8pZyvofka4/vvv8cpp5yCI444As888wx69OiBqqoqvP/++7jxxhvx5ZdfNuv/q4/y2Rpap8pzBx10EP773/9ixowZ4edefvllHHTQQdi6dWtato3491QC21T+j06ngyRJCbfreMnMQ9rB9Z1/uM7zD9e5ekWP2gcY0Vyrieu8+SS7HHMiMnjuuefQt29f9OrVq8F5V65cCZ1Oh9atW6elLbIso9pbnZWfxiYPJ02ahC+//BKPPPJIOMDavHkzAGD16tUYNWoU7HY72rZti8svvxx79+4Nv3bw4MGYNm0arrvuOpSUlKBNmzb4z3/+g+rqalx88cVwOBzo1q0bPvnkk3qX2aRJk9C9e3d8/fXXOPXUU9G1a1ccffTRmDlzJt5///3wvL/88guGDBmCgoICtGjRAlOmTIEzao8zePBgXHfddTHvf+aZZ2LSpEnhx506dcLdd9+NSy65BA6HAx06dMCzzz4bfr5z584AgN69e0OSJAwePLje5Tdx4kS88MILMdNeeOEFTJw4sda8X375JY477jiYzWa0bdsW//jHP2LK6gYPHoxrrrkGN910E0pLS1FWVoZZs2bFtB0AzjrrLEiSVCvT9fLLL6NTp04oKirC+PHjWYZJRESUYzjYhHZkNSPldDqxYcOG8ONNmzZh5cqVKC0tRYcOHQCI1Npbb72FOXPm1Hr9smXLsHz5cpx88slwOBxYtmwZrr/+elx44YUoKSlJS5trfDWw32NPy3s3xDnDCZvJlvLrHnnkEaxbtw5HHHEEbr/9dgBAq1atUF5ejiFDhuCyyy7DQw89hOrqatx4440YP348Pv/88/Dr586di5tuugnfffcd3njjDVx55ZV49913cdZZZ+GWW27BQw89hIsuughbt26t1ccMEMHtr7/+ildffTVhRqW4uBiAGA5/xIgR6N+/P1asWIHdu3fjsssuw9VXX40XX3wxpc88Z84c3HHHHbjlllvwv//9D1deeSUGDRqEQw89FN999x2OO+44LFy4EIcffjhMJlO973X66afj6aefxjfffIMTTzwR33zzDQ4cOIAxY8bgjjvuCM/3119/YfTo0Zg0aRJeeukl/P7775g8eTIsFktMsDR37lxMnz4dy5cvx7JlyzBp0iQMGDAAw4YNw4oVK9C6dWu88MILGDlyZMwgERs3bsR7772Hjz76CAcOHMC4ceNw77334q677kpp2RAREVF2yDIDKS3Jakbq+++/R+/evdG7d28AwPTp09G7d2/cdttt4Xlef/11yLKM8847r9brzWYzXn/9dQwaNAiHH3447rrrLlx//fUx2QcCioqKYDKZYLVaUVZWhrKyMuj1ejz++OPo3bs37r77bvTo0QO9e/fGY489hsWLF2PdunXh1/fq1Qv/+te/0L17d8yYMQMWiwUtW7bE5MmT0b17d9x2223Yt28ffv7554T/f/369QBEP7f6vPrqq3C73XjppZdwxBFHYMiQIXj88cfx8ssvY9euXSl95tGjR+Oqq65Ct27dcPPNN6Nly5ZYvHgxABFEAkCLFi1QVlaG0tLSet/LaDTiwgsvxPPPPw9A9MO78MILa6V9n3zySbRv3x6PP/44evTogTPPPBOzZ8/GnDlzYkZBPOqoozBz5kx0794dEyZMwDHHHINFixbFtK24uBhlZWXhx4AovXzxxRdxxBFHYODAgbjooovCryMiIiL1c7lEMKXgQHu5LasZqcGDBzdYrjZlyhRMmTIl4XN9+vTBt99+m46m1clqtMI5w9nwjGn6381p1apVWLx4Mez22hm2jRs34pBDDgEgTvwVer0eLVq0wJFHHhme1qZNGwDA7t27E/6fZEsS16xZg169esFmi2TdBgwYgGAwiLVr14b/TzKi2yxJEsrKyupsXzIuueQSnHDCCbj77rvx1ltvYdmyZbVGwluzZg369+8fM1DHgAED4HQ6sW3btnCWNbptANC2bduk2tapUyc4HI6UX0dERETqEJ2NApiRynU5MdiEmkiS1KjyOjVyOp0YM2YM7rvvPgAi4+F0OmG322NGqIvPvCgDGEQ/Vl6fiBKQ/f777+HsY2PpdLpagVmisf4Ttbkp98Y68sgj0aNHD5x33nno2bMnjjjiCKxcubJR79XYtjX3ZyIiIqLMcsZdi2cgldtyYrAJajqTyYRAIBAzrU+fPvj111/RqVMndOvWDd26dUOXLl3QrVu3mKxQUx199NE47LDDapW4KcrLywEAPXv2xKpVq1AddblmyZIl0Ol04REYW7VqhR07doSfDwQCWL16dUrtUfpExS+PhlxyySX44osvcMkllyR8vmfPnli2bFlMoLdkyRI4HA4cfPDBSf8fo9GYctuIiIhI/eIzUizty20MpPJEp06dsHz5cmzevBl79+5FMBjE1KlTsX//fpx33nlYsWIFNm7ciEWLFuGSSy5p1hN5SZLwwgsvYN26dRg4cCDmzZuHP/74Az///DPuuusunHHGGQCACy64ABaLBRMnTsTq1auxePFiTJs2DRdddFG4rG/IkCH4+OOP8fHHH+P333/HlVdeGQ7EktW6dWsUFBTg008/xa5du1BRUZHU6yZPnow9e/bgsssuS/j8VVddhT///BPTpk3D77//jvfffx8zZ87E9OnTUxq2vFOnTli0aBF27tyJAwcOJP06IiIiUjeW9mkLA6k8ccMNN0Cv1+Owww5Dq1atsHXrVrRr1w5LlixBIBDA8OHD0atXL9xyyy0oLi5u9vsiHXfccfj+++/RrVs3TJ48GT179sTpp5+OX3/9FQ8//DAAwGq14rPPPsP+/ftx7LHH4pxzzsEpp5yCxx9/PPw+l1xyCSZOnIgJEyZg0KBB6NKlC04++eSU2mIwGPDoo4/imWeeQbt27cKBXDKva9myJQyGxBWxBx10EObNm4fvvvsOvXr1whVXXIFLL70U//rXv1Jq35w5c7BgwQK0b9++yaWQREREpB4s7dMWSW7szYk0pLKyEkVFRQnvXux2u7Fp0yZ07twZFoslSy3MjGAwiMrKShQWFubUzYcpsWS2XZ/Ph3nz5mH06NG8iV8e4PrOP1zn+YfrXN0++ACIvn77/vvA6ac37T25zptffbFBNJ4tExERERFlAEv7tIWBFBERERFRBrC0T1sYSBERERERZQBH7dMWBlJERERERBnA0j5tYSBFRERERJQB8aV9zEjlNgZSREREREQZwIyUtjCQIiIiIiLKAA42oS0MpIiIiIiIMoCDTWgLAykiIiIiogxgaZ+2MJAiIiIiIsoApbTPbhe/GUjlNgZSGiRJUr0/s2bNymrb3nvvvaz9fyIiIqJsUTJSxcXiN0v7cpsh2w2g5rdjx47w32+88QZuu+02rF27NjzNrlwGSZLX64XJZGq29hERERHlo+hAats2ZqRyHTNSqZJl8S3Ixo8sJ9XEsrKy8E9RUREkSQo/rq6uxgUXXIA2bdrAbrfj2GOPxcKFC2Ne36VLF9xxxx2YMGECCgsLMWXKFADAf/7zH7Rv3x5WqxVnnXUWHnzwQRQrl1RC3n//ffTp0wcWiwVdunTB7Nmz4Q/tJTp16gQAOOussyBJUvgxERERUT5QSvuKisRvBlK5jRmpVNXURApbM83pBGy2Jr6FE6NHj8Zdd90Fs9mMl156CWPGjMHatWtx8MEHh+d74IEHcNttt2HmzJkAgCVLluCKK67Afffdh9NPPx0LFy7ErbfeGvPeX3/9NSZMmIBHH30UAwcOxMaNG8NB2MyZM7FixQq0bt0aL7zwAkaOHAm9Xt+kz0JERESUS1japy0MpPJMr1690KtXr/DjO+64A++++y4++OADXHXVVeHpQ4YMwd///vfw43/+858YNWoUbrjhBgDAIYccgqVLl+Kjjz4KzzN79mz84x//wMSJEwFEMls33XQTZs6ciVatWgEAiouLUVZWltbPSURERKQ2SkZKCaSYkcptDKRSZbXWvptaJv93EzmdTsyaNQsff/wxduzYAb/fD5fLha1bt8bMd8wxx8Q8Xrt2Lc4666yYaccdd1xMILVq1SosWbIEd911V3haIBCA2+1GTU0NrM3QfiIiIqJc5PcDXq/4m4GUNjCQSpUkNbm8LptuuOEGLFiwAA888AC6deuGgoICnHPOOfAq3+wQWyM+o9PpxOzZs3H22WfXes5isTS6zURERES5LvoeUkofKZb25TYGUnlmyZIlmDRpUji75HQ6sXnz5gZfd+ihh2LFihUx0+If9+nTB2vXrkW3bt3qfB+j0YhAIJB6w4mIiIhymFLQpNdHrskzI5XbGEjlme7du+Odd97BmDFjIEkSbr31VgSDwQZfN23aNJx00kl48MEHMWbMGHz++ef45JNPIElSeJ7bbrsNp512Gjp06IBzzjkHOp0Oq1atwurVq3HnnXcCECP3LVq0CAMGDIDZbEZJSUnaPisRERGRWigZKZsNMBrF38xI5TYOf55nHnzwQZSUlOCEE07AmDFjMGLECPTp06fB1w0YMABPP/00HnzwQfTq1Quffvoprr/++piSvREjRuCjjz7C/Pnzceyxx+L444/HQw89hI4dO4bnmTNnDhYsWID27dujd+/eafmMRERERGqjBFJ2eySQYkYqtzEjpXGTJk3CpEmTwo87deqEzz//PGaeqVOnAkA4M/XHH39Ap6sdY0+ePBmTJ0+OeRxfxjdixAiMGDGizvaMGTMGY8aMSflzEBEREeUypbTPZgMMoTNwBlK5jYEUJe2BBx7AsGHDYLPZ8Mknn2Du3Ll48skns90sIiIiItWLLu1TAimW9uU2BlKUtO+++w73338/qqqq0KVLFzz66KO47LLLst0sIiIiItVjaZ/2MJCipL355pvZbgIRERFRTmJpn/ZwsAkiIiIiojRjaZ/2MJBKkizL2W4CUUq4zRIREamHkpFiaZ92MJBqgF6vBwB4vd4st4QoNTU1NQDETZCJiIgouxJlpBhI5Tb2kWqAwWCA1WrFnj17YDQaEw4LrhXBYBBerxdut1vTn1PrZFlGTU0Ndu/ejeLi4vDFACIiIsqe6MEmWNqnDQykGiBJEtq2bYtNmzZhy5Yt2W5OWsmyDJfLhYKCAkiSlO3mUBMVFxejrKws280gIiIixA42wdI+bWAglQSTyYTu3btrvrzP5/Phq6++wkknncRysBxnNBqZiSIiIlIRlvZpDwOpJOl0Olgslmw3I630ej38fj8sFgsDKSIiIqJmxNI+7WFHGCIiIiKiNGNpn/YwkCIiIiIiSjPeR0p7GEgREREREaUZ7yOlPQykiIiIiIjSjINNaA8DKSIiIiKiNONgE9rDUfuIiIiIiNIserCJYFD8zYxUbmNGioiIiIgojWSZpX1axECKiIiIiCiN3G4RTAEs7dMSBlJERERERGmklPUBgNXKUfu0goEUEREREVEaKWV9Fgug10cyUoFAJFNFuYeBFBERERFRGkWP2AdEAimAWalcxkCKiIiIiCiNokfsAyKlfQADqVzGQIqIiIiIKI2iR+wDmJHSiqwGUl999RXGjBmDdu3aQZIkvPfeezHPT5o0CZIkxfyMHDkyZp79+/fjggsuQGFhIYqLi3HppZfCGd2jj4iIiIgoi5RT00SlfRy5L3dlNZCqrq5Gr1698MQTT9Q5z8iRI7Fjx47wz2uvvRbz/AUXXIBff/0VCxYswEcffYSvvvoKU6ZMSXfTiYiIiIiSEp+R0usjzzEjlbsMDc+SPqNGjcKoUaPqncdsNqOsrCzhc2vWrMGnn36KFStW4JhjjgEAPPbYYxg9ejQeeOABtGvXrtnbTERERESUivjBJiRJZKX8fmakcllWA6lkfPHFF2jdujVKSkowZMgQ3HnnnWjRogUAYNmyZSguLg4HUQAwdOhQ6HQ6LF++HGeddVbC9/R4PPB4POHHlZWVAACfzwdfHm/NymfP52WQb7jO8wvXd/7hOs8/XOfqVFGhA6BHQUEQPl8AAGA0GuD3S3C5fE0KprjOm1+yy1LVgdTIkSNx9tlno3Pnzti4cSNuueUWjBo1CsuWLYNer8fOnTvRunXrmNcYDAaUlpZi586ddb7vPffcg9mzZ9eaPn/+fFit1mb/HLlmwYIF2W4CZRjXeX7h+s4/XOf5h+tcXX766RAAPbF//5+YN29laOpoAEYsXPgF2ratafL/4DpvPjU1ya0PVQdS48ePD/995JFH4qijjkLXrl3xxRdf4JRTTmn0+86YMQPTp08PP66srET79u0xfPhwFBYWNqnNuczn82HBggUYNmwYjNHjcpJmcZ3nF67v/MN1nn+4ztVp6VIxLEHPngdj9GjR9cRiMcDlAgYMGIwePRr/3lznzU+pVmuIqgOpeF26dEHLli2xYcMGnHLKKSgrK8Pu3btj5vH7/di/f3+d/aoA0e/KbDbXmm40GrkBgsshH3Gd5xeu7/zDdZ5/uM7VxeUSvx0OPYxGMdKEsnokyYjmWFVc580n2eWYU/eR2rZtG/bt24e2bdsCAPr374/y8nL88MMP4Xk+//xzBINB9OvXL1vNJCIiIiIKix+1D4gMgc5R+3JXVjNSTqcTGzZsCD/etGkTVq5cidLSUpSWlmL27NkYO3YsysrKsHHjRtx0003o1q0bRowYAQDo2bMnRo4cicmTJ+Ppp5+Gz+fD1VdfjfHjx3PEPiIiIiJShfj7SAGRQIpjROSurGakvv/+e/Tu3Ru9e/cGAEyfPh29e/fGbbfdBr1ej59//hmnn346DjnkEFx66aXo27cvvv7665iyvFdeeQU9evTAKaecgtGjR+PEE0/Es88+m62PREREREQUI1FGSqkeY0Yqd2U1IzV48GDIslzn85999lmD71FaWopXX321OZtFRERERNRs4u8jBbC0Twtyqo8UEREREVGuUUr7EvWRYmlf7mIgRURERESURizt0yYGUkREREREaVRfaR8zUrmLgRQRERERURrVV9rHjFTuYiBFRERERJRGLO3TJgZSRERERERp4vcDHo/4m6V92sJAioiIiIgoTZRsFMCMlNYwkCIiIiIiShMlkNLrAbM5Mp19pHIfAykiIiIiojSJHmhCkiLTWdqX+xhIERERERGlSaKBJgCW9mkBAykiIiIiojRJdA8pgKV9WsBAioiIiIgoTRLdQwpgaZ8WMJAiIiIiIkoTlvZpFwMpIiIiIqI0YWmfdjGQIiIiIiJKE5b2aRcDKSIiIiKiNKkrI8XSvtzHQIqIiIiIKE2YkdIuBlJERERERGlS12AT7COV+xhIERERERGlCUv7tIuBFBERERFRmrC0T7sYSBERERERpQnvI6VdDKSIiIiIiNKE95HSLgZSRERERERpwtI+7WIgRURERESUJhxsQrsYSBERERERpQmHP9cuBlJERERERGnC0j7tYiBFRERERJQmLO3TLgZSRERERERpIMsNZ6QYSOUuBlJERERERGngdotgCmBpnxYxkCIiIiIiSgOlrA/gDXm1iIEUEREREVEaKGV9Fgug18c+x4xU7mMgRURERESUBnUNfQ6wj5QWMJAiIiIiIkqDukbsA1japwUMpIiIiIiI0qCuEfsAlvZpAQMpIiIiIqI0YEZK2xhIERERERGlQTIZKQZSuYuBFBERERFRGiQz2ARL+3IXAykiIiIiojRgaZ+2GbLdAIqoqgKWLgV0OmDYsGy3hvLZX3+J7bBt22y3hIiIKHextE/bGEipyJYtwMiRQKtWwO7d2W4N5SufD+jVS+zg//qr9g0EiYiIKDks7dM2BlIqYjaL315vdttB+a2iAti3T/ztciUuRyAiIqKGsbRP29hHSkVMJvHb48luOyi/RQfyDOqJiIgaj6V92sZASkWYkSI1iA7kuS0SERE1Xn0ZKZb25T4GUiqiZKSCQV6doOyJDp6YHSUiImq8+vpIsbQv9zGQUhElIwUwE0DZw4wUERFR80imtC8YFD+UexhIqYiSkQKYCaDsYUaKiIioeSRT2gcwK5WrGEipiMEASJL4m5kAyhYONkFERNQ86stIKaV9AAOpXMVASkUkiSP3UfaxtI+IiKh5JHMfKYADTuQqBlIqw5H7KNtY2kdERNQ8krmPFMCMVK5iIKUyzEhRtjEjRURE1DzqK+3T6SJdOhhI5aasBlJfffUVxowZg3bt2kGSJLz33nvh53w+H26++WYceeSRsNlsaNeuHSZMmIDt27fHvEenTp0gSVLMz7333pvhT9J8lECKJ7CULewjRURE1HSBQOTiZKKMFMB7SeW6rAZS1dXV6NWrF5544olaz9XU1ODHH3/Erbfeih9//BHvvPMO1q5di9NPP73WvLfffjt27NgR/pk2bVommp8WSmkfM1KULdHbHrdDIiKixlHK+oDEGSmA95LKdYaGZ0mfUaNGYdSoUQmfKyoqwoIFC2KmPf744zjuuOOwdetWdOjQITzd4XCgrKwsrW3NFGakKNuYkSIiImo6paxPp4u9V2g0JSPFQCo3ZTWQSlVFRQUkSUJxcXHM9HvvvRd33HEHOnTogPPPPx/XX389DIa6P5rH44En6lJ7ZWUlAFFO6MtybtVkMgCQUF3th88nZ/R/K58928uAMifROq+p0QHQA0BWtkNKH37H8w/Xef7hOleP8nIAMMJul+GvI1IS56sSamp8jS7v4zpvfskuy5wJpNxuN26++Wacd955KCwsDE+/5ppr0KdPH5SWlmLp0qWYMWMGduzYgQcffLDO97rnnnswe/bsWtPnz58Pq9WalvYny+U6CUAJli79Hl7vrqy0IT4TSNoXvc5XruwC4EgAwI8/rkbLlluy1CpKF37H8w/Xef7hOs++P/4oAjAYer0H8+Z9lnCeYHAEAAsWL/4amzZVNen/cZ03n5qamqTmk2RZVsXlZkmS8O677+LMM8+s9ZzP58PYsWOxbds2fPHFFzGBVLznn38el19+OZxOJ8x15FETZaTat2+PvXv31vvemTB4sB5Ll+rwxht+nHVW5jNSCxYswLBhw2CMHpOTNCvROp8zR4cZM0RG6pFHArjyymA2m0jNiN/x/MN1nn+4ztVjyRIJJ59sQLduMn77LXFGqnNnA/76S8Ly5T707t24/8N13vwqKyvRsmVLVFRU1BsbqD4j5fP5MG7cOGzZsgWff/55g4FOv3794Pf7sXnzZhx66KEJ5zGbzQmDLKPRmPUNUGlWMGhAtpqihuVAmRW9zgOByHS/Xw+jUZ+lVlG68Duef7jO8w/XefZFRuyT6lwXkZ4oxiaf93GdN59kl6OqAykliFq/fj0WL16MFi1aNPialStXQqfToXXr1hloYfPjqH2UbRxsgoiIqOnqu4eUgqP25basBlJOpxMbNmwIP960aRNWrlyJ0tJStG3bFueccw5+/PFHfPTRRwgEAti5cycAoLS0FCaTCcuWLcPy5ctx8sknw+FwYNmyZbj++utx4YUXoqSkJFsfq0k4ah9lG4c/JyIiajpl+PO67iEF8D5SuS6rgdT333+Pk08+Ofx4+vTpAICJEydi1qxZ+OCDDwAARx99dMzrFi9ejMGDB8NsNuP111/HrFmz4PF40LlzZ1x//fXh98lFzEhRtjEjRURE1HRKIFVfRorDn+e2rAZSgwcPRn1jXTQ0DkafPn3w7bffNnezsooZKcq26CCe2yEREVHjKAO/FRTUPQ9L+3KbLtsNoFjMSFG2RQdP3A6JiIgaRzme1nUzXoClfbmOgZTKMCNF2caMFBERUdMpx9P6AilmpHIbAymVYUaKso19pIiIiJpOOYYqF8kTYR+p3MZASmWYkaJs46h9RERETZdMRoqlfbmNgZTKMCNF2caMFBERUdOxtE/7GEipDDNSlG3MSBERETUdS/u0j4GUyihXLRhIUbYwI0VERNR0LO3TPgZSKqNctWAmgLKFgRQREVHTJZORYmlfbmMgpTLMSFG2sbSPiIio6VLJSDGQyk0MpFSGGSnKNmakiIiImo6lfdrHQEplmJGibGNGioiIqOlY2qd9DKRUhhkpyjZmpIiIiJqOGSntYyClMsxIUbZFB/HcDomIiBpHOZ5y+HPtYiClMsxIUbZFB0/cDomIiBpHOZ7yhrzaxUBKZZiRomxjRoqIiKjpWNqnfQykVIYZKco2ZqSIiIiaLpnBJljal9sYSKmM8mVjJoCyIRAQPwpuh0RERI2TTEaKpX25jYGUyihfNmYCKBviAyevF5Dl7LSFiIgol6Uy2ARL+3ITAymVYUaKsil+u5NlXiUjIiJqDA42oX0MpFSGGSnKpkTbHYN6IiKi1KUy2AQDqdzEQEplmJGibFK2O52u9jQiIiJKXiqDTbC0LzcxkFIZ5aqFzwcEg9ltC+Uf5epZQQEgSbHTiIiIKDl+f+Q8jqV92sVASmWir1rw6gRlWnQ9N7OjREREjRN9EZIZKe1iIKUy0VctmAmgTIuu52Z/PSIiosaJvgjJPlLaZch2AyhW9FULZgIo06LruZWrY9wOiYiIUqNchJSkSLCUCEv7chszUiqj00W+cMlkAv76C3j5ZZ7sUvOIzkixtI+IiKhxoi9MKn2OE2FpX25jIKVCqZzAzpgBTJgAfPBBettE+SF6x8/SPiIiosZJZuhzgKV9uY6BlAqlcgK7a1fsb6Km4GATRERETaecw9U30ATA0r5cx0BKhVI5gXW7Y38TNUX0jp8ZKSLKVX/+KSo2tm3LdksoX0VfmKwPS/tyGwMpFUrlBJaBFDUnZqSISAuefhq4917xmygbWNqXHxhIqRAzUupQUSGuaP7yS7ZbkjnRGSkGUkSUq8rLxe+Kiqw2g/JYdJ/j+rC0L7dx+HMVYkZKHd55R1zR3LQJeP31bLcmM6IzUkqZAUv7iCjX8NhI2ZZqRoqlfbmJGSkVYkZKHZQrmZWV2W1HJjEjRURawGMjZRsHm8gPDKRUSLl6kcwJrMsV+5uaTz4eiBMNf85AiohyTT7uv0ldUh1sgoFUbmJpnwopVy9Y2pdd+bhso0sRWNpHRLkqH/ffpC4s7csPzEipUCqZAB4s0icfl210RoqlfUSUq5T9Nqs1KFs42ER+YEZKhZLNSAUCkSsY+XSynynK8s+njAwzUkSkBfl4IYzUhRmp/MCMlAolm5GKPsHlwaL55eOBmBkpItKCfNx/k7okO9gE+0jlNgZSKpRsRir6AMGDRfPLxwMxb8hLRFqQj/tvUpdkB5tgaV9uYyClQslmpBhIpVc+l/ZFj9qXT5+fiLSBgRRlG0v78gMDKRViRkod8vFAzIwUEWlBPu6/SV2SHWyCpX25jYGUCjEjpQ75eCBmRoqItCAf99+kLslmpFjal9sYSKkQM1LqoCzTQCB/dnDMSBGRFnD4c8q2VAebkGVxvkG5hYGUCiV7AstAKr2iA9l8ycpE7/gZSBFRLpLl2IyULGe3PZSfkh1swhB1I6J8uWirJQykVCjZkioGUumVj8s3uqabpX1ElIv8fiAYFH8Hgzw5pexItbQP4LaaixhIqRAzUuqQj8s3esfPjBQR5aL4/XW+7L9JXVIdbALgyH25iIGUCjUmI5VP/XgyJR8DqUQZKQZSRJRLGEiRGqQ6/DnA87hcxEBKhRqTkUr0mJomn/tIRWek8uWzE5E28NhIapDsYBM6nfgBGEjlIgZSKtSYjFSix9Q0+Z6RYmkfEeWi+P01R+6jbEh2sAmAN+XNZVkNpL766iuMGTMG7dq1gyRJeO+992Kel2UZt912G9q2bYuCggIMHToU69evj5ln//79uOCCC1BYWIji4mJceumlcDqdGfwUzY8ZKXXI50DKbOZgE0SUm3hsJDVItrQP4L2kcllWA6nq6mr06tULTzzxRMLn77//fjz66KN4+umnsXz5cthsNowYMQLuqL3iBRdcgF9//RULFizARx99hK+++gpTpkzJ1EdIC2ak1CEfAykOf05EuY7HRlKDZAebAJiRymWGhmdJn1GjRmHUqFEJn5NlGQ8//DD+9a9/4YwzzgAAvPTSS2jTpg3ee+89jB8/HmvWrMGnn36KFStW4JhjjgEAPPbYYxg9ejQeeOABtGvXLmOfpTkxI5V9fn/sjfHyJSsTnZFSPjMDKSLKJTw2khqkkpFSAql8z0jJMiBJ2W5FahoVSPn9fnzxxRfYuHEjzj//fDgcDmzfvh2FhYWw2+3N0rBNmzZh586dGDp0aHhaUVER+vXrh2XLlmH8+PFYtmwZiouLw0EUAAwdOhQ6nQ7Lly/HWWedlfC9PR4PPFFnxpWVlQAAn88HnwouB+j1EgAD3O4gfL66b3NdXa0DoA8/djr98Pkaf+dB5bOrYRlkW3U1AERu7tDUZatW8evc4zEAkCBJvvB26PHI8PnyfO+uEfyO5598XOdOp9h3RR5rc/9dl3xc52rkdovjqU7X8PZnNIp5XS5fo7JSWljnP/0E3HCDHs8/H0DHjtluTfLLMuVAasuWLRg5ciS2bt0Kj8eDYcOGweFw4L777oPH48HTTz+dcmMT2blzJwCgTZs2MdPbtGkTfm7nzp1o3bp1zPMGgwGlpaXheRK55557MHv27FrT58+fD6vV2tSmN9nKla0AnIC9e6swb94Xdc7322+HAegefrx48TLs2rW/yf9/wYIFTX6PXFdZaQQwOvz4229XwmT6K3sNSjNlndfUjAZgxLfffomaGgOAwaisdGPevPlZbR81L37H808+rfPvvmsD4Pjw4yVLfoDPV/c5gVbl0zpXowMHhgBw4KefvoXPt6/eeX2+4QAK8MUXS7BtW0Wj/2eurnOn04i//30Qdu2yYfLkv3DddT9mu0moqalJar6UA6lrr70WxxxzDFatWoUWLVqEp5911lmYPHlyqm+XFTNmzMD06dPDjysrK9G+fXsMHz4chYWFWWyZYLeLvKbZXIjRo0fXOd+CBbFd3Hr37o9TTmlaRmrBggUYNmwYjNG32s5Df8XFTIcddjRGj+6VncakUfw6DwTELmHYsEGoqhLzSJKl3u2Qcge/4/knH9d5TU1sbdBhh/XF6NH5lZHKt3WuRiLLBAwadDz69at/+3M4DNi3Dzj++BNx3HGpb6u5vM5lGRg7Vo9du3To3FnGG2+UoaQk++ccSrVaQ1IOpL7++mssXboUprjec506dcJf8WefTVBWVgYA2LVrF9q2bRuevmvXLhx99NHheXbv3h3zOr/fj/3794dfn4jZbIY5QdGq0WhUxQaoJMV8Pqne9sT3XfH7DWiO5qtlOWRTMBj72OdrnmWrVkajEQaDMbxN2e3G8DLweuvfDin38Duef/JpncdX5DTXsTHX5NM6VyPleGq1Nrz9RZ5v2raai+v83/8GPvpIjA/w1lsSWrdWR/uTXY4pj9oXDAYRCNTut7Nt2zY4HI5U365OnTt3RllZGRYtWhSeVllZieXLl6N///4AgP79+6O8vBw//PBDeJ7PP/8cwWAQ/fr1a7a2ZFqyo/bF3xuDHWqbTz52Vvb7xZUhgKP2EVHuysf9N6lPYwabyOEuTo3y9dfAjBni70cfBfr2zW57GiPlQGr48OF4+OGHw48lSYLT6cTMmTNTLv9xOp1YuXIlVq5cCUAMMLFy5Ups3boVkiThuuuuw5133okPPvgAv/zyCyZMmIB27drhzDPPBAD07NkTI0eOxOTJk/Hdd99hyZIluPrqqzF+/PicHbEPaPyofbzpYPPJxwNx9PZmMsUG9HL+VMUQUY7Lx/03qQ9H7avfrl3A3/4mRki+8EIgV+9clHJp35w5czBixAgcdthhcLvdOP/887F+/Xq0bNkSr732Wkrv9f333+Pkk08OP1b6LU2cOBEvvvgibrrpJlRXV2PKlCkoLy/HiSeeiE8//RQWiyX8mldeeQVXX301TjnlFOh0OowdOxaPPvpoqh9LVXgfqeyLX/b5MPx59GeMHv5clsWOzpDVmyUQESWHx0ZSg1TuI5VvN+QNBIDzzwd27AAOOwx4+uncG/ZckfKp0cEHH4xVq1bh9ddfx88//wyn04lLL70UF1xwAQoKClJ6r8GDB0Ou51K3JEm4/fbbcfvtt9c5T2lpKV599dWU/q/a8T5S2ZePy1bZ3iQJ0Otjr6J5vQykiCg35OP+m9RFllnaV59Zs4DPPwdsNuB//xO/c1WjTo0MBgMuvPDC5m4LhShfumQDKbsdcDp5sGhO+Xggjr4ZryTFXkXzeCKDoBARqRnL3inbogMiZqRiffopcOed4u///Afo2TO77WmqlAOpl156qd7nJ0yY0OjGkKB86YJB8aWqKxOgHCyKixlINbd8DKSUq2cmEwBZhkEPACLXzgEniChX5OP+m9Ql+pjJPlKx/vlP8fvKK4HzzstuW5pDo+4jFc3n86GmpgYmkwlWq5WBVDNItqQqOpDato0Hi+aUj32klB2/xRQEBpwIyWSC2bQYHq+UF5+fiLRBORbqdOKCJI+NlGnxfY4bkk+lfbt2id+XXprddjSXlEftO3DgQMyP0+nE2rVrceKJJ6Y82AQlFl9SVZfoQCr6MTVdPl7RVLa1NoZ9wLJlwJdfosRUDYAZKSLKHTw2UrYpx0ydTvQ5bkg+lfYppbZa6S6QciCVSPfu3XHvvffWylZR4xgMkdFL6juB5cEiffIxkFK2tWJjdXhakbEm5jkiIrXjsZGyLZWBJoD8ykjViNMKBlLxDAYDtm/f3lxvl9eiO/ozI5Ud+RhIKdtascEZnqYEUiztI6JcwWMjZVtMn+Mk5EsfqehSW60EUin3kfrggw9iHsuyjB07duDxxx/HgAEDmq1h+U65jw8zUtmh7AQtFrFc8yGQULa1Qn0kI1VoYEaKiHJL/LGRo/ZRpkWPgpuMfCnti/4upnjHJNVKOZA688wzYx5LkoRWrVphyJAhmDNnTnO1K+8xI5Vd0ct25878WLbKthYdSDEjRUS5hsdGyjaW9iXGQApAMBhMRzsoTkP3kvL7I1cueLBofsqyLCrKn0BK2dYcUqS0z6FnRoqIcouyvy4piX1MlCnKMZOlfbGU/lFmc3KDcOSCZusjRc2roYxU9HQGUs0vOpAC8iMjo3xGhy6SkbLrGEgRUW5hRoqyLdWMVL6U9mltoAkgyYzU9OnTk37DBx98sNGNoYiGMlLRBwblZJ8Hi+YTHnihWPzOh2WrbGt2KRJIKRmpfAgkiUgbGEhRtjV2sAmtl/blbSD1008/JfVmkjJmNzVZQxkp5cBgMAB2e+w0arp8PBAnKu2zScxIEVFuycf9N6lLqoNN5Etpn9JHSiv9o4AkA6nFixenux0URwmkGspIWSziJ3oaNV18aV8+LFslaLeidmkfM1JElCs4ah9lG0v7EtNiRop9pFRK+fI1lJFiIJUe+dhHSgnabXIkkGJGiohyDTNSlG2NHWyCpX25J+VR+wDg+++/x5tvvomtW7fCG3eG9c477zRLw/IdM1LZlY99pMIZKTlS2mcFAykiyi0ctY+yjRmpxLQYSKWckXr99ddxwgknYM2aNXj33Xfh8/nw66+/4vPPP0eRcvmemowZqeyKv6IZPdy8VinBkjUYyUgpgVQ+ZOSIKPfJcu39t88HBAJZaxLlocYONqH18wwt9pFKOZC6++678dBDD+HDDz+EyWTCI488gt9//x3jxo1Dhw4d0tHGvMSMVHbFl/YB2g8mlM9XEIhkpApkZqSIKHf4fCKYAiKBFKD9/TepS2MHm2BpX+5JOZDauHEjTj31VACAyWRCdXU1JEnC9ddfj2effbbZG5ivGpuRUg4g1DTK8i0sjEzT+oFY2fFbApGMlIWBFBHlkES3BomfTpRuLO1LjIEUgJKSElRVVQEADjroIKxevRoAUF5ejhplCVGTNSYjFQxq/0uYKcpO0G6PXCnS+oE4vOOPCqQKgiztI6LcEb2fttki+2+O3EeZxMEmEsvrQEoJmE466SQsWLAAAHDuuefi2muvxeTJk3HeeefhlFNOSU8r81BDGSnloBAdSAHaP9nPlHwsnQxnpHyR0j5LkBkpIsodyrHRbAYkKX/236QuqWak2EcqdyUdSB111FHo168fjjzySJx77rkAgH/+85+YPn06du3ahbFjx+K5555LW0PzTSoZqegvKq+6NQ9l+ZrNkeWr9QNxuHOsP5KRMjMjRUQ5JPrYGP1b6/tvUpdUB5tgaV/uSjqQ+vLLL3H44YfjnnvuQc+ePTFx4kQsWbIE//jHP/DBBx9gzpw5KFHGGqUmS7aPVEGBuOqWLyf7maIs9+iMlNaDiXApgi8SSJkCrpjniIjUTDkGdjdtAW64AV2NW2OmE2UCB5tILK8DqYEDB+L555/Hjh078Nhjj2Hz5s0YNGgQDjnkENx3333YuXNnOtuZd1LJSEX/5sGieeRzaZ/JGyntM/tZ2kdEuUPZT1/sfRqYMwcT3c/ETCfKBJb2JZbXgZTCZrPh4osvxpdffol169bh3HPPxRNPPIEOHTrg9NNPT0cb81Iqo/ZF/+bBonkkKp3U+rJVtjWDJyoj5WdpHxHlDmU/3ULaDwAokcpjphNlQqqDTeRLaV9e95FKpFu3brjlllvwr3/9Cw6HAx9//HFztSvvpZqRUjZKHiyaLhCI7MzM5vwq7dPDD4M/8kGNzEgRUQ5RjoF2SVwQsunEPoz9hymTGpuRYmlf7jE09oVfffUVnn/+ebz99tvQ6XQYN24cLr300uZsW15jRip7opd5PpX2eTyADdUx04xeZqSIKHeEA6nQvsyGmpjpRJmQ6mATLO3LXSkFUtu3b8eLL76IF198ERs2bMAJJ5yARx99FOPGjYPNZktXG/PGun3rMOHdCbCZbBhjWgSAfaSyIXoZ5lMg5fXWDqQMPmakiCh3KPtpayiAsob2aVrff5O6pDrYRL6U9uV1IDVq1CgsXLgQLVu2xIQJE3DJJZfg0EMPTWfb8k5QDmL5X8tRbCnGOYViGgOpzFOWoV4vrhLlUx+pWoGUl4EUEeWOcCAli32ZlRkpygKW9iWmxT5SSQdSRqMR//vf/3DaaadBr9ens015y26yAwCcXieMRhmAxNK+LIi+hxSQX32k7BAj9gULLNC53NAF/DDAB4/HmOXWERE1LHxrEFnsyyxBZ8x0okzgYBOJ5XVG6oMPPkhnOwiRQMof9ENv9gIwMyOVBdH3kIr+rfVl6/EARaGM1DazBx1CV46sqIHXW5TFlhERJSdcUYC/AACS9GfMdKJMYEYqMS0GUk0atY+al80Y6WcW1IuraMxIZV6+LtvoPlL7zTL8kpguAqksNoyIKEnh/bdfXAkyBzhqH2UeB5tIjIEUpZVRb4RZLy5fBI0ikGJGKvPiS/vyqY+UUtrnNAE1oVIDK2o0X9ZIRNoQLu3zi4Onxe+LmU6UCRxsojZZ1mYfKQZSKqOU9wX1IjPAjFTm1bVstR5MRGekquMCKWakiCgXhAMpnzgjtYR+89hImcTSvto8HhFMAcxIURopgVRAz4xUtuRrH6mYQMrIjBQR5R63GzDCC2NQnLExkKJsaGiwCVmW8eovr+L77d8DyI/SPqWsD2BGitJICaR8EvtIZUu+Ltv6SvuYkSKiXOB2x97GwRSUYYBP8/tvUpeGMlLP/fQcLnjnAkx4dwKA/CjtUwIpozHyebWAgZTKhEfu0zEjlS352EdKlkVJAUv7iCiXxQdSgNiHaXn/TepT32ATO6p24Ib5N4i/nTsA5Edpnxb7RwEMpFRHCaS8YEYqW/Kxj5QSKCkZKZb2EVEuYiBFalDfYBNXf3I1KjwVAIAqTxVkWc6r0j4t9Y8CGEipTnxpHzNSmZePfaSU7Uw5AWFpHxHloroCKQ5/TplUV2nfO2vewTtr3oFOEqffATkAT8CTV6V9DKQorZRAyiMzI5Ut8cs2H0r7lO2srtK+YFDbO3gi0gZmpCjblFJ5ILa0r9xdjqnzpgIAbjrhpvD0Kk9VXpT2MZCijIgPpJiRyrz4PlL5sGyV7cwhJS7ti56HiEitRCDljJlmRZWm99+kLtHHyuiM1I3zb8RO504c2uJQzBw8E1ajiCiqvFV5UdrHPlKUEUog5Q6KA4HPBwSDsfP4/UAgIP626H3A+vV5cbKfKXWV9mm5n5Dy2RxS4tI+gIEUEamf2w3YdBUx06zG/Tw2UsZEnysoGanFmxbj/376PwDAf8b8BxaDJXy+5/Q6Y0r7lHstaQ0zUpQR8YEUUDvVG31AcMz+O3DIIei4bkGt56hx8jHbFx5sQkpc2gdoO5AkIm1wuwGbfn/MNKuBgRRlTnxGyuVzYfKHkwEAVx5zJQZ2HAgAcJgcAGJL+4DIhXKtYSBFGaEEUq5AJJCKP4GNPiAYfl8NACjd/Xut56hx8rmPlF2qfR8ph44ZKSLKDSKQOhAzzaZnIEWZoxxPDQZApwNmfTELGw9sxEGOg3Dv0HvD8znMoUDKGxtIabW8j4EUZYQSSNX4I4FU/AmsUmdqMgFShShhsITm58hETVdXHyktZ2R8PglA1GATUX2k7CoJpPx+YM0a7ZY9EFHTidK+8phpVn05j42UMcqx0mQCftn1C+YsmwMAeOrUp1BoLgzPl6i0D9DugBPsI0UZoXyxqn3O8BWKujJSFguA8nIAgDkUSPGqW9Pl8/DnVrl2aZ8SSGU7kLz1VuCww4B3381uO4hIvVwuwKaP6yOlr9D0/pvUJXro8/kb5yMgBzC863CMOXRMzHx1lfYxI5VbGEipTPQVCqWTYnwmIGEg5WUg1VyUZWg0+/Hh2g/hlvbFTNei8PDncu3SPpukjozUmjXi99q12W0HEamX2w3YpMqYaVZdpab336QuyvHUZBLncgDQpbhLrflY2qcNDKRUJjqQUkrL6sxImWUgVNpnZCDVbJRluFZ6F6e/fjoe+e0fMdO1SARJMgqCtUv7lEAq2xmpqirxu7q6/vmIKH+JQCpu+HNJDH/OsmDKBOWio9kMVPvEActmstWaz26MnO9JEqDXi+laLe1jIEUZkUpGqtRcHR7exehhINVclGVYKW0FAOxyid/ZDiTSyeMBzPBADzHWfsyofSrJSCmBlNNZ/3xElJ9kWezLbFJVzHSrTjzO9j6M8kN0aZ+SkVLO7aKFM1IesX1q/V5S7CNFGZFKRqqloTw8zeCpjnmOGk9Z3n59aACPQOSeXlodltTrBexRN7GMuSGvrI5AqjJUrcNAiogSCZcohwbNORDq32qVeHykzIku7QtnpIxxGanKSsy46lU8Ok+U9gGIuZeUFikZqRGLbgJ69gQOHKj/BTmCgZTKpJKRigmkXJGMFMsXmkZZvn6dWKY19QxFrxUeT+Tkw20AAvpIIFWgkvtIMSNFRPVR9t02WVz63hMqIVLuhceR+ygTYkr7vHWU9i1ejDab9mDcr5GslZKR0nppX4+f3wB+/x1Ytiy7DWomqg+kOnXqBEmSav1MnToVADB48OBaz11xxRVZbnUjbduG1rc/iDsXAS6/CyazSH/UlZEqjRqZSO+qe7h0So2yfHXeA/j7EqDV9gO1ntMany926HMgKpAKqiMjxT5SRFSfSCAl9ll7Queu9lAgpdX9N6lLosEmapX2/fADAKDIHclIab20TwmkTO5QecnWrdlrTDMyNDxLdq1YsQKBqHqq1atXY9iwYTj33HPD0yZPnozbb789/Niaqz3ZKipgfegxXF4A/OsUwGCtBlBYdx+pqHtl6GqcMc8rZYGUOmX5Hr/yN9y8AHh7z258pRdlfdnOyqSLxyOFS/ucoUyoEkhZ5OxnpGQ5koliRoqIElH23XaIP5SMlA2umOeJ0inhYBPxpX2hQMoSANzO0KBhGi/tExlhGSYXA6mMatWqVczje++9F127dsWgQYPC06xWK8rKyjLdtOZXWgoAKHEDUhAwWJ0ACuvMSJVI5ZGJ1WLUF1kWzxcVZaTFmhSus3eKL3tJpQ8Wi8iEaPVA7PXGZqT0kh41RnEBw6yCjFR1daRklYEUESUSzkgFxU58n10CIMMqe2KeJ0qn6MEm9oRK+2plpH78MfynXCkCqXwo7bOhGpIsBrViIJUFXq8X//3vfzF9+nRIkhSe/sorr+C///0vysrKMGbMGNx66631ZqU8Hg88UdFJZagXu8/ngy+bW7DdDiMAvQw4vIBkrgTQDjU1fvh8kY5P1dU6AHo45PLIa51OWCwyXC4JVVU+JSZLifLZs7oMVMDtNgCQoPeIAMLuDsJk8aO62oCqKp+mdnLKuq6pCYQzUtUmoMRSAo9pP4AgCgI1AGS4XIGY7TCT9u8HAHG5rqpKhs+n0Ut2acbveP7Jp3Uuyn+NsAXFVZ+aIhsAZziQcjr9WduHZVI+rXM1qqmRABhgMATDZXtmnTmyPrZvh3HnzvD8cnk5fD4fDAZx7uF2p76d5sI6r642oBCRe7wFt2xBQMXtTXZZ5lQg9d5776G8vByTJk0KTzv//PPRsWNHtGvXDj///DNuvvlmrF27Fu+8806d73PPPfdg9uzZtabPnz8/62WBp5lM0Hu9KHEBle5tAHpgxYpVsFq3hedZubIbgMMhlW8JT5OrqqAv8AIwY/78r3DQQY2/bL9gwYLGfwANqKwcCcCMYJX4wju8AIz7ALTBokVLsGlTRX0vz0lr1mwMZ6ScJkAf0CNgMgOhkhgL3Pjhh3Vo02ZzVtr31192AKcAAPbtc2HevPzeRpsq37/j+Sgf1vnatSUAToLVL06Ayq1mAE4UBERg9cUX32Lfvn3Za2CG5cM6V6Mff+wEoBcOHNiJ/VX7AQA/Lf8JFb+Ic4c2332H46Pm9+7Zi3nz5sHjOQWAHV9/vQz79u1v1P9W8zrft28oSqICKffatVgwb14WW1S/GqVTVwNyKpB67rnnMGrUKLRr1y48bcqUKeG/jzzySLRt2xannHIKNm7ciK5duyZ8nxkzZmD69Onhx5WVlWjfvj2GDx+OwsLC9H2AJOhatgS2b0epCzC3NWA9gJ49e2H06KPC8/z0kxgj5GB7MPK6YBAtHUE4q4HjjjsJvXql/r99Ph8WLFiAYcOGwagU6+ahYFB8LWyhWjKHB3C08ePATuDYY09Ev37auaKprPMOHbphF5YCEKV9rYpaocKohxJIWVGDQw45AqNHH5aVdv7wQyQDHQgUYPTo0VlpR67jdzz/5NM6t9nEfsIWEBlr88EHAdgHa+hxr17HY+RI7ey/65JP61yN1q8X52gdOpThN50okR8xZAS6l3YHAOhC/aMUDn8Qo0ePRlGRAdu3A8cc0x+DB6eekVL7OpckA4oQuRBdsH8/Ro8YEbkTscoo1WoNyZlAasuWLVi4cGG9mSYA6NevHwBgw4YNdQZSZrMZ5gSjMRiNxuxvgCUlwPbtKHEDbouIhgMBA6KbpWQbHXLsTQdLTdXYjAIEAkY05WOoYjlkkVL1aQx1Cir0AEabyNb4/YYmLVu18vl0MYNNOMwOeINeuPWiM6wVNfD7W8BozM4OL3rYYqdTyuvtsznk+3c8H+XDOvf7AR0CsATERUZdmzYAAKs/EHpem/vvuuTDOlcjZXw0S4EUHrWv2FocWRcrV8bMb6yqCa0rZUrjt1M1r/OaGsSU9kmBAIx79wIHH5zFVtUt2eWo+uHPFS+88AJat26NU089td75VoY20LZt22agVWlQUiJ+uQCYxBewrsEm7L7y2JcanTHPU+oCgUigavSIQMruBYwF4iqKVpdtzPDnJtEx1m6yR27Ki5qsDjZRFXXNwOfL/lDsRKQ+bndkPwYAprKDAEQCKa3uv0ldwhdjzV4EZLHtxYzaF8pIBQsdAIAClx/egDccPKm421CjyXLtQAqAJgacyIlAKhgM4oUXXsDEiRNDnfGEjRs34o477sAPP/yAzZs344MPPsCECRNw0kkn4aijjqrnHVUsNEpEaVQgVdfw5zYGUs0uOmg1ecTeTAfAbt5b63ktib4hb7VRBFI2o02VgRTAe0kRUW3RgVQQgL1NBwCA1RcMP0+UbuHzBHOkr3r4hrw7dwLbtwOSBJwwAABQ5BH3m9LyfaR8PnGhOrq0D4AmAqmcKO1buHAhtm7diksuuSRmuslkwsKFC/Hwww+juroa7du3x9ixY/Gvf/0rSy1tBkpGyg3IxvozUgW+2A2y2MBAqqnCy1rvhcUbqVEuNOwBoN1l6/XG3kfKbrLD7XfHBFLZDCLjAymnM/xVISICEBtIVZuA4pYiI2UJiJI/t1udfTFIW5SLjjqz2BbNejMMutDptjLseY8e0B0kts9CD1DlqYLRKC6kazGQUsrztZiRyolAavjw4ZDl2h3v2rdvjy+//DILLUqjqNI+JZCqKyNV4CmPmV6kZyDVVMqyk8zVsEal1wt1e2Oe15qYjFQokHL5XXCpJCMV3+eT95IiongxgZQRaNmyQ/i5AjjhdvMGi5R+4YuOxtDNeE1RZX1KINW3b/iGn0VuoMpbpen7SCkD4BVJlUD06bwGAqmcKO3LK1GlfUFD/Rkpi6scgDhgAAykmoOy7MwOZ0wgZZX2xzyvNfE35FVzHymApX1EVFt8RqpVafvwc1bT3phBa4jSJXysDHXPiLkZrzJiX58+QGiU6Hwo7VMCqZaGUCWVMkI2AylqdlGlfQF9/Rkpk1tskH+J/opwSAykmiq8bO1VMYGUAyKQ0nIfqfjSvvg+Umor7SMiiiYCKbGzqDYCLeytIvsw4x4eGykjlGOlrGSkEgw0EZ2RipT2iae0HEiV6EPlJUccIX7/+Wd2GtSMGEipTVRpnxJIJcpImeGGwSeOCttCgT0DqaYLj7Zji8tIyeUAtLtsc2nUPoCBFBHV5nYDNoO46KXsx2pM4t5SVsM+ze6/SV3CgZQhrrRvz55I4HD00XlV2qdkg4t1cYEUM1LU7EKBVKkL8NeTkVJGPgkC2B7KSCkZBR4sGk9ZdrUCqUBlzPNak2jUvvhAihkpIlIztxuw6UOBlBEoMBTAbRKnOVbDfs3uv0ldlHM2pXtGuLRP6R91yCGitC0PS/uKpFBpnxJI7d+f8wd0BlJqE+ojVeIG/FLdGSklkKoyA5WhewszkGo6ZdkZrHGBlL8q5nmt8XobLu1TU0aKfaSIKJ4IpMoBAB6LHpIkwWMSI/XZGEhRhijnbAF9XGlfdFkfkJelfeFR+w4+OPz5c728j4GU2kSV9vmkxBkplwsoRjkAoNwiTnwBoEDWdtYkE5Rlp7dUxQVSiYNarYgZbEKFpX3KqH2m0Lae4xewiCgN3G7ApisHAHjNhpjfVn05j42UEcqxMhxIKaV9SkaqTx/xO49K+5RAyhE6T0VREdAhNKpmjpf3MZBSm1AgVewB/EFxGT5RRipRIGUJHgAAjkzUBMqytpgPxHw5rD5tZ/s8Hqn2DXlN6htsom1b8ZuBFBHFE4GUqNbwWsSB0WcROzGrvoLHRsoI5Vjp14VK+4yh0r74jFSotK8wD0r7lO+eIxA1ah8DKUqLqLuMFrhF5F5fH6kKcySQMvvLw89T4yjLzqY/EDO9wOuKeV5rEpX2qSkjpQRSZWXiN0v7iCieywXYQp3Z/QXiwOg3i99WXYVm99+kLuFASorKSO3bB2zeLJ7o3Vv8jirtc7oq86K0T+lvzkCK0sdoRMBmBQAU1KSWkTKFIn0eLBovfLPjuEDK6nXHPK81fk8ABRAfrlrFw58zI0VEdXG7AWuoJN5fYAEABApEJ2Krrkqz+29SF+WiYziQMtqAn34SE7t2BYqLxd+hQEoHwFd5QPOlfRKCKAj1N2dpH6VVsFh8uew14kuYKCOVMJDysY9UU4UDKWVkmRCrV0QRWu0jpffUhP9Wc0aKgRQR1UXcR0rsHIJWEUgFlUBKcvLYSBmhnCd4EDVqX3xZHwBYLAjoxWl4oOKApkv7ampEP2wdZDGBGSlKq1JR3mercQGQY07eZTmutC86kPJqe2S5TAj3kZIqY6ZbPeISkVaXrcEdOvkA4DYk7iOVrUDK44lcTGAgRUR1EYGUuAAZtBbE/GYgRZmiHK98iCrtSxRISRJ8DlGBJFeUh0v7tJiRcrki560wGACLhYEUpY9U0gIAUOSSAYM75gTW5xPBVMKMlFd8aXmwaDxl2Vnk2EDKpvFAyuiNjNgHSV33kYoe+px9pIioLiKQEtl12RoaKa1AnKhaUaPZ/Tepi3Ks9MpRpX3xI/aF+O1i+9RVVmk+IxUe+ryoCJCkSCD1559AMJi9xjURAykV0peKQKrUBcDkjDmBVQ4EiQIps0fbAyJkgrLsTMHYGxfZPAEA2i3tM3kjA00YdUaY9Ka4QKo6axkpJZAqKIiUljMjRUTx3G7AJoeGB7PZYn7b5BqO2kcZoZwnuIPiQFXiloCNG8XEuEAq6HAAAKRK7d9HKhxIhUYrRLt2gE4nMgS7dmWvcU3EQEqFpBYikCpxAzA5Y05glRP96FH7TEWiFNDs0faACJkQzkgFxZWkgEHczNHhlgEpoNlla/LFDn0OIHawCSn7gZTDETk3YiBFRPHcbsAWFDtpnV3sx3RW8dsKl2b336QuyrHSHTqPaLdhp5jQqRNQWhozr1wkggpDVbXmB5tQzlv9dit+2vGTKPE76CAxQw6X9zGQUqOom/LWlZEqlcoBiIxUccuDAQDm0Iw8WDSesqxNflEe4mlRDABweAGYqjW5bAMBwBIUn1cZsQ8ALAYL3CYJgMhIZbu0z+EAQudGDKSIqBYRSIkdlc4urvTrQzsNa9Cjyf03qY9yrHQFxIGq1e9/ignR/aNCpFB2xljt0nRpn8sVyUit9vyJvs/2xS+7fgHatxczMJCiZhUKpJTSvkQZqeKoQKpl604AAJM/AAN8PFg0gbLsjAERWPhbieygwwPAqM3hc/1+Xcw9pJS7sEuSBLkg1FFbBaV90YEU+0gRUTwRSIkdld4uTlD1NvHbKjOQoswIZ6QC4kBV8tsmMSFBIKUrFud7Jqcrb0r7dundkCFj3vp5sf2kchQDKTUKpX6V0r5EGanoUftate4cft4GbWZNMkVZdma/+CPYuiUA8UWxmXZrso+U368Lj3QVXdoHAFKow3YB3FnPSBUWMiNFRHVzuwFbQNRFGR3Fod+hQCro5bGR0i4YjARCrlAg5Vi9TkyI6x8FAPoScb5nrfFD0osXar20b59BfMDPN3+uiZH7GEipUVxpX8I+UnI5AJGROrhlF3hDa9IODvHaFOHSPl+oV3LLlgiI6jYUGnZpctn6fFGBlCk2kELo5tBW2Y1gUJQBZlplqH9qfB8pWc58W4hIvUQgJU5GjYXF4ncooLIGfAgEtHm1n9Qj+oJjtc8Jhxsw/7FFTEiQkTIWi6qXQg8QNIjjsBa30eiMVLlZHLy/3vI1/Ae3EzMwkKJmFVfa5/FEThrdbkAPPxyyuExfbgHaF7UPj9zHQKppwqP2+ULlIY5CVFvE18Rh2KPJZRtf2hcdSOltop+BBV7oEMhKeV+i0r5AQLsjKBJR6oJBwOuVYfWLqz3GwtAgTKHf1tBVII7cR+kUPkZKQbj8LvQOjTOBDh2Ali1rza8vFhmpIjcQ0IuDnRYDqeg+UpXm0DS/C2utoS8kAylqVnGlfbIcyQS43VFDSAIIOGwoLShlINVM3G4Aei8sPrHADfYi1FhED1CHfp8ml21MRiqutE9ni/xdAFdWgpdEo/YB7CdFRBEej9hHKSc1lkJxHFUCKVsowNLiPpzUI3yMNIp+1kfsDj0++uiE80tFRQCAIg/gDwVSWi/tUwIpAPgGoQCKgRQ1q7jSPiCq86I7sjHWGACrrRhF5qKYQCo6g0WpcbsBGKthDe3ITPYiuAtEIFWo2wefL6fvG5dQTB8pE2A3RoInU6ijNiBuaJntjJRyQ3SA/aSIKELcjDdydcVSJAKpgkJROmX1a/sWFqQOyjHSYFXuIRV6onXrxC8IBVKFHsCvE6/RYkYqurSvwhKZ/qF7lfhjz56cTRczkFKjUCBl9wFGgwiawjd4c8fejLfYUoxiS3FMIBU9P6XG4wFgcoYDKb3NDrdVXD5x6A5E5tEQn6/u0j672YHq8E15a7KekQJ4Lykiqi06kHIZAHuBOEENB1I+aPYWFqQe4X7WttCIfYHQATS6nCKakpFyA36ddkv7ogOpSjPQsagjAGDB/hWQlZr9HB25j4GUGhUVIRga4KBEtxdAbEZKCaQqLECRpQhFlqiMVGhYdB4sGsftBmCuCgdSsFrhDQVShaFASmvLtr5R+2ymqJvyZjkjpdwMnUOgE1G86ECq2ihuKA4AJmWwCR8AkzZvYUHqoQRSBqvYFov8oZtD2e2JXxA6sBV6AJ+k3dI+lyu2tO+4g47DwYUHwxv0obqtuNiRq+V9DKTUSK+HxyZyny3isiDRpX3lFqDIXIRCc2E4kHIY94Tno9S5Q/3SogMpn13cS6lQqozMoyHRfaRqZaSM9qwHUtGj9gEcAp2IaosJpKLvhxfKBNh80Oy9AEk9lGOk0SYOUOFAqqGMlAfwSdos7fP7xXKJzki1KGiBIZ2HAAD+KtaLGRlIUXPyFophp1tKIpBKlJFSSvsMOgM8oQERii37wvNR6hIFUgGbCKSKQml3rZX2RY/aFz/8eXxGiqV9RKRGbjdg05UDiMusW63heSwmbQ4YROoRzkgViKC+0K/cm6aOjFRUaZ9XoxkppetTuI+UGSgtKMWQTiKQ+q0gdDBnIEXNyVskzhpLZbHhJeojVWEWGSkA8BWIlFShiYFUU8T3kYLViqBdnLkXhYINrS3b+kbts5uyn5GKD6SYkSKieG43YNPvBxDKSIVK+6IDKathX672Z6ccoRwj9aFAyuEN9dOoKyMVKu2zBICAT1w411pGqkYMYBhT2tfC2gIndz4ZAPCDQVRSMZCiZhUIBVIlQXEWmWjUvvJQHykA8FtFKaAjNDiF1k72MyVRRkoOncE7QsGG1pZtdB+p+NI+m1E9faTiAyn2kSIihdsN2AziQmKNSYJRH9px6fXwGMTJrNWwX3P7b1IX5aK3ziKu9Nl8oUCqgT5SAGBwi+1Xa4GUywXoEIAjdDFaKe3rUNQB3Uq7YUtRaJhpBlLUnILFIkAq8ceOwpeotA8AgjZx1c0RKm3gwaJxRCAVO9gEHKHOoEFxWSWfSvviM1JqKO1jRoqI4omMVDkAwGPWxzynPLYZWdpH6RUJpMSVPps3FCTUlZHS6+EpEAdZfY3IqGqttK+mJjKiNCAGSmthFQNMDOk0BH8qsSQDKWpOcom4B0axX9QhKJkAlytu1L5QaZ8SSNk1OiBCpiTKSOlCNcyOgDsyj4bUV9qnxlH72EeKiOJF95HyWIwxz3nNog+xVV+uuf03qYtyjNSZxTG1wBO68WRdGSkgPLiY3qXd0j6lksqjB7wGkZECgJM7n4ytRaEZt27NyZugMpBSKV1pKJDyib1+naP2hUr7YBeX622hqJ914KkLBkNXguICKX1hMQDA4RcrQWsH4uiMVK1R+7KckQoGIwETM1JEVBcRSIljoy8ukPKZxWOrrkJz+29Sl/Ax0iwOUAWegHhcTyDls4sL4cYasf1qMZBSBpqosohSRyUjdXKnk/FXIRAExMLbsydLrWw8BlIqpSttCQAoCX0r6xu1DwD0ofIzm6zNfjyZEN4BxgVShmIR1NpD+XatLVufV4oZNlhNg01EB0vsI0VEdRGBVOiGpgXmmOf8FiWQqtTc/pvUJXyMNIoDlMUdiorqKu0DEAgNaGWoEcGG1kr7XK5IIFVuEhmn0gJxXtXG3gbd2x6OncppRw7elJeBlEoZW7UGABR7xDeqrj5SSmmfvlD8tgVqwvNRapRlJhmrUKBcEbJaYSoRV04cvth1oRneAAwQV81qlfZlebAJpaxPrwcsovqBGSkiqsXtBmxSXYGUeGzVVbJag9Iq+oIsAJjcoaionoxUsFA8Z6rR5n2kojNSlWZAgoQSS0n4+SGdh8SW9+UYBlIqZWrZBgBQ4pYBvTfhqH0V5khpnyFUfmbVaD+eTFCWWYGhPDLRaoWpWGQH7d5AzHxaoXdFPpDaBpuIHmhCihtFloEUESlcrkhpeyA0iq0iWCAeWyXekJfSSzlGyoZqGP2AwR/qI1VPRkouCg2BHiqz0GIgFT30ebGlGHpdZEAYBlKUFuaWZQCAUhcAY3WDo/aZi0TWxKbRfjyZoCxje2gIeQBAQQEspa0AAIVesUPU2rLVhz6QRw8E9BIKjAXh57I92ET8iH0AM1JEVJvbjXCJshx17ygACFrFPs0qVWtu/03qohwjZUM1bNElevUEUlKoosgUSpdqrbQvOiMVPWKfYlDHQfgzFEg5N/yW6eY1GQMplTK2UjJSAEzOSEbKJccONhEq7TMXiw3TqtF+PJmgLDO7QXzh/WYjoNOhoESUWdq9AODX3LI1hCLIaqMInHRSZLcQm5GqzlpGKupWG+wjRUS1uN2ATRal7bItNpBSbsprBQMpSi/lGBkwOEPnDACMRsBkqvM1umJR5lZQIzZOrWWkovtIKfeQilZSUIJgh4MBAPvWrsx085qMgZRalYgvVokLgLEq/OWUaqrD/VmqCqRwGVZBscia2HziG8iDReqUZWbTiVRHMNQpx9ayLQBALwNW0x7N9ZEyuMVVsPgR+4C4PlJSFTNSRKRKIpAKdYCyxe7HpFAgZUMNj42UVsoxMqivhk05XtbTPwoA9KHb3VhrxMmF1gKp+NI+ZaCJaK169AUABDZvymjbmgMDKbUKBVKWAFBg3Bf+chqrywEAPh1gtBdBCnUcsZWIDJbVG4SEIA8WjaAESAWhOvtgqM7e7ChBINQ/p9C4U3PL1uARHyi+fxQAFBgL4FICKV3mA6lKcRErJpBiHykiiicCKbEv09UKpMROwyq7Nbf/JnUJZ6R01ZGMVD1lfQDC/bAL3H5ACmq7tM9cu7QPALoffQoAwL5rf0bb1hwYSKmVwwF/aO2UGHaEv5xmVzmAUFlfQXF4dlupCKR0AArg4sGiEcKDTUjiDF0uEHX1kk4HZ2gQKIdxt+aWrdErPlCijJRO0iEQ7qjtzOpgEwqW9hFRPLcbsAXFDkpnd8Q8pzy2ym6O2kdppRwjfTpnpI9UAxkpUwtRUVTkBmCs1mRGqr7SPgDodewYAEDrygA27fw9o+1rKgZSaiVJqLKKu7GX6HeHMwEmV9SIfeai8OyFxW0QGhsGdnBkosZQlpk1VGePqA7L1RYxwkyhXnuBlMkrzizihz5XyEoghWqW9hGRKrndgC0gdlAGR1HMcwabEkh5NLf/JnVRjpF+KSoj1VAgFcpIFXoAmJwIBABZTl8bM83lii3tSxRI2dt1hNskQpLvv3svk81rMgZSKlZtEzVVpbrd4ascFnc5gFBGyhI5WBRZS1Ad6sto1+/jwaIRlGVmCSrlIZF0fE0okHIY9mmuj5SSkUpU2gcgqqO2Okbtiy7t09LBhogaTwRSIgUQH0gZQ4+tQS+PjZRW4YyUlHxpn1RcDAAo8gAwh+6FpqGsVEOj9gEQyYM2okuL64+1mWxekzGQUrFqh8gElEiRPlJWbzmA2KHPAXEC7FQCKZP2+vFkgscDQO9FQWgPFl1n7yoQQW2hTntBqslX92ATAKICKZeqRu2TZbBMh4gAKIGU2HcbHcUxzxntoUAq4NPc/pvURTlGeuFMerAJFInts8gNwKTtQKqujBQAFHc/AgAwoWRwhlrWPBhIqZhbCaSwPzIQgjdU2meJLe3TSTrUmMXqtJt28WDRCO5QfbI1VNest0XSIB6riFIdugOaW7ZmX/2lfZIt0lFbDRmp6FvEsJ8UEQHi1iA2vxjR1lwUOyqYqVBc6bYGgnC5AxlvG+UPrxeA3osg/ElnpJQrhUppH6Cte0klM2ofABg7dRV/5NhNeQ3ZbgDVzeMQZ4ylOIBqr7gCb/OXA4i9h5TCbdEDCMJu2KO5k/1McIfu2aUEUtEZKa9VjDbhkMqxV2OlfSZ/KJAyAXZj7UBKZxXTrLI74xmpRKP26fUimKqpEeV9rVpltk1EpD4BlxeGUKlvfCBlDgVSNi9Q468GUAiidPB4ABjFFb5kB5tQMlKFHkAyVUCGtjJS0feRqmvUPgDA+PFA797AiSdmsHVNx0BKxbxF4stXIldiv0dc6ShGOYDapX0A4LGYAPhgN+yFk4FUytxuAOaqcCAVnfrw2cQIfoVSBbZpbNmaGyjtM9jFSYc16IXXIwOQMta2RBkpQFzgUwIpIiKpJpKethTGnqiFS/t8QLW/CgykKF28XoSzSkU+HYBgwxmpUCClA2A37EEVtBVIJVvah2HDxE+OYWmfigWKxM6+JCju3+N2R9KjFebYwSYAwBcqP7Mb9jMj1QieUFo9USDlt4u/HRocEdESECcgdZX26UNDBxsQRNCT2XqDugIpjtxHRNGUQMqrA2y24tjnlPJkH+AOcKdB6SPOI8S2WBwI3YSxoYyUxQK/XlygLDbtA6Ct0j5PtR92iGVSWV9GKkcxkFKxYLEIlEoD4v49bndsRiq+tM8fGqbarsF+PJkQX9oXHUgF7eJAXAin5patJbq0L0EgpVzNBSAuLWVQQ4EU+0gREQDo3WJnUGMEbKa4DIAyYI4PcMsMpCh9okv7ivxitN8GM1KSBFfoQniJfi8AbWWkJGdV+G+31QibsYHlkWMYSKmYXCrqukv8rnBGqr7SvoBNHCzsUoXmTvYzob5ASnaIM3dHsFpzw59bAiI4qqu0z2otgi+0p9C5sxNIFcZV4jAjRUTRdG6xs6g2ofaJWlQg5WEgRWkUXdpXqARSDWWkALhtoh92kWE/AG0FUoYaUdbnMgAORwtIUua6B2QCAykVk0pEh9kSv7t2aZ+ldmmfrARSOgZSjVFfICU5QqPqBGs0t2wLgiI4qqu0z2aywRXqTSm51JGRir6XFBGRwSdOQBPux6ICKS8DKUqj6NI+hzd0it1QRgqAVwmkpHIA2irtM9bUfzPeXMdASsV0LcQGV+r1JFXap5xt2jVYfpYJYgeYeLAJXaFY1o6gW3PLNhxI1ZGRspvsqAmVeus9mQukZDnxqH0AS/uIKJY+KPqWVJuAAmNB7JNRfaT8ukoEg5luHeWL6IyUI9lR+wD4Qv2wlXM8LWWkTO6ogSY01j8KYCClasaWbQAAJV5fUqV9ulDWxC5Xa+5kPxPqy0jpiooBAIUBj+aWrTUoopG6SvtsRltWAimPJ3Iw4WATRFQfU/AAAMBl0kEnxZ3ahPblBhkwGlmxQekT3UfKmuwNeQEEHEo/bFGGoZWMVDAImL1RQ58zI5VZs2bNgiRJMT89evQIP+92uzF16lS0aNECdrsdY8eOxa5du7LY4uZlUgIpTwAet9zgqH16h3hsl7VXfpYJ9QVShtB9SRx+r+b6SCmBVF2lfdnKSCllfXr4YV/8AXD66UD37sAvv7C0j4jCAgHAIotAymNOcFeXqH25jaPakmL9eqBrV+CZZ5rtLaNL+2yeUOozidK+YOhqYWFoVEmtZKSiz1tZ2pclhx9+OHbs2BH++eabb8LPXX/99fjwww/x1ltv4csvv8T27dtx9tlnZ7G1zcvcsgxA6CqauwqeCjcKII4AiUr7jIXFAAC7BsvPMqG+QMpUIr78Dr9fc8vWKtc/2ITNFMlIGbyZC6RcP6/H3ZiBbWgP/VlnAB9+CGzYAPz3v8xIEVGYxwNY9eJkzWNJEEgZjQiEhpe26ss1tw+nRpo3D/jjD+DNN5vtLaNL+yzeUCCVREZKLlL6YYsgTCuBVPw9pEoLSht4Re5R/Q15DQYDysrKak2vqKjAc889h1dffRVDhgwBALzwwgvo2bMnvv32Wxx//PGZbmqzsxW1hFsPWAKAxXUAgf1iePMgxBCSFoMlZn5TsdhA7UEPvF5xlU6vz3Src1d995Eyl7QEABR6A9DaMdgGsdNPpo9URgKpxYuB2bPR4csvMUOZ1qoVcNhhwJdfAqtWwR76erOPFBG53YBNJ07WfBZjwnm8ZiMKarwMpChi61bxu7y82d4yurSvwB2KhpLISEnh7gPidiRaKe2LDqQqLNrsI6X6QGr9+vVo164dLBYL+vfvj3vuuQcdOnTADz/8AJ/Ph6FDh4bn7dGjBzp06IBly5bVG0h5PB54ouqzKkM92n0+H3wq2nrNOgsOFABtnUCBZy98e0TgVGUGCguK4Y+7ZGG0i+HS7QHxGZxOX3Qs0CDls6tpGWSSy6UHiiKDTfhNJsihZaF3iGXr8AXhlWV4PH7oVJ/PbZjX7YENoftIGQGTZKq1/s2SGdWhcxOjrzrt24dhwgRI27ZB1ukwLzgS88ouwcMbRkL66ScYBg6EvGoVCk4OANCjsjIIny+Q1vZoSb5/x/NRPqzzqirAJoWO4wXmhJ/VbzECNV5YdRWoqvJp5kQ1kXxY581Bv3kzdADkAwfgb6Zl5fUawqV9plAg5TObG46MQn3cHT4R5bvdfvh8ctL/V63rvKIiNiNVbCpWXRvrkmw7VR1I9evXDy+++CIOPfRQ7NixA7Nnz8bAgQOxevVq7Ny5EyaTCcXFxTGvadOmDXbu3Fnv+95zzz2YPXt2renz58+HNZXII808QQ+OsIhAyujainUrRKeRcgtg8Bswb968mPmDW/7CYQDsPvHl/fDDBXA4Ut9gFyxY0OS256Lt208EWkUyUl//8AMq94shdfdVbMGRAPQyYEUNPvjgc5hMuT/0k1zlxpmhv50m4KtFX8Egxe4WNrk24eBwH6mqWttdczKVl2PUtm2QJQkPXvkmbnhiLLpYyzFv4ULo3W6cKkmQdu7E3t++AnAyNm7ciXnzVqStPVqVr9/xfKbldb5zpxU2SRwfnQgm3Ef110lwALDqqrBo0RJs3lyR4VZmnpbXeXM46ZdfUALAt2cPPmmm45rHczpgckIfAIyhi3wLli6Fb/Xqel9n3X8A3QE4fGKEiqVLv4PLtafWfFIgALmeUiO1rfM//iiK6SPl/G0T5m1P3zlEc6qpSa4CR9WB1KhRo8J/H3XUUejXrx86duyIN998EwUFBfW8sn4zZszA9OnTw48rKyvRvn17DB8+HIXxd/7MIlmW8V3oYxbr9uGQ1gcBEIFUuxbtMHr06Jj59xUagNsfgd0fBCBj4MBhaNcu+f/n8/mwYMECDBs2DEZj4vIILbvzTj1gcqIgFEidOHy4GNwAwN7qPQjiWugAOFCOwYNHIi6Gz0kVa/8EIMpFgxYTTj/19FrzbDywEd/dcz0AwBL01NrumpO0cKH4o2tXlPU/A3gCOPjgwsj/7NoV2LABQ1t58AAAu70sre3Rmnz/juejfFjnv/0GfCPdBQAwtShJuE+oLHQAe52wSlXo2/dEnHBC8lf7c00+rPPmYLjiCgCAsboao0eORFPLTPx+IBiUAGM1bFHXsIeddRZgNtf72l1/rQWefweFoSxI797HYfTo2G1Ud+ut0D3yCPxffQUcfXTMc2pd50uXStiEFwGIQdJGnTgcJ3Y4MbuNSpJSrdYQVQdS8YqLi3HIIYdgw4YNGDZsGLxeL8rLy2OyUrt27UrYpyqa2WyGOcFGbTQaVbUBAkCVVQ8gAHtwN/RVIsirMIuhz+PbWtzqYACAPdTZMRBwoDEfR43LIRPi+0gZi4qgLMASeymqzECRByg07kIgcFCjlq3qVIurXzVGwG52JFzvxdbicB8pU6AmvdvGr78CAKReveAK3QW4sFAHozF0gDv6aGDDBrTbsxrASNTURD1HScvX73g+0/I69/sBW2jQHNlmS/g5g6Eb1lslJ/x+gzb23w3Q8jpvMo8HCFUvSbIMo9sNFBU18KL6hSvBTNXiPAwA9HoYbTZAkup9bUErcd5a5AkAkAEk2EY//BBwu2F89VXg2GMTvo/a1rnXG1va16awjaraV59k25lTZyBOpxMbN25E27Zt0bdvXxiNRixatCj8/Nq1a7F161b0798/i61sXk6bCQBQKO+GrkqkR8sttYc+BwBzaIhuuxeA+YA6O9TOmQM88ki2W5GQ2w0Y9FUIV+xFDzahN6MqFHs7jLs1MwS6v6L+e0gBsYNNFEiVCKSzS9KqVeJ3r17h4c+Ve0j9uONHvG/ZAgBo8ZeYj6P2EZHbDdgQKsOpqzzfKso7rODtQVRlwwbgssuATZsy+3//+iv28YEDTX7L8HmByQlb9D2kGgiiAKCgVNzuptADwOiqPWpfICCWFQB88kmT25opNTVAUej+pxy1LwtuuOEGjBkzBh07dsT27dsxc+ZM6PV6nHfeeSgqKsKll16K6dOno7S0FIWFhZg2bRr69++viRH7FE67GYALRfJe6CrLASQe+hwApNAZpzEImCx74XZ3yGBLk7BvH3DDDeLv888XI7GpiNsNFOiiUrlRB2RJkuC06IDKIByG3Zo5EPvLQ/eQqieQshqt4UDKqquE1ws0obK2ftGB1Erxp1Jte/fXd8MTWIEzABRtZiBFRILbDdhkMWhOnUNNW8XIaTbwhvWqcu21YhjyvXuB997L3P9VRuxTNMPIfV4leDJGZaSSGPocACyh+4YWeQCYquDzxV0Q2Lw58g/WrhXDtnfp0tQmp53LBbSTDgCyGLVPi4GUqjNS27Ztw3nnnYdDDz0U48aNQ4sWLfDtt9+iVegE/KGHHsJpp52GsWPH4qSTTkJZWRneeeedLLe6ebkcYqS+EhwIB1IVFlHaV0vUEJt24071HSz++CPyt3LCrCIejyj7AABZpwNMppjnXRbRwbNQv1d9y7aRApX134wXAHSSLjyksBJIpYXHA6xZI/5OkJFat28dVoljDaxb1sAEDwMpIhKBVFDslHW2xPsxKRRIWcH7LKrG+vUiiAKADz7IbFYqDYGUkpGSzFGBVBJDnwOArliMDFwUup9lrYzUunWxj3MkK1VTAxRK5QAAv60AJr2p/hfkIFUHUq+//jq2b98Oj8eDbdu24fXXX0fXrl3Dz1ssFjzxxBPYv38/qqur8c477zTYPyrXeB3iqkQpDsSW9iXISMFohMcgUsh20y71HSyid5IqDKTcbsAqicBCthbUSse7CkQw4dDtV9+ybaRgVcOlfQAQsIidn1WqSl9Z45o1orNDcTHQoQOUfp4OBxCUg9iwfwP+LAKqbUZIfj96Yg3vI0VEoUBK7Jh0dkfCefShzICVN6xXj8cfj/wty8ATT2Tuf8cHUs1d2qf0l0oyI6WUXlgCgMm0r3YgtXZt7OMcCqSKQn2k9KFgUWtUHUgR4C0WX8KSYAWM1eUA6u4jBQDu0F3d7cY96jtYqDyQcnm9sAZDe78EdfYea6i/mu6AZvpIBSvrvxlveL4CkRm1StXpy0gp28RRRwGSFJOR2lG1Ay6/C5CA9QeLK3y9sArV1UAw90ehJ6ImEIGU2DEZHImPjXqbOFG1yh64XBlrGtWlqgp44QXx9zXXiN/PPZe5u6yns7TPlHpGClEjRhcadtW+7ZQSSJ12mvj9+edQ30lebTU1QKEs1qm+WHtlfQADKdULFIsvV0mwCiZXOYDIqH2JeEJZE7say89UHEgFg4AP1eER+6QEgZTXKkabcOjK1bdsG0l2ig7aDWWk5FCnKKvkTC2Q2r8fOPVU4H//a3jeqP5RAGICqfX714dn+6WtyBT2gpg/yVs95JcnngBGjlRXJ7KNG4GTTxYlPJlWXg6cfTbwzDOZ/9+Udm43YAvdiL6uQMpgDwVSAT9qXLyJd9bNnSt28j16AA8+KG5tUV4O/Pe/mfn/SiClDHnenBkpY9xgE8nQ61FtFm0pNOypOyN1zjlAu3ai89FXXzW1yWnncfpglcWCMZW0zHJr0oOBlMoFQ4FUacAJi7uB0j4A3gKRNbEb9qnvZD86kFqzBulLbaTOGxoyPhJI1b6K5LOJYKIQFepbto0UdDbcRwoA5FBgaYUrtWzcm2+KGvh//rPheZVAKnR/jJhAal8kkFreQix8JZBieV8Cd94JfPaZuGqpFjNnAl98AUybhtpnCWl2773Au+8C99yT2f9LGSECKbFNGQuLE85jCgVYVh9Q6VbRBYZ8FAwCjz0m/p42DdDrxW8AePRRUeaXbkogdcgh4ndzZaSkIGRjTeoZKQA1BaKiqEi/u3ZGSukjdeihgHKP1Rwo7wuWRwbwKihtncWWpA8DKZWTS0RNaUnABau3HED9pX3+UAmWXY1Zk+hAyueLDCygAm43AHNVOJBKVNoXCN2HxCFVqm/ZNpLkbHjUPgCQlHuwyDWpxb/rQwHQunW1h5uNJst1ZqQKC4EN+zeEZ13WQtTlHI1VAGRVJV1UYffu8P1RMj6kcF127BBBNSBOYD78MHP/e9euyEnbrl2ZOUmjxlu0qHbZVQPcbsDmF1kmU2Hi8iG9kpHyAZUe7jSyav58cUwoLAQmTBDTJk0SQcdvv6X/ApAsR7axo44Sv5srI2UQx6eU+0gBcIW6DxTp4vpIOZ2R42dzBVLffw8sX9741ydJrhCBVLURKHaoa6Tm5sJASuV0pS0AAEV+Nxz+/QBEIFVXaV8wlDWx6VSWNQkGgS3iHkDo2FH8VlF5nzs0Uk44kEowvnfAIa4sOeRqzfSRkquTG2wiPOKV7E7ts6+PZJKweHHd823fLobH1+uBww8HUHdp36+tAFmvRwvsQztsZyAVL/p7pZZA6plnxMUTZQCXRx/N3P++995I/afbra5yR4r188/A0KHiJ4Ub1nlqArAERIBsLkzcoV0KZQZsPsDJQCq7lO//JZdEAo2iIhFMRT+fLgcOREoZjjxS/G6uUftMYttqTEbKbRPdBwqlA7GBlJKNatUKKCkR3w+DITIMeqq8XmDIEPGT5tp4JZCqNAMtClqk9X9lCwMplVMCKR2ANsEdAMTw53WV9sl28aW1SyoLpLZvF19evV70mQHUHUglyEjJoR1+oexU17JtAl11aLCJBkr79DYxEpZV9qSWkdoQySTVe5VR2RYOPRSwiKxq9Kh90RkpjxFwdRX3SOuFVTwvjqe2QMrrBZ5+Wvx9//1iH/DFF8Avv6T/f2/bBjz1VOy0XbvS/3+pcX77TfyOHhY7CcroowBgKarjZE0pT/YBTh93Glmzfr3IpEgSMHVq7HNXXy1+f/hhevddSjaqVSugbVvxdzNkpEQXAbEtFgdCt2lNISPls4ljX5HuQGxpn9I/SilDLCoCTjhB/N2YrNTOneJKZU2NqGBII71TdEmpNAMtrAykKAustmI4QzdD1UFccauvtA+he2jYJZWd7Cs7xQ4dgD59xN8qCqSUK0n1BVLKqDqOoEtdy7YJpJrkSvvCgVTQm3wgFQiIAQYUixbVXVYVV9YHRDJSNnswHEi1tIrOqvu6HwwAOBor2UcqntoCqbfeEsFLu3bi5ptnny2mK+V26XT33eLLPXAg0LmzmMZASr127Ij8nUJWQgmkggBshUkEUl4NBFIrVgBlZcDzz2e7JalRhjw/9VSgW7fY53r0AIYPT/9Q6Eog1aGDyPAAzZeRMoYCKX8jAqnQ7W6K5IrEGalDD41Ma0p5n1L6DYhKkDTSOcUV0QpmpChb7CY7DsRVmVWY685I6RziZN8uq+zu7coJXefOkZPlVatU018hmYyUrlAs88KAdu5DonMlN2pfeMSroDf50r5t28QlOqNR/GzdWveJfVwgFQhEKg6qddvh8rugl/Tod1A/8dadxA6ZGakE4gOpbH/HlBPiK68U24HSqfy//03vQXzTJuD//k/8feedQJvQ3ZzTfAWWmiA6kFq4MJKhaoAc1dfTVtd+LCqQqvFrYKfx5pviosC772a7JcmrrIwMea7sB+IpQ6H/3/+lrww3OpAqLhZ/N1tGSrS5yKcXE1Mo7VO6DxSiKjaQUjJSiQKpxgyDnsFASl8dKe0rLeDw55QFdpMdByyRxzUGQG+ywqg3JpxfHzrZt8s16jrZjw6kDj9cDDm6d2/sgTOLRCBV/2AT+iJx5coR8GS3j9THHwOvvNIsb6V3JVfaZ1RGvAr64XUneeMmpX9Uly5APxEA1VneFxdIRR8/d/lENqpzSWe0L2wv3rqDWD/1BlKyLA7G//kPUFGRXJtznccTO4iL05n2A2W9li8HvvsOMJmAKVPEtBNPFCMzulzivjHpcscdol/WsGHASSdFAqlkM1I//ihKETM9wmA+275d/NaHTkKjb9haj2DoHovVRsBmquPENSaQqmpKK9Xhhx/Eb5UcQ5MSPeT5sGGJ5xk1SgyFXlGRvqHQEwVSzdZHSgT1Dn/o9DqFjFTQISo/imRn4tK+6EDqqKMaPwx69Dazf39qr02RsYalfZRldpMd+6MyUuUWwG6so6wPgCE09KtdVlnWZPNm8btzZzGQg7JDUEl5XzIZKUMokCr0e7O3bD0ecR+JCy8E3nuvyW+ndydX2mdyFIf/DlQn+eGV/lHdu4v7BwGJB5xwuSKlC3Ej9hmNwOYqEZB1K+2Gtg5Rz/5Lmdh1dcd6uPfX0Vn2s8+AyZPFCXzbtsDEicDXX2c/Q5NOa9aIE//i4kjtfzbL+5TyvfPOA1qHhr6VpMhV5yeeSE+gsm4d8NJL4u877hC/Uw2krrsOuPlm4KOPmr15VAflBO/ii8XvuXOTOsENusTFgnr3Y1GBlDuY4xkpWRaBPhCbXVCzYDASGE+bFhl4Jp5OF8lWPfZYevbXf/4pfqextM/hDX2+FDJSKApVvQRrIrtFWY4cH5U+UoBYfo0t78tgRsrkDpX2WVjaR1kSX9pXbgEKTcV1zm8KdbS1BzzqCqSiM1JAbHmfCiTTR8oYuiu3w+/L3rLdtCmSxp86tcmZFr0nudI+s704/HegKslRfpSMVPfuYnQgQGSk4g+Mq1eLg2yrVqLmH7Ej9m0M9Y/qXtodZXbx/FpjBSosraFHEAUbVyf+/0pJmcMhgrWXXhKZiR49RKZBi31lojN7ynctW4FU9JDn8WU8550HtGyZvqHQZ88W9aGnnRbJhiqBXLKlfcrFn1QGxaio0Hagnm5KIDV+vBhNraYmqT5AQXcokDICJr0p8UxaCqT++COy79+1S+w/1S7RkOd1SfdQ6IkyUjU1Tb63ZXRpX2OGP5dCgVRRwBUJpHbsEJUFer3I1EVrbCAVnZFKcyBl9IqMFzNSlDXxpX0VFqDQVHdGylIcCqT8ftS4VVSSovJAKpmMlLlE3AOh0BeAx52lk6Xo4cS3bxdXzJvA6E6utM9WUAh3qNomWO1K7s2VtnbrBhx/vBiNb+dO4PffY+eLPvkPXaWMHrFPGfo8OpDa6dyJ7S3FNlS0OcE2tG5dZGSoH38Eli0DLr1UHJzXrRPLrUcP7ZX8qSmQUoY8P+EEoG/f2OcsFpEtBJp/qOPVq4HXXhN/3357ZHoqGSlZjly1TfZ+d2vWiOBQKWGk1Cmlfe3aRbKWjz/e4FDoklv0b3GZ9HXPFMoMaCKQUsr6AJHRzWb5bjL++AP4+9/F39FDntcleih0ZcTP5hQdSBUVRbJjTcxKRZf2WT2hc4QUMlL60MXaIr87UtqnlPV17ixKpKM1dhj0DGakzP49AACnWaqzb3+uYyClcokyUnWO2AfAUixGNbN7Aaevss75MsrnEwMPADkeSIllq5dTCCaam1Iup4x29MwzwJdfNvrtDN7kSvvsJjtqQt3ygs4kM1LRpX0WCzBggHgcf4WxnhH7ogOpbqXd0NYuytV2OHdgd1sxf+mfCbYhZcQnZWSo448X/aV27BC/CwvFQTM+qMt1agmkooc8V06I4115ZWQo9J9/br7/PWuWCITOOQfo3TsyPZVAat8+hM9kkhzwAIsWiZPaZctSai6FuFyRCxtt2wLnnw+Ulort9+OP632pzqsEUoa6Zwrt021ewIMcD6SUsj6Fmsv7PvpIXEj57TdRdXD99cm9buxY8XvlyuZtj88XCdg7dBClhKEReZs64ITXi3BpX4E3lCVMISNlKBEXwgv93khGKlH/KEXUMOi6zz5LvqEZDKSsQRFIea0FkOoq58xxDKRULlEfqZKC4jrnD/eR8gJOn0qutm/dKkoPLJbIyYxy0rx2rTiAZpnHA8Bc/2ATtpLWCBdQVGWps7KS5Tn33MgV/cmTG70Mjd7kbshrM9nCgZRcnUQgFT30uRL0RZf3RasnkLI7gti4X7xP9xaRjNQu5y7sO1jcSLHNrrhAqr6RoRwOkZk67DDxWLlbvBbIcuSkI9uBVPSQ58pw5/Hat2/+odB/+gl4+21xhXn27NjnlNK+ZAIp5UQLEPuoZG4OuzpUYrp3b3JtzRDp9dfR8+WX1V/+pZQbFRSIE0SrFbjsMjGtge1D7ysHAHjMDQdSpiAgB8ub2Ngsi85IAeoccCIQAG69FRgzRly0Ov54EQB26JDc67t0+X/2zjo6iquNw8/sblwJIYZbcZdgLe6lWF2gRo26G4WWfnX30lLqCoVSCsWluBSX4hohgbitzffHnVlJ1qIkMM85OUl2Z2dnd2bufX/3NfH7xIlSNWf2SlKSuBf8/e1jQgXlSTk25A0qVI65FB4pP0VIRZjMJYWUY36UI0p4n1QaIVWFxSZCrEKcmpVCGhcjmpCq5pQI7QuAWkEe3KPK6keoEfIs1URIqYZco0Z2F3p8vAiDsVph794LdmgqvnikQgPDyVU862pvhCrH0cvzxhviezx0yJ5QX0oCTL6F9jl6pHzqhO5Y+lydOFUhtWqV3aiTZbs3woWQ8q9tL33eMKIhsaFCiJusJtKaCKFQ99wu57wUXypD1a0rfl9MQurMGTEp6vWiMuaFFFLFS567Q/VW/fBDxayM/vyz+H3NNXaxrFKa8ueOhkZhoTDmvOEopKpLnpTRiP6ee7hszhyk0lb2qmpU8Rofb58n7rtPeAy8lEL3U6IvigLc5EeB05huqC5zY1mQZbuQihZREtXOI5WeDiNGiLYDIPJ5V6+GevV830e9eiJszdGDVBGoYX3164trCyqsBLpjaF9AkaKESuGRCogSwi7CaLaH9rnqIeWIKqRWrkTnS46XY9gyVKpHSpYhxKrca2EXZ1gfaEKq2hPiF1IitK9WiG9CKteSWbkH5yvF86NATJTVKLzPJyHlH0p2gPhbn59ZVYfmjGPeUWSkPYTtjTdKHwJhNKK3isE+z19HgD7A7aYhfqX0SDmWPldLGXfpIq7P8+ft4unECRHO4+cnhI+CzeFX21763E/vh7/e31b5J7l+KEX4E2zKthcGsFrtq9eeKkOpE7oacnoxoN5HLVoI7696v504UbXeCFclz93Ru7e9FPoLL8D8+SV/ShN+qRqYrgS0KqSysrz3XSm+wu8tT0qW7ULKYqk+uXebNyMpCx/S5s0X+GC8oH7narVJgIYNYcwY8bcHr5SfUs7cFOB+DCMgAKsyHvjL1STsvSycOCEMfj8/GDhQPFZeIZWfb79+y8vWrWKsX7JEeBe//17kuRXP7/GGXm9fhKvIxSDH/CiVCvJIqaF9Oiv4q0KqFB6poNpijAo3ypjMymKMp9A+sJVBlwoKqO3LonRGhnNRjUoUUkVFEIG41/S1IivtfS40mpCq5uh1erIcVtlEaJ9vQqrAWk0mc1dCCmqkkMpR5mmp8AI09Swqsk8CzZuL32PHilhyi0WErJWmlHRenu1Pk3+ox/hlR4+UVFAKIaUeJ4iJ/4orxN9qeJ967lu3dppoVSFlDrfnR6mo4X1FwefYR2vn/SxZIt7bW2Woi9EjVTxEsl49YYwYjVUb+qNWWXMsee4Ox1Lon3wCo0eX/OnY0fcCEWruSOfOJZ+PjLR7x7x5pYqvgHsTUmfOOIun6hLe5xBGW2OEVEKC8+Pq9fHtt249BgEWYayZgzwIKUnCrMylNVpIqYsF7doJoQnlv7/vvlvs759/yrcfs1mE8p08KRb7Nm2Cm24q+/4qw6vuSkhVqEcqlxBHx1ApPFJBtcXcFl4ERlOh2KH62d0JKUmCYcMAqOOLLVVcdFeikMrPh3CrsDP8oy7Oin2gCakaQW6Q3ajPCoRagZHuN3YQUoVyDRFSFZ1MWgZ8KX8eoA8gRw3tK0qrsmOzceyY8CyEhtpX10Gs9kVGCiPyvfd835/SydaoA73ec/yyY44U+XketwWcQxAdKZ4n5SI/CuxV+wqD7RX7VNReUgWGZHZSTIyrIWXeKkNdzB4p9bs0GCpnRdfX4xg50rftb7wRbrlFlCkv/hMeLm7OtWu97+f4cWEI+ftD27Yln5ck30ugq4apKry8FZwovppfXYSUQ982acuW6hNy6ArH0D5HrrjCXgpdzX0sRoBZjEnmoCCXz6tYAoXQCrTW4Ia8qpDq0sXWLqJcHqnCQpFXCMKTXB5OnxbH4u8vPFPt2pVvf1UlpCrSI+WfZy99rtOJ6AAfCakjFhF0AMZUkWeszvnquXaFsnAU6ougVq8VNSw0M7PSmo4XFEC4VeRvB9auUynvUR3QhFQNIDfQbhB6q9qnGo/BZjBRuUmEPuNNSO3aVTUT/NSpIm67qKjEU8Ij5bnYhCRJ5AWKZGY/0wUwlBzD+hy9R3Fx8NZb4u8XXrBPFN7IsxeaCNB5XjVz9EjJhT6s5joeqyOqkFq9WsS+uxFSqkcqL8DeQ0pF9Ujl61KchZRjyfPJkz0f36XgkQKRlwhVJ6Rk2XsoSnECAoS3YePGkj833CC22bTJ+34cV+rdhRH5WrlPNUh69hS/vXmkiofUVAchVVAA69fb/pWSk6v3woGr0D5w9lp+9JHLMNVAZdVbDvYipJTnA8mt1prSI45eV9W4Lo9Has0ae7EitVFtWVFzCdWy4uWlhnqkQlWPVEiI+/ByFxiCQzEpVrnBmOQ8lnraj/JZgtJ8WOBVrxXHHNJyfm535OdDhFnYW4FRXqITajCakKoB5AXZvQWZgXiuxe+wCu9nuQBeE1e4E1KtWokV36ws35K5y8Nff4meMosWCQOtGL6E9gEUKELK33QB+na48/KA8MB06yYmRF+r9+TZS58HSp6FlGOOFEU+DLrujrVDB7H6l5srDF/HKnMOqEIqU18ytE8tgZ4rFxNSH30k/lZLnnvC0SNVYy0qB/Lz7eLV8bus6oITaWlihVOSXF+npaV7d/G7NELKVVifiq+V+1TviCr89+/3fJ1UR4/U+vVgNCLXrUumeh1U5/A+d6F9ILyWISHiOnaRBxJkEeHGsptx276h0pRXyitv79ULg2OhiS5d7KKzPB4px2au5RXa6jyuhhyWlxrmkSoqAvzy7KF9pQjrA0CSyA4UgsnPlOq90ISKKqR8GXfUa6VePbvYraTKfQWZRQRaRfXCkGgX9/VFgiakagCFDjlRWQEQ6Sm0LyAAi07ciAFyJRv7hYWi8aWnKm55efYwmuJCyt9fiClwnyeVlydKKef5EE7mjpwcUT1MxUVYj89CKkiJsTdnlv14yoo7Lw8Iw1XpJ+FzA9Fce8W+IL3nhFi9Tk+hn1I0oshLyKir0ucqOh307y/+/uMPexNBV0JKsnJetpc+V1E9UplWh9C+o0ftYT/u+hY5ohprhYXlX437+2/nRskXgj17xEp9nTrOISBVLaTUFdQGDUSieXlJTBS/t271Hn6irtQXb/7rSGk9Un37ims2K8uzsaoKKbUfTXUQUkr4rNyvHxlq6WRfBOmFwl1oH4jxWPUOugjzDLKI4iE6L4arFKL0kpLzvdYbqZacOiWuLYNBeF4rIrTPUUiV1yOlFv1RPeHlpTI86p48UhUU2ufkkSolOYFinvUzpfnu3Vc+S0B2tvequo6e39pK3lIl5UkVpdtDaMM0IaVxIfE3hHFWsevPhngJ7ZMkjEoceKULqffeEyuFL7zgfht1YI2IsK/6OOKp4IQsi0IK114rEljLuoT47LPOE4QLIyq/yAh6o1chVagkMweZK8cV7hFPHimwu+p9FVLKpJETAEF67ytnRUq+iM7oRUippc/9/V33DFFX+WfMEL8TEuzx2go5OUBYEibspc9VbELKlMJ5anMGJUwvN1cI80GDvH4WAgPt71meVdjly0X52RtvLPs+KgLHsD7HEJCqFlLqCqq7nielpWVL0fsrP99znlLxlXp3+FICXZbtxkajRvZ+Nu7uK8cWDn36iN/VSEhZ+/cnsyYIKXehfSrqd1tMSJnNEGIVqkgf7jnXUxcqDNtgCmumkFIXC9q0EWOY+l1lZZWtl+CxY3ZjHSoutK+iPVJnzrgMyS81WVn2BNz69e2PV3SxCdWOKK1HCshTeqEFmtO895BSiYxEVt/L2zlURXdcXKULKfM5YSvk+kHtUC20T+MCEqgLZdJV8OgQOFLbS2gfYAoWyY1BVLKxrxouCxe630YVUsW9USqehNT339vD1FauFJWFShuGtX69vUR4+/bitwshlWvMI8DscEO4EVJGJcY+yHIBqj558kiB3bvnq5BSykofjvJRSCl5J3qT60RtWT03rkqfO6J6pNRwgmLeKFCEVJTYj1r6XEUN7TtXJCaEHTi8/v77fY9Jr4g8KdULtmdP6a7Nig4ndJNrdsE8Ur7mR3lDrxchq+BZBJw8KYwBg8F1oQkVXzxSmZl2oy0uzvt9deyYMGIDAuwetAstpLKzYcsWAOS+fe0eKV88exWNL9d6UZF9PHAV2gd2IbVundPDhYUQYhXny5uQ0ocoOcRWI3n5FdjktaoovlgQHm4vZlAWr5TqjVJbT6SkYG9gVAYqWkjFxgrPtiz7nvvrCXUfUVHO3qIKLn9eHo9UnhL1Emg85/t4Kkk2YSiVRkhFRYm/K0lImc4JOykrEGoHa1X7NC4gQfpQ5reEd5XILY+hfdgTakOkrMqdM9WbfP9+98mu7vKjVNwJqbNn4eGHxd/jx4vwmq+/hldf9f34iorgzjvFIHzbbTBunH3fxcgzORSaALdhSRYlNCS0qoWUq9LnxVENvhMnfAuFVMKR9sSIHChvmPyFN87gQkh9t/M7Il6LYNqqaVjV68KT4HOsOuhCSGVnA1HCA+eYHwV2j9TZfHHN2cL7vJU8L055hVRODvz+u/i7sNC3Rq8gPLgxMfawxorAm5A6fbp8BpKvVLSQArs48SSk1JX6tm09V8nyJUdKDTGLihL7Uj297jxialhf69b26/pCC6l//hEhtk2aQMOG5NStixweLjx7VdkA/fXXhefX23uq80dAgOvIBRDXgV4vxjcHY7GwEEIswnIN8FLgQB8qhFawCc4roc01iuJCSpLKV3BCXQSdOFHkK8tyuZrfysrC6eS9b2KyVMB4I0kVG96nXjfFIyUquCFvmXOkgPwgMX7F55+yCxwfPPyy+pm8CakqDO0znxceqewAiAqKqpT3qA5oQqoGEGxwuBllibAAz6tushK+ECplVYg33CVWq3NeyKpVrrfzVUgdOeLQhRUhos6fFz1kfvrJXkjguefgl198O8ZXXhEiLzZWVLXzYETlmRzyo/z87GWPi2FRQ0MsVTwJuyt97kh0tD1czTFcwx2KcbMnptg15gZTgBjgDeaSn/2zbZ+RY8zhxdUvMvcvpYKgO8EnSfbwPnDvkapdsvQ5OIT2FWUi+RUym6uxhoaJqoylmbjKWwJ9zhzncBpfC6b8+qswtB1KU5cLWbY3OC7+XcbFCTFgtVbMiq43fE2OLg2+CClfwvrAt9C+4iFm3jxSqpBq29Z+/11oIaVeW+p9ptMhd+0q/q7K8L7ffhPjuLcCOOp3Hhfn3qMcGirmA3DySgkhJQZvfy9CShfiIKTyapiQcgxfdSyoUtaCE4WF9jYUI0bYx8OyhvdZrcgnxRi4wLiHrUlby7af4lSkV91VfhRUbLGJ4lX7SrsPRUi1yVLsq3r1fNqPXBaPlCqkKqnYRGGGuK+zA6B2kOaR0riAOBq5BksYOsnzaZPClF5S5FZeHPjJkzjt3KHxoxPehFSdOvaJYPdu8fuvv4R40ungyy+FqLn3XnjkEfH8xIlOZX1dsmeP3Xv14YdiddlDWE+BxXuhCQBZ+W7DrHlVW+zNXenz4qhGn7e+NxaLbZs9MaK8uTcsAcJL529yTmY9X3CejadFJcRAQyB+R44DcDLaQyd7X4RUVMnS5yA8sgF64R0LiU1hO5058m82PPqo18/gRHk9Ut984/y/L0JKlu3bqWGv5eX4ceHC8/Ozh+ioVPSKrifMZnuRkYrKkQK7kNq713mxxRHFwMxt24Lnlj/HwXMHXW/nS2ifYtRnRYXw6j+vUthcGbtqkpBSx2OH+0z2JUSyolGvcW/FWDxV7HPERZ6UEFIi9MIv1LOQkpSxPdgEGTVNSCUliQUAvd55zCxrwQm17HnduqJwhZozVNaFpeRkdCYzZgnOhMGKY25sgtKi2g4VMV66E1KOxSbKMbEXmUygN9mFVBk8UkVK1EuH88fFA76Opb4IKccQ2irwSBmzVCGlI8ivAooPVVM0IVUDCPFzKGlujfS6vT5MrLqFkld5QkpdeVaNem9CylMVH8fwvuxsuOce8f9jjzmvML/5JoweLQaD0aPtRltxLBYR0mcyie2uvlo8XgFCShchqnKFWfMrz9vnCm+FJlR8zZM6cgSKisjX6zkWWUohZXEWUkuPLMUqW2lTpw2b7txE6yzhzbv34Dt8+e+Xrnc2aJAwCKKiSnwmWXbOkSoe2idJks0rFVBbGA9litIpj0fqxAm7F1Y17nyZ6FNS7AsQFVXyXw3ra93adf+kqsqTOnZM3HNBQc6J3OUlLk4YPrIscnyK47BS/51hL6+sfYX+3/TnTLYLgax6pdPT3ecKKaFNa81HeXbFs0xP/U08npLiesW6ugmpc+fsbQXUfERAVkvJV1UJ9Jwcu4Gmjl/u8FSxzxF3Qsos8p0Cwr2EDykr+8EmyMyvYUJK9Ua1auUceq5+Z6UN7VPzo4YNE/N4eT1Synh2OhwselhxvIKFVFV4pCyWMk4mggKLCKkvT7EJU7B4Tb0CpaiTj9591SPl8fypto+fn/jMlSykTNni/XKVtICLFU1I1QAchVSA7L3JnT5MbBMqV6KQUkPHBgwQBvHRo64NSW8eKXAWUs88Iwzbpk1h2jTn7fR6+OEHIa7S0zGPGIZl8ybxOsefV14Rq67h4aLQhCr2PIT1+Cqk9JHiuw23FFatkPJWaELF18p9ivG3PzIcWQdhAT6EDgSJbQLMztWhFh0WE/LwZsNpH92Gphni+94XaWLSn5O44487KDAVqyjVqBEsWwZLlogCAQ4UFoLFarV7pGqXFI/xYcJ48I8qh5Aqj0fq++/F7/794fLLxd++CCNHY6AChFSeMY/CbYph7MKzB1SdkFLHhObNhTe5IvHUT+rMGdG/Sq/nJ50IV03KSWLUT6PINRa7MKKjxXggy+7FjmKQHvDLBOC13Z9gjFMEWPH7ymi0FW1xElLnzwuj7EKwerX4fK1bO5XCt3mkPHn2KhLH681Xj5Q3IdW7t/i9a5eowAYU5lsJMQsvQkCEFyHl4JHKKqhhQspdef+yeqRUITV8uPhdTo9UzkExp5yIFP+vO7mOQnMFGCBVIaQCA+2LUOXIkyq0imsq3KjYHGUI7TOHFkvd8DVMWvVIeQrhdgzrk6RKLzYh5whbK9/fQ97qRYAmpGoAjt4CX4SUIUKsroSaLWTnVZK1rxpNXbvajZziOR8ZGbbJzieP1Lx58Mkn4u8ZM1wLmpAQmD8fY0IshoOH0Sf2EHHzjj9qOfY33rAby2Bfjc7LK1GMoUj2TUj51YoEIMxSVLXlc1VDpKI8UoqQ2ldLKZ7hg0eKILFNgMX+wa2ylb8P/w3A8ObD4fRpJKMR2d+fu8e8jE7S8dWOr3hk8SMl99evn8ucFrX0OX6FJUqfq6geKX2kMMDK1GasrB4pWYZvvxV/T5hgv7Z98Ug5GgPlDFUpNBfS9tO2LJrzGgD7E/yxWF0Y71UlpCojP0pFDe9z5U1RVuotrVqwLl0Ym1FBUWxP2c4Nc25w/k4MBrvYcZcnpRj1J4OFx8oqW9kWqXhhi4fMHjokPFthYcKQUQ0TWS5/f7Ky4iKsD/Du2atoHK+3kyc9l6/2NbQvPl4ssskybNgAgDHLvkgTGOElD0MZ20OMkFVYBWKyInGXB1iWYhNHj4o53GCwt4sop0fq6HZx3WXFRRIXGkeRpcgW8l0uqkJISVKF5EkVyWIiCjcrOdZl8EhZw4rZeGXxSLkLTyy+YFHJHildvthvQWDpv4eahCakagCh/vZVjUAp0uv2/mFim1AjpOd66flTVhyNJjV8pHh4nzrwxcR4XplRhZTqdr7jjpJGgCMJCXz7v+vYVBeSQsEYGy0GBsefm2+GSZOcXxcWZq/oVSy8r4gc34RUpDCUwizGqhVSpQ3tO3zYc5U2RUjtri1W4cKDfFg5U4RUoMMq4/bk7aTmpRLqH0qfBn1sgk9q0oSn+z3HN2NEHtHiI16SzR0QFftclz5XiQsRxoMurAI8UhkZ3psYOrJ5s7j+g4JERUm11G9pPVJnzpSrFPX25O0czzxO+xQxad6f9CWN3m/ECytf4HjmcfuGVe2Rqsj8KBVPBSeUlfozzeOxylaaRzVn4Y0LCTQEsuDgAh5dXCx3zlvlPiXMLCkMGkQ0IDo4ms0RygVWfIFCrUbXtq0wxvz87PkWFyq8z52QAt8Kd1QUjtebLHuuUulraB+UKINuyrSvogRHRLt6hR0Hj1ROYQ3zSLkqNAFlKzaheqN69xY9HsHukSqjkDp3QNyHwc1aMaCxuPYqJE9KHb/S0soVdofFYl80c9XfsAIq9xkVIRVhVtp+lMEjRXEh5et4WrcusiQhFRWJ78oVjh4pqHQhpS8U32WRJqQ0LjRhDhdhkOTdIyWpOVJGOJdXSUJKMZqeP/MdRzs1Eo+tXOm8EuJLWB+IgSJAiaGNixO5UF5YFX6OHpOg7uMw+u2uYiJ2/Pnuu5LhRZLkNrzPiI8eKSV0JNxkpqiwiqpNOJY+9xbaV6+eWAUzmz3nJailz2uL7yjch9A+vVLxKshib4yshvUNajIIf71/iRDEEc1HAHA88zhZhb5di54KTaiooX3WELHCVqb5NSLCPtGVJrxP9UaNGyfEueqROnHCe6Kyo3FpsZSrh9WmM5sIK4Smyrx/slEkp7NPM33NdJq834Srf72aInNRxSZre6IySp+rdOkiQnuTkkp6EBUDc2u8FYDT/wzgw6cTebv3dwB8sPkDPtz0oX17bwUnlFXb5FDoltCN94a+x37FPs/ducV5W8f8KJULmSeVkiLEniRB374ln79QQgo8h/f5GtoHJfKkTJni5i8wQEhQuOfXOgqpohokpJKTxY9OZ69cqFKW0L7iYX1QrtA+q2xFPnEcgHrtejOgUQUKqchIu8gpzxiWnCzGXIPBKeTVRjGP1IoV9mKovmJEXFNhJsX2KINHSucYnhoQ4HtProAAitTvyV14n3qNxMfzxro3+OjIT+L/SqraF6DkeZkCvdutNRlNSNUAwgPsN2OI3ocLUrl5hZDKrPgDys+33aifZa/kReNSEV985ozzZOmrkDIY4IorxN8ff+y+j4gDe87usf399+G/WXdynYetHXCzGm3yUUgFRNURhyzLFGVVkUvKl9LnKpJkr9zmLryvqMjmUdwVK4zPiGDvA74+RBgpQQ79QVQh1UweTt++sOE7Z89ZVFAU9cJFyMiuVN9mJcfS58ULTaiooX2WoHJ4pCSp9HlSRUXw88/i74kTxW91dTMnx/tqZnHjshyGwaYzm2inrgfUrcvuKcn8NP4nBjUZhIzMnP1z+HXvr3ahl5paOs9baalMIRUcLCqLQUkRoAipeUFiJb1g3wB++AEeGXo1fQpF2OPDix/mr4N/ie09lUCXZbuQCoNW0a24sd2NhHcU4cvZOzZhla327VUh1aaN/bELKaTU8OpOnexhho44CqnKLjuqXutqjqqnhR1fQ/vALqQ2bQKjkcIssZqe5+dDiLKDkCqRP1edUfOjWrYs6eVQxWdqqpgnvOFY9txRSKmhfampSmdZ39mWtI2Ec2JeWLppEMtnCiG16cymivmeK8KrroqLevVcN4pXRIh8PoNnn4WBA8X6zQ8/+P4WRpTQPlVIlcEjpXMMT23WzPWxuiG/jrBP3Aop5T7LqRXCU8ue4tkd74jHCwqcW3lUEIFGET5rCbl4e0iBJqRqBOFBDkLKEOn9BQ5CKrOgEjxSilg6FwTnQmDV2U3QS+kW7Bje56uQAmGc7txpb5rrAbPVzP50IRIGNxkMwJSVU3w7djer0Sadb0LKMQZf7dpdWWxP3k5aXprvpc9VipVAN1vNLD+6HKPqSfrvP7BYkCMiOBMpxGCkD6F9asioKqQcy54v+mA4a9ZA2gZxrDNWNGPpUjGvd4gVoZs7U3eW3KkLHCv2ufNIqULKFCCEVJlypMAupHxdhV24UKzeJSTYQ6eCguwC3Vt4n3pPqBNsOQpObDq9iQ7qInSHDgQaArm+7fUsvWUpT/d+GlCEbq1aovAKVJ5XKjvbvtrpJhRFlmXWnFhTsvCIr7jypiQnQ0oKsk7H7ABhqEdl92PAAGELrn3tSfx334lVtnLd7OvYkbLDs0cqO9smNpNDISivFZIk8cBEkbsZl17El2sdvFvVzSPlKawPRFiYXi++t7KWufYV9VpXQ9HceaSMRnsoki8eqRYtREhSQQFs305htiKk/CHYz/3YDTgJqTyTCwP/yJHS9VsrKBCescouLKIKqeJhfWAfe8xm30K0ipc9V6lTR3hAZLnUnvJFhxbSMFP8/f7cpvz0SWNi/BthtppZe3Ktx9f6RDmFVL4pn80b5gBwNjqIX/f+avuZu38u2UXZtgXcnz7NtHVOMZtFlsDbb9v3lZyTzP60kouUsgxmqfxV+/wiHcJTfQzrs8pW1p5cS160Ili8eKQO+4vjzAkAi16RAZUQ3hdUpCzchVy8PaRAE1I1gggHIRVqKJ1HqjKE1KktywH4T7k3TmadJKuXkgBbViEVFQXt2/v0/ofPH8ZoMRKkD+HDIV/gr/dn5fGVvoURuDGizD4KqdDAcHKU4j7mjMpLVt6Tuo8uM7rS4aNEig4oeRje8qNUihWceHb5swz6bhDjfhmH2Wq2GX9y67YQIAbUyBDvA76/ErsdrOT1qGXPG4e0Ye+G+gQGQvsgYcz+trM5Q4aIBdTC44qQSimNkBL7ceeRig8VBlehoRyhfWBfhfXVcFDD+m6+2Xml0DG8zx1msz3/QF1VL6OQSstL41jmMToql7Hc3rlinxpSufjIYiyy1XYPSpUlpNScyZgYexhOMWZu+YG+X/dl0MzRyGXxhrgSUoo3KqtxXQr8gdS2THk0hmXLRPRS+/YSxrmfwJFB5JnyuOGHuz3nSKk9pAIlCvzhubta8+mnUL9ZZwrCg9EB3/z6LKezTwtjVPWyVDch5VD23IngYPs4W5nhfbJsH/8Hi8Uutx4p9TwYDPacDU9Ikr1639q1FOWIsKQ8P8lrj0VnIVVs/M7IEMWTevXyXRj973+iaudtt1Wuh89Tw2k/P/s150vBieJlz1UcS6CXUmRv2PEnSm0WTqFUjzteCXlSZRRSt8y9hdl/C+/LYuN+rpt9ne1n3K/jmLxwMqaQSAAObslErxctLNXWlY8/LrqxWK0w/IfhdPq8k3MeKkq6q7+YiEKNyrVQBo9UQO0Y+z8+evcfX/I4A74fwI5gJUTPi0dqp05ZuJAgJ1TJQa4EIRViFItmhvA6Fb7v6oQmpGoAkQ5hV2H+pRRSRZkVeiwWq4WFC94FoKBJfbrEi4F9S0ulZOfKlfbwgtIIqVKghvUVnGjDnVc35K7OdwPw/IrnvRtoLsJ6ZBmshhyC1Lx/D0IqLCCMbCWdy5pZeR6pT/5ag4yV5MJjrF/2o3jQW36UikMJ9DPZZ/hws1hB/+vQXyLxXkmQN7dsYxv4a/kQ2hcULlbsAqwymM22sD7/EyI85JYbLTSyiN5evSY0JzxcLEIv/7F0HqmsbM+lz8HukcrXpYJkLbuQKk1oX3q6aBYNcMstzs+pceyehMqpU8JACwiAHj28b++BTWeEEdz+tCie8uDMDjzyiGhtZTZDz/o9iQiI4HzBebYkbal8IeVDWN9bC2cDsD51KV9s/br076EKqa1b7UU6FANzQ5joqxOaPoB77hE24bBhYiH/66/8iN8yE4AD2dsojFJChz0IqaRQGWQJ0lvw0EOwcZNEYLuOADRMyufev+5F3rdPDB7R0XZxBhdOSB0/Lgo66PX2kvyu8FQBsaJIT7e7idWqcO48Uo75Ub6WzXfIkzLninDafD8fQqAc+kjlW4oNGqtXi/yYM2d8zzdS+3V99x1Mn+7ba8qCJyEFpSs44So/SqUMlfvO5Z/j3D5xfGekWIwEoNdD6obqIaTm7JvD7/t/p1G2EI1Sgwb0a9SPfo360TWhKwBrjq3l+78iAaijz+CPP0TNq3fegbfeEvt55x24ZsJ5dqbupMhSxOrjq53ep6gI8BPXfHCRYoeUwSMVVEohtfH0Rt7b+B4A24MVMeTFI7Uq57j9IX9lLK0EIRWqhIj6RXhJSajhaEKqBhDp4JEKL6WQyi6qWI/Ux1s+JviYWK3qdMWN1JfFhPZneIqYpNLThcdDlu1GYiUJKdLasHYtRO55hkBDIBtOb7CV4naLi9XooiLA30ePlH+ozSNlzfLdI/X9ru/pOqNriVUsdyxTJiYA8+Hd4o/SeqQOHODV1f+j0FxoKyH+4eYPOfrPfADymrUAnRC9tXzwSAVF2nPXrPl5NiF1aKHwfjxy9SkRpuPvz4tf1efMGSUkIkUIqT1n97guz12M01mi9Lkkuy59DhAbKgZmq2SCwIzye6R8WYH95RdRCbFzZ2cPBPhWuU81Aho2tN8TZfRIbT6zmZAiaJeueAfTOvDee8IRERMDt0000C5YeAIWHlpY+QUnvJQ+LzIbOWhebvv/4YWPkZJbyr43LVqI4h75+fZqeUrI0+LwTABu6zvAVpgThKaYOBEOba2PlBcDOgsrk5VwE1c5Ug4V+8hsBOYgTCa45hoobCLyoNqe07Hg4AIWzVOK4qgV+1QulJBS86O6dxffkzuqouCEeq3Hx9vvlZMncVnq1KFiX0ZBBolfJjJp/iTyjB7idR0q91nzxCp8gS9CSi1/bhK9A51wjKbwVUg4bjd1aukSanzl7FkxPklSyUITKr4WnHBV9tyRMlTuW3JkCQ0yxd8n5Cb07y8c9hwXXtHtKdvJKCh9Jby9Z/fS7YtuolBMGYVURkEG9y+6H4D+erEQefOVz7Jy4kpWTlzJkpuXAHAy5zj7zorFmGuHZjJypH0fjz0m2gYaDPD7OvtioLqYpWI0YluYDDIq81wZhFRIbYfwVi9Cymgxcuf8O5ERwm1HkAePlCzbro+1lgO2h9OClGOthIITYUZhVAVF+ZD7WIPRhFQNoJbDzRgZGOn9BQ5CKsdYcULqROYJnl3+LC0U+2DG4u7M+0BMaKuTN9gntxUrhFApLBQrjK5KjZYDm5A6KyboN6bEc30TMVg+v9KLV8pFaF9phZTqkTL72G/CYrXw1LKn2Ja8je92fud1e7MZjhYoQiqjEc2U8c3UpJFP70eTJqL4R0EBi1d+AUDshq+5s+HrAMhKaF9Gwya2l4QGeMktAMJq2RNGdxzZxNm8s/jLoVhP9KZvX2jld9j+/no9oaHw8MMQKTcDUxAF5gIOnffSmBM4mSe2CbO4Ln0O4K/3p3aQEgYUllz+HClfPFKOvaOK40ton6OH1pftPbDl1EZ+ng0hFjPnQ+rxyq/NmTBBRMhmZIiJf+0sIXAXHV5k90hVVgl0L6XPv1q2FtkvF3JjIakzBWRwx+wHS/ceej2oTWVVb4pasa/pWbDqeGGii0p1QEiIRDxiNf/PY8q978EjlRwKpLXikUeELXP6NMzaIBYoxiKKuexZ8QsAplbFjB1VSFVSSWG3eMuPUlH7/jl69ioax2s9JkYIO8dwP0ccPFILDi5g85nNfLn9S3rM7MHBcwdd779zZ9HKIi2NmFNCVBf4uR4rnHAI7VObp9pwFFK+hrap240aJX7ffjv8849vr/UVNT/qssvcC2TVI+UttM9V2XNHylC5b9HhRTTKFH8fpxEPPihC4chJgPQWWGUra06s8Xl/ACm5KYz8cSRbk7by5vo37ePlsWOlCqF8YukTpOSmUC+wBfqdQijd80oD2rYV+v7ybrWQsoV9ItcXC6N19CVF3003iWAE/wZ2IbXxlLOQEnaEmIgCC5X7qgyhfZFhkWyLh7RgKFILR7nhtbWvsTdtL7X86+BflMDxCOW7cSWkMjJsRUROheaDMQSODOJckPJ8JYxX4UaxUBsaXa/C912d0IRUDSA00B8sBgAifCkj6SCk9hUtZfrq6U4/L695WSRdlwJZlrnnr3vIM+bRKkOs/H23pQWcFLHqu1J3UXC5UnBi5Ur7hFmvnojhrkB2JtuFVJ8+Ymz498MnCfUP5d/kf5l3YJ77F7sI7SsspHQeKUVImbJ8W3FecWwFSTli1bX4KpYrlq8uwlJbfMbrTT/SQNHC76Ut9On9MBhs3qtmZ80EJg1k86/9+O7eJ7i69q22ctmrQhSFZgxGr/O+mhsZHEaecirX7BN9oeSjg8Diz4MPUqL0OQgdPaCfHlJFUrMveVJJhWI/tfHsgVPD+whNqXyP1IEDwnjX6+GGG0o+70ton6OHVt3+5EnfKm05YJWtjPl8NVceggICOPLabMZdo+ebb4Q2WL0abr0VODwMgK1JW8mMF95Et6F9ZjPMnFn2qlheQvtmrhHXbiPzMDqf+RKsehae+I0/DvxRuvdx9KakpsKZM1iR2BEHTUM6Ex0a6falvRoLIbUySxGvZ8+WNMocKvaR3oqbb4bff1f6gB8WQqplGrw95G3aKWkGr2b+6Wzwq3k+VemRkmW7R6qYkNp9djfrM9fbqw22bCmKjzh69ioaRyElSfYxwVV4n0PFPsfCBHvO7qHrjK7M3T+35GsCAmyCsPUhIaoL/f29H5c7IZWa6vxd+OKRyc+3r+J//bUolGQ0wpgxnku9lxZvYX3gu0fKU1gflDq0T23I3lCZozLDGzJqlBApw4YBx0of3pdvyueqn67ihHKfnso+xalayvyUk+Oz52TFsRXM3C5Ces/O/ILa+WKMX3uyPnv3YvuRk0XERM+bFSHhZoF0yBAYOtE+f+1K3eVUOEcN7ZOsEFCkCKkyeKQigkLpdQc0eQgyPVjo+9L28fKalwEw//kBxgMDOKmah6mpJRtgK9dGYVgwRgOQ1BXd8cGcU82dChZSprwcAhRnV1RsxS6mVzc0IVUDCAyUIF+scsaG+BBr6iCkTlg38MKqF5x+pqycwthfxpYq4fuH3T/w9+G/qV/oT1iBBSsSR2iKX1E8nG+KjMz21pFi41Wr7InFFRzWV2gu5GimUho7vC2//CKK7ezaUIfOxocAeGHVC84lih1x4ZEqjZAK0AfYPFKWHN8MpW92fmP7e9OZTV6/91kL9oDeRIAlis8mRKKXIccfntnzHvvS9vn0njlNxcpiqzQoXChi94sKJdKeuQuAlBCYvEd48SSTb4N9rZAQ8hUhtemQmBhN+4bToAFcdRVumwYPHAik+p4nddYs9hNj8JwTpvaSIjS5/DlSKSmeV+fnKsbcsGHO+TAqpQnta9xYvK9OJwyv0vR/AdJencLd68XK4qTgr+lwV6LtObWTwFdfwc1j4iG5ozj8DMUwciek3n0X7rxTWAulLYNrtXoM7bNYYGeeMOCu6Ticb1/vhLThcQDu+P0+n/uLAc5CSlmp/y80nLwAGNfJsydmfE9R8exYtFJxy2QqaTQ5hPbVsrSiUyeRcjhzJuxHCCn54CEe7fYg/fPEdbA4KIluX3SzG/wXIrTv0CHhVQ0IgJ49bQ9nFGQw8PuBvHH8Dcb8OobzBefFdad69iorvK94WLc6JrgqOOEQ2rf2lBBSn478lD4N+pBjzGHcr+N4aulTolCOI0oERONUcV+VRkgFmSFXOm4fi1URquKLR0bdJjRUTELffSe+1/PnYeTIijNMPVXsU/HFI2WxiLkZFJXjglKG9m1L2kZafhqNlMXV5oMa2mrwPPEENiG19LBvQsoqW7ll7i1sSdpCVFAUTWs1BeCf9G32uduHxZ58Uz53/SnmupD99+J3qBO1EQLs4/kNWL4c28+EoWJuOuGnfHceWlicNtnnLwtm/k3+1/a/GtpnsyOgTB6pkEB/jOYIcgNg8qJ7yCkqmUJgla3cOf9OTFYTYckjydlwHZzsw7lgKPRXzPri17BybaSGiPskPCeRfg0HcF7xSFnT3TTxLSP79q4CwCJBVFz9Ct13dUMTUjUAf3/gj1nw18c0imjidXtHIdXJfBd3d7nb6SfQEMjxzOPsTfNtNTItL42H/34YgHtlEdZ0nEY8Pz2QMWOAk2JCWxSaIsIFsrNhjig1anPJVxAH0g9gxQoFtRjUI56EBPjoI/HcurceI9QQwZ6ze0T/HFeoRrCDm1sIqRyfhJQkSeQpHjZrrveJMqcoh9/3/277Pz0/nWOZ7icCWYYle8QKZKvILkScFYbH4ZBwLJKZCbPvdC8SHVjmLwbR1kfqweme/PGHyD9vnCtio48khFBoEQazZPJtsI8MDrUJqaPJyoRyaDiTJwsD3pVHCpQFciVPanuSdyF1DrGfukHl90jJsuxZuMbEiIO3Wj0Lmi1KI1Z3YVOqkDp/Xik76AJHIeXnZ1/9LU1434IF1JkiavM+2bUhhcOvx5X9KEnwxRcQnydWnp9ds108npWFX/EvKzsbXhdhnxw+TOHzT5NRkOH04/E7TEoSq/N6vQjrLMZvS05ijtoHVh2PjhlMmzbwQPupcK4Z50xJPL74Kd8/vyqk9u4lc94qALbVFQb2wMaehZTqkSpK2EdhgFIOvnh4n0No34C2rW2pT9ddB+MerE8ewejMJk7O+5fAJOHVDu/cg+yibMb9Oo6nlz2NOSpSvKiShJRVtpY8H2pYWq9eohy/wjsb3iGzMBOAv4/8TZcZXYTxV9l5UsULDalCyoNHKjc63LZQNL7VeFZMWMEjPUTZtDfWv8GQ74aQmutwvtRQcgWjfyBecRjbTdJB+8KO+v2pHgRfhIS6Tf364oYLDob588VYcOgQjB1b0itQFsrgkXKZi3r4sCgAEhRUMsdTpZRV+9Q82YZnxRzS4/pGtuf694d2Yf0A2H9+j/O5c8PTy57m9/2/46/3Z9518xh1mQiZXHtybanyPKetmsaRjCP4F9Ylb95rXN5QOVcREfQdFc6AAdh+RnUTc9MOozIOu/FImSwmu82kRFhsPG2/f9TQPlvpc0lyuhd9xWAA/n4PLAbm/PcriV8mlii3/smWT9hwegMGSxg5P31KbKxEXWtvkOBEuJvwPuXaOBEoxsvuCYncNqwT5/zEyvD50xXoRQUWfT8VgB0RUYSFVWxUUnVDE1I1AIMBpCPDYMt9TonUblEmAx0wNPtdPrvyM6ef/o1EEuiiQ4t8ev9nlz/LuYJzXBbRnqQPxWCeW7cFzz0H48djE1L/nFkP/fqJFy1YIH5XsEdq71llIDvblv79hJVzww0iqsKSW4uQnWKl+8mlTzo17bURFWUvW62E95UmRwogT7FcpVzvCbRz9s+hwFxAs8gWtI4Qq8CbTrs3XrZtg4xAsco1qHUXm+GR7nc5FIWy7ewGPtnyicf33Je2j99k8dlbnYzi9tuFx+jPP6F/HfH4/vPXEmEQolIy+yakwgLtHqmeJ2RIbUuQqT533qls4MYj1aIF1DZ1BGDrae9CKksvPnODEC8eqVDVI5XiMkcqoyCDxu835sqfrnQvBPR6+2qupzwp1ZhxtyocHm5vJO1OGBU3Ln0JB3Rk+3a4/np0VpkvOsObtcY5JUUXJzAQPnlECKmUhOVk+IvzHVxcPLz/Ppw7Z4uVN7z7AQMfjiLqDfvPFV9f4f47VMP6mjRxGcb70WIxzsSaehIXIfLsXp4aRO31In/vyx2f+55DERcnci5lGenbrwH4t2EeBp2B3g16e3xp/fD6hOpqg95MshQpHiz2XViSxDWQHAY3DW3l9Nwbb+k4HSJyFpbdKyoQUq8e8+9ZYzP4X1/3Ok9uf0M8l5kpvF4VyJoTawh9JVRU33TERX5UWl4a7216D4Cb4m6iSWQTjmcep9fMXiyNViqOVpWQUhdXXHmkFCG1Vy88Bo3DWlJwrg5+ej/eGfoOv1z9CyF+Iaw8vpLuX3bnTLZyn/bs6VTkwxTgg9HqYNgGm+DbnUreo+qRuuYa8bs0QkoVHyCuz7/+EuPBP//A888DYkFn85nN3P3n3YS/Gk7C2wk8s+wZDp93UxJe5dw5+3jSqZP77RyE1O7U3TT/uDnTjkxz7tm2Uxl727d33+RV9UidPeuTCLQJqRxxnYe1tRcHkiR45qFo2yLakoOrPO5rxrYZIh8KmDV6Fr3rX05di7At1p1a53PBiX+T/+XtDaLxk/H3TwkPCGfG84qocJGvrfY53JqvnAs3HqkD6QcwWoyE+4ejP3AdAMv22+8foxHwyyNU7WUcHOx7FUoHDAZgx63w9WriQhLYn76f7l92Z/Y+MeaczDrJM8ufAcD892v4F9Zn7ly4eWgLKKjlVUidDBMLaVf3SOTKkXrO54sxLfX40VIfqzs2nNpA8CaxeLe24EpvJlWNRxNSNQBJwrbq7JOQcrhqLVm5mEw4/QxtIpLQFx72nnNTaC7k570/A5D50wc0KBDlrduMbYEkwYgR4JcsBrtNpzdj7neF8sbKilgFC6ktJ+35UX2VvHJJgk8/FRE1qX88RKTchFPZp0j8MpEfd//ovAOdzu6VUoRUaUL7APIDxAqOLt+zkJJl+GC1mKiPzZvAviViFdhTntTcuUC8MNq71+9sMzzaj+qAtPw1AJ5c8gwns9w3jZy6aip7lbYNrc2nePstMbBGRMB1bcT3t/FcLyzf/QlZ9Qk64cEadyDEL4RFij303mL4YIEfE28wEhWFON9HxLVRXEhJEgxsK3rXpBvPcC7fvScvLS+N3CCxKt2qVkePx2PzSIW5Du378+CfnMg6wcJDC/nz4J/ud+StKW96un1SKmbMyLJo15WSgufwvoICe9hNcSHli0fq9Gm48krIy2NDizDuGwkk9XCb6qByZceeovdc8Hn+Q1wUTkLq/Hlkpb7vfSPhlzZgkOHL+aB3WNRee3KtrflyCTzkRxUVwabzwti6soX9YMPC4KPH+8G2SQDc+vskCs0uKrq5QvGmRBSK+3dbAiTWTSTU33OIqiRJdK8nFoJOG5QBtVjlPstpEWaWrI9m5MBIp+f8/KCBIq76p/8GwA5zWxYv8uP1Ae/w/djvAfjk8E/IqoFfgZWw8k353PbHbRSYC3hv03v2vBOr1S4EHPpHvbHuDXKNuXSK68TVsVez8faNjLpsFEWWIm5OEm58ed8+4ZGsSKxW+zXti0dKCe3baBGvOba6Dw0bihDVGTNgUPy1bJm0hWZRzTiZdZJRP40i15gr+pU5eFZ8ElI6HWY/MYkGm0TIuunYETHO6vVw441iu9KE9tUvFrbUpo04cMAyZzYfbPqADp91IPHLRGb8O4McYw7Jucm8tu41mn/YnP7f9Of7Xd+7blSthvU1a+a6OISKshhkTU5i5I8jOZ1zmh05O7j9z9vtEQxqqfYOHdzvp3Ztu5Hh5Ts4l3+OTac3EVEAEVbl2IsJlauvhrB0Ie6/WO4+vG/JkSXc99d9AEzr+yKBh26kUyd44nqxOLI7dTeF9ZUFLw9CymQxccf8O8Rn3nMd0qFR/PQT1JfdC6mmUU0J8QshxU9RQHl5LhdAVO9l+7j2dK4j2lc4zuXqgmyIKqTKkB8F4jKUJOBUL5aO/5d+jfqRa8zlmt+u4bHFj3HPgnvE9X+yN2y9hxkzxJrCmNESnOplz5MqLqSU+SclFMiuy9VD6xIZCQHBYnGwKMWHHmQusFjEGt/27eIt8/Nhysop9FHefm2RJqQ0qgmK7e6bkNLrMSod3md/k4u/P04/D48UBs3ak2tFR28PLDu6jFxjLn4F9Ti75XK6hAijSd9KVOcKC4Nh3VpAfm0KLQXsaxvnvINiQmrmvzP5Zsc3lJV1h4QQiNW1tYVMg9BGn30GGMPIemsT3WsPJt+Uz02/38QDCx/AaDE6bwy21eicfCMYjD4LqQJ/MWEbCl1/d+fPw3vvQfOuJ9ieKQwcy/ab4YwwAItX+3Hk9z+MELsLgC4Jdo9UbO9mTE68F072psCSyy2/T3DZXX1Hyg5m75vNwSiwIlFLziDSaDcWAw6K7+9sdBty/+sO754gauezHj+vip/ejycG+DFd0coPnNrOW9v6iQn3lL30eQnDAhjaPwzOi7AvT3lSi48sBkmG5I40iIp3ux14D+1TV0tBDOxuQyK9NeVVjJmzdSPZU3QKi0UsNj/yiIhcbd1a2DErjzcC4MTq4yXrR6iGZViY8IqC75X7CgpEVbCkJKytWjFybD5mPbSLSrQtRLvDoDMw/LIhAByLFI8lr3dYaX77baTsbHbFwPx2/rSdtQVrZC06p0CG/D9S7jNybQvRM8u2el8cD0Lqz4VFmOsvA+Dugc6q77rroE/BG5ATz7Hsg9z71722oiyuyM2Fn3+GWfsSnR7fEQcDvIT1qSQ2EELqbKiSb+MoKnNz8S8QpdFNoa1chkwGdRE92hpzHIBlKW0YNUqc//Wf30Sz0A4UYaYwXBlDKjC8b+rKqRzNsK8a3/XnXeSb8kW7ifR0kY+h5D5tOZDM+xuFWDo562Vuumkkd0+szeiCeTzd7X+kh+k4HgGSLPO/d8dx8+83O/3c/sftPjfQLkFSkhgL9Hr7vaV6pIqXQDebbWJ2WaGS/3lCiXD4B+6+Wzhbnr2rFY9EL6FOcB22p2znhjk3iPA1h/A+S6BvnnV1u4i8KM7mnWXvrx+LJ7p1s/fgS0ryXtHQlUdKIblPR6w6Cf2x47z260PsPrubAH0AN7W7ieUTljPn2jkMbzYcCYlVx1dxy9xbiH87nieWPOE8TvkS1gfIsWIg0GXnkJ5+ivphDTFIBuYcmMPzK4RXzOaRciGklh5ZyuNLHic5N8Xn8L4lR5YgI9PoqHJu69QpkRPk5wfXdRf35saUlS7r6mxL2sbVv16NRbZwReQtzHloCuPHw65dQG4cnGuGjMyhcOV8eBBS72x4RxTSKqgFi97n9dfFYq9NVLgQUjpJR7vYdmQ52lYuwvvU+6FDbAeu7t0NZIkM+YQtZFEN7bN5pMqQH6ViELXFeOGxWG6RlzK545Pi8218R8xrZn+Y/yWPPqJj4kSxbefOMsHp3d0LKcUjlRIKtfISbTVx2rcXPeeC87Oc7SQPnD8PP/0kytzHxopAjc6dxdpgSJtVbN6/nA7K0LqO3mWJcKxRaEKqhjBokLC7XKQguEZZDQnBVcxTUzjXHLPVzPKjy0s+78Cc/SLXybRrLLWjdPSJKWk0XT1eslXvWxqUJAZUFQchtT15O3f+eSe3/nErJzLLVvb5YIYQAj0al4zxHj9ehPnJedEcnb6I/joxgXy05SP6fd2P09nKxFCs4ERmvtJEz1chpax8+hWUFFLTpwsHxyOPwJEQ0VMkvqg/a/9qQHSRqDK1LWm7ywHrv//gwLl9YDASERBJ48jGTuFy01/SEbX2SzD7s+bkalp/0preX/Vm1vZZYoUKeH75CwAU/ncDaaHKd79fEVwZGbbV39f+bKNEokkYDL5XjZOtYbwwAK68OpAcQyQhOzeIEfTLL8UGSunz4jgWnNh0wr2BtvCQ4iU9PNxjKxxwDO0rWf7cbDXz9yFRWRCrnl2pu3hi1mzX6UvePFKKMbMiMpPuHw0kruVxrrhCiOWTJ+2LGzszhYfplzdPkJAAt90m8vc++QQWfCgm/3NhjfjkU4nvv4ecKB9D+xYsEKvJdeqw/cvpZARaIDeWcQN9q4Q0vJkQMGnNhScwaZ2Rdu0MdEhII/eV9wGYMgAK199H2+5duT3zHQAML0+nT8xxfn1WzNQ/7/2ZIrOLcB8PhSY++nMt+OcRbI2lS92OTs9JEnz+fiS6v4Uh+/WOr2nwbgP6fnYVHyyez+69Zg4dgtmzRcRVTIy4v2futQupI9F6sgN9F1Kd48Xqa2qUcsE4CillxTbHH5o3bud6B2qPNoWEwWJB59w5cZ4PzxE5pGfV3ixlEFI5ObBmjbh2nnpKGCtdR23jrXXivDD7R8iqx5GMIzS7cxpf3SJW+o/UvZy7H/CnWTPo/uirmORCONWTc5uGk5/vx9y5Ou68Q8drI5+l6frFbKkvlKJ1xXJ+2P2D08+sHbN4fOnjpT52wGbo5tdpwIirDDRvDvvPuSmBrlROlPV6lubsEI+d7MN338Gbbwqb32SCefNg8o2Nyf1iPjprIAsOLuC++Y86CSlrkG+GqxQixnf/PUMByFqk5LAOGCDmBjVn0lspccccKQdWHV9Fxx+vYGeMiAQYndSIkdJH3JiUTM433/P0dQN4645xBM9dyMSMEwzSv0RtfUOyirJ4a8NbTpULPQmpTZvEVKvXg65WCPnKuBuXVotTLy/Hb9FnALy69lU+2/SVSyFlsVqYsuIFhnw/hLc3vE2nzzqTWSfM+fO5YcF/Sljff8q9onrYizHttivAqscUfoiv5jjvc9b2WfT5ShQVCUm7gjWPfcHuXRJhYSIqslcvbKkDm/2VYghuhJTZauaNtcK7zuJ3uHlsrCjDDqJ/lodj7BDbAasOCoOVFWtXQirVLqTGjQiHNDEWrFKqRpYI7SujRwrs/bTnzoU7bjPw8ZjXabL5d/xl5dysmcLQLi1tqa0gxtMOtVrahJTsxiOVHAYd63S3PXzlKDEm1i6Q+Xv3FrfHlJ8v7skrrhDX3Y03irZp586JSNa4ODD4ydB/Cj1Og16GozRGXy/B5aLUxYQmpGoIs2cLm9pXF6l/LXETr1+cS0YGTj/jxwOHhHHluGpfHJPFxLx9onkr+8cx52cT/qeUAcnBaBo1CnSnxWD394F19vASf39IsDdie3PNh7a/f97poqStF3KKcsjWHxfvmdjG5TYffSQOLf2snpUvTCdgzp8EWCPZcHoDnT/vLMJhipVAzyoQ1rWvQsoYKL5b/yJnN8jGjfDCC2LBtX0HmdghYgX/lWsn0Lu7if9dFwwFtTBTxLbTu0rs1zGsr0tCZySj0b6q1KwZkZHw9jMt4ZsVcOAqsOpZf2o9t8+/nfi347l+9vX8dfhPsOqI3DmVyJ6K0acKKbW8b4MGtO4Rzl9/QYMGMt26+V41TqfkU/2lH8b6D7aJSTktDf73P9txuqJhQ6hVJCbw5XtcCymL1cLC/4T40R8d7q4lkQ1Hj1RRkXM0xuYzm8ksyhArk2ueA+Cdf6dSu46FYcPg448d7CQvHinrtq2ACCEr0J8lfchIImKymDBBGHjnz4t99VQSrZsaTpCaKioiP/AATJ4MCz8Rk//apMZMngy33AJXPy62z9h5gowMYQgcOneoZC7SVvH+jB/PSqtiiJxO5MqREr4wrJmo0LWnljjPDeXj/PefxC3JrxNKHlsSYH7jIFj7NADfMJElDCaIQmZwFxzrC1n1yCzMZMHBBSXfwE0PqZwcWJcqxpeBDYajk0pON61bw2MjxsKvv8HJ3lhkC2tS/+ShjaNpP6s+l933NNfcdYTZs4VjrlkzGPxkZ2TFaNwcZyHQEEiPej18+i66xAuDNDVahNzlHrELqVOb7BX7BndsVfLFUEJI3fhKW06fhsWLxTll941g1XHKT/G6eBFSFgusXw8ffCDak7VuLSK4+vYV184bb8APP5nYVu8O0Tx79/Ww5wb461MAkhu9Te3/5gHw2cEBzJgBR9JPQpfPxfHFvczff1t44401PPecxWaPH1o8iHmHRR7J1dtiScx8hzvqvsPr/d/hxX4vAiIfS12g8UZ2UTYpuSlkZ8PSGeJa35DSmEWLxLz1yqtuSqArCzvG6FoUySbIiaNuSBNuvFH0ItqxQ3gmnnpK6JWCQz2wzhbj6oydH9BvtkOOUbCXlRcF/wilBPp/A0CGptuVRb3+/YUq8bawoqI8f4r6/PorTHlBpt09b9J/1iDO5p9lbbRY8m/581X8NXUysz6txfz5om7Nhg2iHtPX79dn2ZQpnHvhKOwfA8Ccfx1C4DxU7HvtNXF5Wa3AkKdICRPiPe6PVyCjKQUb74DVUwB47re7bOPb+tz2fPop3DY5neiHR/DyP6KqKzlxpOan8EfeDgD+nnmKn34SDs/ifZStspU/9/8NQLssZY53I1LqRofberi9+tNKvvkGHn2ykAb33c3t82+n0FIIB0eQN/MPwkMCmDJFrC1Nny4KiapCaqlVOdfHj7tsGfHPibWcL0qH/Ci6+N3MjBkOKXTq3Kd6HIuh5kllBykv8CSk4jrQrJmoegfw2wYRYWIL7VPnoXJ4pJYsgXXrhJhU79mjC8difG83/DSP5inP8vPPds+VyqDW0ZwKFQ8ajx1xek528Ehd1cW+GBXfRix8RxXAV8uWuTweWRa9tJ58UniKrVYRwfrkk6LdxrlzYg78c+8SaLiWfqfF+BwwsA8bNzr3K78Y0YRUDUGS3OeHukRZDQmTcomMxOnn8ceBw0JI/fXfIrdJ5GtOrCHTeB7y6jC05eX0bXBMhDsEBzsJpFq1oGuMGOw2nF6LrAqphg1tyZbp+en8ut+erzRj7ZxSfBjBhiNK6EdOHFcOrO1ym6goMfF++61oAl+0+0qKPtwGKR1Iy09jyHdDSQ5WBmFlNTqrQBgLwSblbvcmpILEdxtYZHeDyLIYVED08JmxYAuplv8IMgQxvtV4eOopJk1vwDVbxWTzwZyS4X1z5wIJipCK7yJW0axWcS4V8TdhAkwc0Bvplz/gnVOw7FU414xcYy6/7BUNQtk5gc9ebkFAB8Xo26d8b0ojXjWvoGdPOHTIzA03/Ofx8zqiNwtjJer8cAZOaiIsAjW2AErkRznSpZ6YrHad3eHy+S1JW8gynYfCCMYn9nRybLrCJqSCMsFQ6OSVUldLOTKEx3s/RqA1CuocwNTiRxYvhvvvF4J75068NuXNXrsOgG21ahFkToCYfXR94xq+/MrE6NEifz0uDhKvFed2bKfjLFsGjz4qFi3Gj4ehzYVxqW/WmPHjxbV51Cq2908+TvQ1zxE6pSGXfXQZL695xfkAVGOqSxf+2imum9CsRI/VkB2JD4unY1xHjim1MLrX2cean0/yaIDwBE3pDw/3eoii87GYzWA2Sww89DlycDD9WcXCcV/DrpsB+HpHsfC+wkK7R62YR+qPP8DcSJyHmxLdJ3O98AKMaHQ1zdasJWHufkJ3Po5UUAfCUqDP63B/KwY/+xlbt8ocPAhTXw9GaidWwf+Nh971exNo8CXmGRpFNqJWYC1SFaMzdbc97HXHInvFvi4NXBtcNG1qt14kCVq1wmAQVeO//RZuGh0HR4aSrg4hHoSU1SrS3nr3hoceEtWz9+8XY0n9+qJAzIMPwrCX3oK4nYQbotgw9X2ys+H0iisZHH89etlKf0QD2OCRA3j0URj6v5fBYKR/o/788PIABgyQueyyDKZOtbJ1qzB4vvoK9INvxogfrfJSyXxvODMnPcK0YY+w7d0pROsbY7QYfer/Y5WtdPusF/FvxxP1aD/WLp0HwBm/xkJcAr/+CoX1XZRAV1YzztdSPAGnenPzTZJTjn67dkI0HD8uWrk9O+YaYnaJypWr273IyVBlYgzxTUjZekmdr0uzQ82plw0WP4Pi/sD3XkrK88Mm1eO6Cdm8/N/V7Il/EnQW2DGB7SeEB3Fk+FruvFNc559+KhZfZs+GDz+EZ54Rw+eQwTrCUkTusk1IZWTYPSnFbvakJFE8COCxnz+FXu+I3Bdg1fsxZGWZePHFddzb6gWCD99IhzRxvR/R1af3sDDue3kLX/t3IbP2EjAGo5v3PXV+Ogy7buK00tT1yKmZ3HhrDu3aiTGuXj3oPSCLng9+Sv2XupNjTYOiMEY3UO4HDxV6xyutCY7KK7j1oRO8m3k5p2JngCzBipdIWP0nLzwZyfHj8NJL9ujn8eMh4KywLRYU7ELW6YRicVFh9aMVwqbQHx7N3DkGeziZ2WxfTHRTrbBDnJib0gLM9u/egZTcFM7mnUUn6WgbI/bRJU54dTYoofqi/HnFeKT0enE5Tp8u1tFSUsTC3HXDGnJF7GgW/KkjMrLk6zq2zSXJLPKRpZOnnPrkmdT8zxCJWwZ2tb9IifHTy7Dr2FKXx/Pjj+K69fMTtYmOHRPmxOuvCw+VwSAKqkxZJaKArskU9mHda/vYpteLGU1IXayoN7GL5JEePaB7bF8wBZKUd9p1dTvg262K2Dkwmuef0zuvPBerRjNhSGcwBZJHOoeHdBUWwtNP255/ZfEXWKQiOCdWJY+a1nEmq3T9c+ZvEMcZnOecH1Ucf3+xOvzvvyIHe1SfJvDlBjhwFRbZzNenlFWXEkJK2YEXIWUOFKWTg4z5tscWLBArNYGBYvD7bpcwOMe1GkdYQBgsXYoky7yz8xR6C8z/d5NTUaQzZ4SRQLwwmjvHd3augqcs6eh0YkBNTYWvP4pnXMzTBH91EGatgp23wLH+DPGbzrXXYl89VyeRYkIKSr9SFH38Xjg6gIcGXSvsyaAgmDULW8arajm54EqlzGy6tM9laOPvu1TxM5gHJhtKPF+cyMBIAvSKAVYsT+qXbWJfYanDeXlKONMGC5Vb/5ZpvPKaiTZthMdk/HjIiXCfE1CYdJ5INZcu7GbW3ruAEL8Qlh9fyuSFk50XIZQVWd3JEwwcCG+/LQym2bNhdHshpK68vzGzZ8OGLYXc/+d6AEIoIKrzKxT5i4lu+orXSMtTwlhk2ali4L+pYtLu3ah7qQpCDW823JYjFZmVTK/Vr6ErKmRtfVjfOpTnBzyOv7+YwPV6Ifik6WKletjyx2lzQoRBLTy00H5sIAqMyLKI7Sh2U86cfRJi9iHJOoY0Hez22EJDRaGzQ4fgzI6W5Pz+JoX/O82ca+eICqN6E0v97+WDU7dSYFbuuSlT2Nkuhu/a+x7WB6LgROf4zijVmik8YfdIHd0ovL/JYdAq2o1Hys/PvljQpEmJVedp00DaPcEmpKxp7nuzfPUV/P23yH+98krx2gULhNF08qQQovdN+Y+VsvAQfXTle/RoF0NYmND+39/0Pv0ywgkvslIYGsiLf3TkvueOsPz8LACm95/u8n3j4kTY6bfzIzEMGQjAGz3m0qCB8PrN/0MifYMSseBDZddVhzdyMFOs+Fvqr6ZxnIg2CL9mN4+8vp3ERGFkbs1yUXBCEVLHA5XB92Qft0OI2v7qf/+DlNlPcU1T4aWb3t/CykbgP9JDEQVHlPG9Z/sCBmwUgnxXk2D7uK+G6nnySOXm2rwWp2rnEvhAN2j9O3r8eLDJp5z5+Gtm7hXXZbPcHXzxTg4vvgj33AOjR4tx5/774ZVXxHi+eDG884DY/oy0kbTMPPsCSpMm9oqgCrNmCW9m66v+5r3/HgAgqokQ//7nkgkKgg4d0nn/PZn0r2YyIkeEee9ueo5aI95Gd2cfiDxJrF8zZg/bSMGmmzh7JoT0Gd9xWTNRka6+/giBD3YjpPFeaLiGM90msr5XPBtr30cS28Dih9+aV+gQoQhONx4pgKvaic+ma/knhvs7Q92thEhRvNVxEefmTuHMaR0vvljiYxIeDmOvuAzyosmTiihKUPKbi4X3WWUrC4+K625w3fHO0ZaHD4sLMCTE7TG2ixHXQZqfIqSKeaTU/KjmUc0JVvLPr+4pvDrJus1YrFZbQ97yFptwRWysENw//yw8QO6iNfR6meBoUYXLv8hoL3ZTVIR/tvhMeYaW1A5zOLaAAMxB4jNJbObseeeiJ0lJwjsOYjHgwQdda+Y/D/7J1qStROiCaXZQWUAq1qLgYkUTUhcrHoQUwBMPB9ma5c3fX3KytMpWZu8VA1Mb/ThxP3hIKr9mbAAkiRWauUd2iuWy228HRMjSp1tFye4256agT0kESWb6b/NK9ZHWHRZCoGmYmx4YxZAkUY19/nw4sCeIYcaZkBfNfn/FeFIN5KJcJCsEmRWj2IuQsgQLIRVsFPEOZrNdMz70EMTEG/lpz08ATOgwQWyg5JHUSz/Hzbsgv9YmvvrKvs958wCdGV28GLC7xHdx25cJRIzyxIkiPORcusTCT/tyd8y3jMtZwTcf1hMCSQ1j8CCkSsu7N9zP7YblPHpfpP1BSYJJk0SMkocSvdcNaQSF4cg6ExsPHyjx/M+K+KlfOILenitZK28rOYX3qR6p1NxUjhYK8XH7FcMICID7u99PTEgMp/KOEj3oa1avFnnHR47A4+86eKSKeWe/eGQ1AEdqwduP30XnhE78fPXP6CQdX/z7BW+tf8u+sTq7pKaWbGqrTPzHIuHBRQ+S8HYCD2+7lSTlNr1Kn8g486+Q1AWTLpd7v1dKaB8/LlZH/f1JaxxLtv4YyBI39e/m/QtyYETzEZyMAKsEBqMR3eci9GvKAHi012PUDnbh4X3oIejWDSkriz+CP4AzXbFi5rvtP9u3cRwTHFR5WhqsThLns1OdntQKKmYhecFf78+4VuNYPmE5bwx6A52k49ud39JzZk+OnD+CdewYBtxsJjWsdEIKxL2VqnzvQTmppKaKKF9jivDcpkf6268rV6gLFC7uo2bN4NbE0aQHiKSAM0ddh7GePat4sLt9QtPnr+Lq6d/w+DN5jBxp16NW2cpdC+6iyFLEkKZDuLn9zU77iAmJ4XW9CNtcUs/IocyjvLTmJcxWM8OaDfNaDh5Ad/U4AK4yzeH4cRFK98IL2CIW5u93H7Gg8vIckWMUeHIkE+q/RKtcsbjxa+FmOs/ozLkxPSE0hTk73Yf27dNnAtAiqA9tXEdtOyFJEj/c8CkDGw/kyy4w4FYIb+hhdc0RZXy/7qp8BpwUAm5ufDZHziuhUD40pbWeFCIrSwomf/IACkMPUi+8Huvu+If3b7mHhAQJqX49MSZYrT6Vmb91dBP0ufVBb+LVH9a5DeuzWkWPOGof5GiXa7HIFiZ2mEiLNkoZ22LemiC/QO4PFnPz9rr5ZHR/HKvOyJiWY/jv0a2Mv7ydLYeldm2Ja8aJ66xxnh+Fof+RN7Et3NYXOn4LfgXE0IZu59+lz6YkPrn1fvyTjosXe/BI9W7QGz+dH9bA85j9ztMlvgt7HtzGY2OG2rxP7phwiwSnxLWcWkeZm4sJqd83baHQ/wwUhfHqXQOdd6DOe23auC1HHhYQRtNaTclUHdvFPFKOYX224xraDkxByP7ZzF//H4WFcoUVmygPg7tcQary1vIJJTVAsXWK9JAQ26vEa/R1RBPx2kYTH/2x3va4LIuCLxkZIsTwKTct/6yylSkrRRjp/6KuQcovEG7Fli0r6FNVbzQhdbHiRUiNGSPCswC+3VBSSP29ZyP5uhQoDOfVScrA5CGpPCYGGshi9WHOlnVOz708ex6F/qchN4Yfn72OK6LF5P3Lrt9L7McTh7PFgNizaemFQIsWMOe7aOJ2fGAzogqTxESZa8wlyLFAk5cSM8ZIZdDJzwOLhW++EdFzUVFCUP118C/OF5wnISyBgY0HikHfaPfATF0NfpEHefmtDJtXau5cIHo/Vn0h4QHhNI1q6rYvU3ECA2H4cFG1cM4ce0sRm8GXlARZWRUipK69FmbOLNtiW1ycRHCOCDuY/Y+zgZmak8Ypi8gFemDEMJ89ZfFhasEJu0dq5moRu09SFx67WxhXIf4hPNNH9N6YvmY6oRFFzJ4tvJffLFPCVAsLncpVr10Lp9YLz+KBepFc0VJ8b1dediXvDn0XgCeXPcmcfYrntlYt+xfjkOibVZhFoSIcr9r0MB9u/pCMwgwaRDTA0kB4w2Z2eZw5069hdJjwIsw5/RH7Tyfbjal27fh9x3bxd3pLxgzzUArZBT3q9SAkNJLTSvSTZLWyvDHsbFXL1gOpBHq9KCJiMNB011xu2SKMiHdXOlTddJMf9dtvIDcVhUPGtfNSo90DkiTxRO8nWD5hOTEhMexK3UWXGV14be1rnC84T6h/qC3vyVe6JHSxGRoxnGX+fOEJivdTDOn4OCRPF6Cq8t2str44JYhzOR0BOLh3u8ttnngCMoI3w4gH2Gf5k1v/uJWEdxK4Z8E9bE3aiizLfLHtC9acWEOIXwifX/m5y2PqvD8TgGWNrFzz2zV8v0uUYHfnjSrB6NHCuNy2DenkCTp0gBdfhOGt+oPZn6T8ExxIL7nooVJYKLMmTYzjE9vfzje3T6G7UXgNmnYeiL/en8NFGwka/CqbM92H9p0MM4IxhEmjOvp23IgqorOvnU3rOmLBqEV0yXnJJYqQalk3lyGSKOm/ojG2786X0L5FM8Rzp6OMWHT5DGw8kH/v+pfEes4VJW3XyNq1eMNgkOgYIRYFfli/EtlNoYmlS0WhT/8+H1Mo59CnQR9mjJqBpIbbuyiSEbhX3KdHG4Sik3S8NvA1fr/2dyICXYwjipBsUxhuW6QI9Q/lzk53svGOjaS8sJvN7z/MP4ujRQ6TWnXUg0cq2C+YwYpXelLnSay9fS2NIht5/U4ABg+G0PPie9yleoyKCalX5ooxOD53JB3bFgvz9XHe6xDXgQx16i/ukXIoNKESGmwgqlCcmx9WbSLPWACSbM+RqkCPVGm4e3gPW8GJ3ct2A1B03F76fHiHkvmkkqJmaxfAnO32cN7vvhNjo7+/8Jy6aBMIwOx9s9mVuovwgHBuzVPu8969y9RHqyZSrT/lq6++Srdu3QgLCyMmJoYxY8bwnzpxK/Tr1w9Jkpx+7rnnngt0xNUIL0LKYIB7BwkD52DhWjKLVaCbPltMjrXSR3HlcGW5yo3RpDK8rTLYZdonDYsF3vrnAwC66u6ifZsApt8ohFRm5EqWrvWtz8r585AbJAbE0T3KJgSCg2He9Os5e04YQjknj2KVreQaHXpIgVchlRdTl3wDBFosFOw9ytSp4vHnnhM5aN8qYX03t7sZvU5v9wi1bAlxcTTOhNu3Q5K0mZkzxWdbtQpbflSnuE4iMd+DR8onIiLszWZXrRIZoTrdBV0lahomJqI1h5yF1JtzRdlz/dmO3HeL57Lnjtg9UvZeUrPWioWBFvrhTiEe93S9h7phdTmVfYov/v2Cbt1Ekn8RgaQhxLGaJ5WbKzx+XfzWABDRx3l1/8HEB3mgu4h3uHnuzXy0+SO+2fktGbFiBlu64ktmbZ/FxHkTafG/OAKzRUja6SgD17S+hr9v+pujDx6lfjtlv4ox8sOLwwhM6wWGQsa+84pTWN+cjWJVO96S6LWiYXEMOgODmwy25UmByI16otcTro0plfbtbcuQnx79k8hcPaet29h8TEneduOl/v6nImgiKoKOaD6idAfrgn6N+vHvXf/Sq34vsoqyeG6FKCByRcMr8NO7md3d4BjaF0oef/2axx9/QLwkzn1AvUaed/DggyJx4eGHXT5dvz40qicMUFP6cfIcQoBBhBt/+4MRrroTJCuJdRNpUqsJ2UXZfL7tc7p90Y2On3fkyWUiHPV/A/7n2ug0GpH+EflRGy4LZGfqTqyyldEtRtM1oWvJ7V0RE+NcIkxh6jMhcEJ4OH7c4j68739f7cASfgzJHMQrtw8Vx6SExE2/43vmXy+KFVnaz+Kwn3KvOpZAVyuJhQKne3Dzjd5Deh2JDIxky6Qt7L1vLx3jOvr2IjXiYMsWapnSyJMC2FwXZv37rfC+eQntS02FhTOUQhORZlrUbsGCGxdQJ8RFUmcphBTA7f3FdXM2eAWF6+25kY6IFlUy/u3FHP1kryfx1/s7NeV1wmSy5cm+++QKDj9wmKf6POV+sUD5/Lpz51gy/g/W376e5MeS+eKqL0isl+j8utxcMa+ARyEF8NP4n9hx9w5mjJrhc04jCFvlyvbie9wWVLJy3/nzMjuKxHcxqff4kjvwVUjFdnDvkUopKaQAOscK4bzuxCZyCkVIxIX2SNWrXZukUDFBbFshxoe9K4XnNyUUxnRNLPkiJU+qdj78V7QSo1FMhQ8+KJ6eNg0aNzjH6i+n8O22WXyz4xunH9Ub9WiPRwnZpFy3l0hYH1RzIbV69WomT57Mxo0bWbp0KSaTiSFDhpBXrNbxpEmTSE5Otv288cYbF+iIqxFehBTAk5OaostoDjozb/1uL4OelSWzKUes8NzRc5zdO+AhtA/ggTE9QZYoDDnE7qPClfzyzO3kRf8DFgNf3SsEbu9WzahV1B70ZqZ856FRqgMLVpwTyefA5S3dJIL7QGKixFWthDchKs/E64s/IdeYYxdSgYFeV1FCgyLYp8yZC9/cy5kzIkzsvvtEUY2/Dv4FwC0dlGB/VUh17gzPip5Nz6+BgIS1vPKK8CJZLFC7rUOhCfDZI+UR1Sv1m2ggSrNmXoViZdKnmZiIDmU7C6kftwrvRefw4aWaf+JCnHtJnc80c1heAsDkIc6ekEBDIM9fIZJh//fP/8g35XPXXaKAxxlEeN+5ncJAeuIJOJr1H52zhNBvM2xCifd+d+i7jGw+kkJzIQ8seoBb/7iVdTphjP+28C1un3873+78lth0YTTmR4Zw8Jkkfr3mV4Y2GypEdrFeUiEhEm8OfxmA/0JmcPovxbvbpQtbbflRLiZCHxjebDiHlTCahc3gcMs6PJD4gPcXPv88tGhBSPZZ3vlTfE+PffudeM6Fl/rECdhwRpQ9jwmK893A9ULd8LqsnLiSB7s/aHtsQKPShfUBNK3VFF14OAWKzb5/VSpLlkCCRcT1RzR2kx+lYjAI47Z4ySwHbripJwC1Cy288OMftseLikSeDL3ehNjd1A6qzZ83/MmhBw6xYsIKbmx3IwH6AHal7iK7KJvEuonc3/1+12+yaZMIIa1Th5uutxcoean/S96/BEfGiYUt5tgLACUmQnPE/fP1OtdCymyGj5YLA7Zt4DCiwkKEF0eWxTgaG8uQpkNoGd0So5RDWqeFZFOsBLoS2pccBs0C+njMfXVHsF+wzSvlE+oA85cYp3dH9MFkCeVEzlHWnVrn1SP1+ONQO0CE/J6KgC+v+tK9MFC9lxs3umzyWpxRbUWhpvCoLQSdUhbSHEL7kpNFqDoJW8nVnybEL8Tm6XErpA4cEBER4eFEt+5K41rOvR1LUKuWbY7QJ6fQs35P982uVW9UZKTnhsFAeEC4U2hcaXjkepGDfThKCZl2EFIvfb4budYRJEsgj48ZVvLFpRBSGeppdPBIFZoLbV7Z4sc/trsYi1P0mziTJmytUKNS/OQCeaQAjHFC1Oad3AHA7g3CM54SqqdNjIt7RRVSBWCJ28xfy3KYNEkEsnTrBlfetp8vbmpJ30kvc/rh27n1j1udfg6eO0hUUBQPJz5kXzTQhFT14O+//+bWW2+lTZs2dOjQga+//pqTJ0+yTV2lVQgODiYuLs72Ex4efoGOuBrhg5AKD4cOIWKynPWPfbKc+tku5IjjSOYgXrhRJJmTlWXvueLGI9WmaSRBOWKw+vCPdeTlweurRMnzjv5X066RvXzLde3F5L0p5/cSfeNc8edGsQIeZm4kijeUg+endwRElZr3lz5Nsnm/z4UmAML8Q9mj5Lzu/00M0i+/LGyHX/b8gslqonN8Z1t1H1vVvFat4K67yImNpF4OTDb+zpkz8Nhj4umgpmIlp0tCF2FxOZQ+LzOqkJqvlLEvR1hfRTC+t5iI8sN2cvq0yL04eMhCcpAoe/7wlaULA7OF9oWJXlIvz9oMQRnoimpx76iSguP2TrfTKLIRKbkpfLLlEyRJVNLKClNC7F46w4IFIkwyvM0XNFccprV6DyyxL71Oz89X/8xDiQ8xvNlw0a+poejtNEDXlOHNhnN3l7v5tctrAAQ3b11y1VpdxXXoJXX/yP40sPQHvZGAA6JHSU6LTmQEib9v7lc2ITWs2TDe7gkzOsO9V8IzfZ5xbyA5Ehho6xN2238n6XcM1uV8T1q6pcTiitEoEuhpLsaTES2GeQ6TKyX+en/eH/4+v13zG7d1vI3bO91e6n1IkkRnh/C+KHMqhYUQr1ThjG3WsdzHGdVcDBDR+TBj07e2as1vvAEHzx+AfkLsvD/sfeqE1EEn6ejfuD8/jPuB5MeS+XD4h9zS/hZ+HP+jENyuWKGE4AwYwAOJD/JMn2f4eMTHtI9tX7qDHTtW/F63zskIf+464Uk8rVvD4ZMl55FffoHMOCGkHh6ieAJUA7dRI1AiRO7vJoRgyMCPOIRzwQlZ8UglhcE1iT4kRlYE6hivzGdR4wbDvmsA+Hzjt3aPVHJyCfGzfDl8/0s+9euKzx3fsit9GngwGFu3FiIjL8/ex8kD9SPq0yCkOZ1SxdhoqdfQZuSCCK8ym6HuYCF6R1420i7i1OiD4qF96vu2b+9bdSFJ8ilPDPAprK8i6NbZn+DMRLtHXbnOjEaYuUF8Fx1DhxIWUGw8Kyy0R3b4ENqneqQs58/ZHt+Xtg+LbCEqKIq6Yc4l6K7sqIzFsbtYsk54y8KLlAWWC+SRAkjoIMaAOoVHOXIEzh/fAUBhZIzr8US5xuLzaoHOwn2v/cOiCw/3swAAQWNJREFURaIQzg3Tf6PXrO40/08sND3wr4Ex9Qbb5rzhzYYzsvlIvh79NRGn00QCaECA1ybSFxOl86NfYLKysgCIKpad+MMPP/D9998TFxfHqFGjmDJlCsEeDOKioiKKHEqmZWeLsDaTyYTJh1WjmoAuKAg9YE1OxuLhMz0wdAi3r/yApJCFbN1qoqhIz6yN86AbdAobRqDeH5PJhLRvHwZAjo/HHBTkdnWtfWRvNll38/e+fwh/vTcFTUXJ87euvc/pu510+Sg++28aNF3Mm+9n8c5rngXMhiO7oSM0C29T/nPkB+bI2hgyzxFdkMeW0I/orOxSDg7G7GX/IX5B7FWEVLOiPbRvLzNs9Hk+2fQLr68XHfJuanOT7Tj1+/ahA8zNmyPrdKQ8NImwZ9/kqT37+YxccnJCQbKQbtgBFmgX3Q7zkiUYrFbk0FDMUVE+rWa6QnfZZegBtROtpVUrrA77Uo+xqq77rg1agKyDkDRm/32ayRPjmDpjM4Sex2COYGzXrqU6ljpBijAJTSEz08w36/+CttAhdDCy1YqpWL8RCYnnej/HpL8m8dra17iu1XXEhcbRflg8/AYFh08zZowMkkyfhsLrkhsfTUB4uMtzECAF8ObAN23/606+Bcue5dqQ7oy/VuQS6d4XTW+tDRuWuBelunXFfXX8uNN19+WNU7nj85XUMRZilgx8dVgPgVlI5iAGd2hRpvMVHRiNX9v23B2zi/jQeO7ocIfv+0lMRHf33eg//5wv5utof+8Znn5pDjOVnDJTw4Ys/9vMQw/pOXhQgvuEkBrcaHClXFujm49mdPPR4r3LsP+OsR05G7KSRlkiTyqIHCKN4lqp27x016BLIiLwQwip3JglfPzNKQYmxvHy/3Rw4yTQGxnWdBjXtLymxHuFGkK5u9Pd3N3pbo+fT79ihRhX+vZFtlh58YoXXW7v9R6Pi0PfrRu6LVuw/P471kmTALh+UGPuXtOYouBjPP7xMn57eaTtJVYrvPDBYRixDz1+jGo5RMwThw9jAKyNGtmu9Rta38CzK54lm4McDrmCLnmQu/0AAUOGoE9JRUKUZJ48ukuVjEO6gAAcTcnGd1zBZS914yCz+G3fr3w84g3C/PyQTCZMJ0+KcAOETX7PPQbo+yL1jovxtF/vW7wes75XL3QLF2JZvRprB+8emaGX9SN0qTD+D4Z2opmyf1FkQphspuZzwCzuA9v7166NHyCnpmJS7BuTyYRu+3b0gKVdO6ex3+Mx16uH7uBBzMeOIXvwLOiOHhV2Rv36Hu2MiqB7bC/+yxSeQPnUKcz5+fz0mz+59YSovbff6JLnYvdu/KxW5KgozLVre5xHE4ITKAoLAgrIPXuaYGXbf8+IBc72Me0xm81Or4kLiiPYGku+LpUjJhFGF2oSV5clKMjn77sicLzPG3fsAfxIfVMGH3yaQ6tCcT2FNmzk8nrVRUaiB1oTB2SQErQCdIPp+vwTPLpRzF+dMgOBQsLyzfxmHIs84c4S+zF/8424/7t2xaLTldluqS74Oh7VGCFltVp5+OGH6d27N20dVhZuvPFGGjZsSEJCArt27eKpp57iv//+4/ff3RcyePXVV3nxxRdLPL5kyRKPAqwmEWsy0QOQfv6Z7Q0bktzDdcPKUKsRnSUQa/gZHnplLR3jG1DQSMTm96ndiIULRchVvVWr6AKci4pinfKYK9qERbIpC05J63jvny+hXxFxllbk7DvHwv3218myTC25HhmG08xY+Re950QQFGR2uc+cHD/OmITnJ94QYjum8jAgNIiwTIjNNrA31mzzSOVZrSz3sv8zx06SrAiptn6bkcaOpP77KyiyismrlqEW0SnR4jhlmRF796ID1qSlkbNwIaZmndDVgqYZMk9EvcCL598h6rJ/OW/JJ1AKQH7+PXTfCCM8uW1btizyXoLYHdFZWTiu8/5rNJLk4vMtXeq6f0RlEFrUkNzAY3yzeAUJ4bWZs2sj9ILmui4s+XtJqfZ1OkvJYwhNYebMNM53FIUmEqPqur1OouQoGgQ24GTBSQZ+OZCXm71MO0MetYC6nMFikajd5W9apooeQ5kNm7DNx2su4fx5ugEZO3awVnlNu1WraAIcsVrZV2w/YadOMQAwHzlS4nhHpzYHDrE7PIyp322DAVCrsDVLF5f9XCX6JbKb3VwXdR0rl64s1WsNffsyYPZsmp07x9TVsMAgvFS5UXUYOjKP9esjAQiv/x/ZMfvQoUM+LLPwePnv14pGypBsRWdiSSU+YjNkQZ4f7N59gn17XfcU8xVDbi4jgTAjBFisPPfLd3z53iSM7WZDw7UE6gIZFzCORWW8t/VFRQxfL6prrZIk8ny4Pj3d481ataLNli2cmzGDDQ6NX9oGdmIbx1hwYBE//+xPeLgYKDdujOdogGgj0Sa4PetXiGNptWwZlwEnJIldDsfUN7wvf6b9ybFmp2AnbP15K4UNf2aY1YIVKKA1W9f9U9qvoUxcdvo0avCmKTiYRSnJjO8Sx6uZDSiKPMmz373O67VqEXL2LBt/+43zilf/559bcDgvH3q9TX3FybPr7HnSvHz3zWvXpjWQ8vvvbPUhuiAiI4KOilPp9xMdaPXnIvR6mR076nDsWC8CG2znrPkwfpIfuiM62/0lmc2MkiQki4XVc+ZAZCRLly6l5/LlxAC7JYkTPo5jnWSZBsDBFSs45OARK07rFStoDhwD9lTAvOyJjlEhrDZDoV4i0GJl5Xff8/ynnWHsHiTZQPApfxYmOx+DzW6Jj2edD/eaITQaOEXmyWOsUj7PH6dFaG5YfpjLOaWBrjkHSIXGYjwNVdL/dh4+zKlK/k5csXTpUiLOFZIANMiS+XjuFuZIIoS2KCjc5WdokppKO6BekbLweNkCgpqtY51FFGO5PnI08enzbdvnv/YaK+PjS3g4O/7yCw2Bw3Fx7L8An72iyc/P974RgFxDuOeee+SGDRvKp06d8rjd8uXLZUA+fPiw220KCwvlrKws28+pU6dkQE5PT5eNRuPF8VNUJJvvvluWQbYGBcmm9evdbtvzo+Ey05BHtrlVvjnmTZlpyPppfvLZ7LO2bczPPCPLIJvvvNPj+x5OOywzDZkX9DKPJshMQ565dabLbR//+3Gx7dXXyW+/bXa7z9mzTTK3XS4zDXnWv7Mq5Pux9O0ryyA/2He8zDTkITcjvqsOHby+9vO/1sj1HhHbG3XIfs8jMw255Yct5df/eV0+k3HGvv3Ro2K/er1szM21Pf7CrY1kGeSc4FA5jCx5/ItfyyHPIC/tHi3LIotAtlxzjWw8f758n/XkSdv+ZJCNO3c6PZ+XlyfPmzdPzsvLq7Jr8/L3r5GZhhw58lX5889NMpO6yUxD/nzTF6Xe1/rj68U19Eg9mdBk8fc05FPnT3l83d6UvXLU61Ey05DH/jRWNn7+mSyDfKjZULlRI6s88ouJ8g9txXdmfukln4/HtHatON/16tmvtREjxH4+/rjkazIy7Ofm7Fmn5049cKssg/xlR2QmDJSZhjzu84fL9d3n5ubKP835qczn2zR3riyDbJKQP+hikGWQl+sGySDLOp1Vvv8Bo/zMohdlpiH3/rJ3lV1Tpf3Znbxb/qKT+N5/bD1Vvn3gK7IM8ok6/hXzHkVFslWvl2WQEx5F5p72MuGnZJ4Jk5mG/O66d8u1f9OiRfbrrKjI47Y+3eN794r9GQyyMSXF9vjve+aKe+rhhvKzz5lko9EoFxUZ5c6dLTJ3dZGZhvzZ5s/s1/q114pr/bXXnPa/L2WfLE2T5ImjxXe+NmignL16kyyDnBSKPObz+6vs3Jtff90+xl55pe0zxVz/rMw05Pin+8npbXvKMshHX/1e3rfPKK9da5L9Aotk7u4oMw05N9jP5Xjq8lytXCm+27g4r+fKaDTKpzNOy/tri+MbGvCz/PPP4nsfN84igyx3e/wFMV//MLLEa6116sgyyPkbN9rOuTUmRtyz69b5/h2p8/1dd3ncznLNNWK7N96o9POWlp0mM1WSDyjfzbY3l8j0eVVmGnLfLwe7/hyPPy6O7+67fXqPt18bK8sgp9aNtD3W96u+MtOQv9z6pcvXTF32krhHngmVmYa8uV6o+L5/+aXKrmmjsdh9fuqUuL5BNvR/Ut6YIL6zlO8+d32NzpwpyyAX9LvcNocyDTn81XD5t92/ycbNm8U1HB4uW0NCxOdbvLjk9XfZZeK5efOq9LNX1k96eroMyFlZWR51R43wSN1///0sWLCANWvWUE9NBHVDYqKIWT18+DBNmzZ1uU1AQAABAQElHvfz88PPXX3HmshHH8GJE0h//41h3DiR8OoilvmW9oOZ+Mgi7t77NQChW+DYdYOoE+aQz6EUPtC3aoXew3fUNLopEdQnS3cKwpOo5R/DTR1uws9Q8jXXtL2Gtza+Bc3/4v1PTDz0UCB6F+G7/6yVIUZ4pDrGd6yYc6Qk5k65ogcfpR8g2CRysKSQEK/7r187ntPhkBUAEUXwdJ2xDB/7BD3q9SiZD6J8b1Lz5vg5xExnjBvB/gWf0Co9l+Sn3uO9uMO8+Dy0SUsXSexvvYXuwQfRlTe/pF49EaOfmQl+fvi1auWyhmlVXvsD2nTkn7W/kRm4ixffOg83irLno1qNLPUx1ItUxoPQVGgmvFGta3WhXi3P40Tr2NbMu24eg74bxNz/5vKNv4E7gWaBZ9j7bwGxb83hbWVVWN+tm8dr3gllxVlKSsJPlkXdWCWPQN+0acn9REZCdDSkp+OXlCQahKmf7bDI4diWgK0C3vjEnuU+T0H6oLKf7zFjkK+7DsMvv/DANuFB3mdtQdcBZ0i8+2sWpnzF0U1HARjVYlS1HU9bx7ZmUYQ/YGRwj4MEN9LBcsivHV5xxxwdDampxOYbSIrbBTeMgoAcetbryQM9HnCf++QLa0Q1SWnAAPzUJkBe8HjOW7eGdu2Qdu/Gb/FiUYEFGNJ8MAbJH3PkCT785hBPPdmKDRvg3yMn4Kpt6CQdY1uPte9XvdabNXO61lvFtmJE8xEcOikKPNQtOMzMl9N5GFGx7/peV1TdteJQ8lI3cCA65X2fv2oCDx58heTAVfwt67kJ+Pi1Dbz9zI2ABL1fh/gdNNDVIiRfVHXza9zYfU1olR49wN8fKSUFv1OnwI1dolJXH45Vyc38t2Euxs8Ntn6IAFl150EWXN366pLfWVwcpKXhp1TS8zt3DunsWdDpMHTs6P1YVZQiOPozZzyPfUoer75JE9/HyDIS7RdNg4D2HIvcSYtzsO77k9BdRB7d2NHFdwG2Qk/69u19Or6GjTsAc/HLzsPPzw9Zltl1dhcAnRM6u3yPyxv3grVAgMgjVMufGyIjff++KxA/Pz/8EhIw+xswGM3UvWwGcUohvdjLOro+JqXKS2B2Hm1j2rLn7B7axrRlzrVzuKz2ZfCjSNGQ2rWDjh3h448xfPIJDBli30damq34kOGKKy7IZ69ofB2TqnWxCVmWuf/++5k7dy4rVqygcWMv1WaAHTt2ABCvJl5eyhgMIiO4XTuRRHzllaJohCMnTnDbg7O426F+x0cL4WG154eKhx5SxenXxB5TfX+PuwkwlBStAF0TulI3rB4E5HJSv1Q0pXXBso3JEJSBhI6W0RVUujtGSQY3neO7YQu57OwN4nEfQjuHdmvK5VmfkxInjOaXYq6lZ/2erpPq1Yp9atEHhe4NezK1n/g75OM3eeTBH2mTBvl1IkV95Ice8i0x2BuSZH/vli2rxeDWrb6SJxC7k6QgUfa8XXRHe+GIUhAbqpT50pugvegDM66tbwUrLm94OV9dJboiv3taqWp45gzzDsxDys6lhZpvXJqk2ZgYUZzBahXlkx0rlLkbv9TFDTVxG8TrlKI6/ybYH+7dsGyFJioS6YMPyA+3V37Mv2oV//ZtwMf7n+doxlHCA8K5v9v9PNzj4Qt3kF7QSToC4kVCfe6pw+SeFDkElngPjXhLS7QoqT+qtqjgR/wO/HR+fDHqi/KJKHAqNFFhuKjeF+IfQr9Gogx6TtxCPv0U/vc/oJUwYC9vcDkxITH2fXi41h9MfNBWNbIBJzm0UhThSQ6DyxtWUaEJcB7jHb6/yde1YEDGDwTktOR0pAWA+i0/RvdAa3T9X4T+0wD4oM0T4gUREfjUhyAwUJQ+A9/KoO/YgU6GU+GQ1nwbK1eK5s1mM3QccJiDWbvQS3quanFVydcWKzgh7RIigObNfZrbbKgL1m5KwNtQxywPzXgrkqGtetsKTmQd2AF1tyAhMbrFaNcvKGXvxKZNRNuA0DwTstXK6ezTZBRmYNAZ7JUhizWo7la3GxL2uTrEpITHXcBiE+h0mOuKa6FhUSZxaq2YODfjmxq+ee4cX4/+mrcGv8XGOzYKEQXOdsz9ShXR+fOd+3mtUyrMtm0rKj9eQlRrITV58mS+//57fvzxR8LCwkhJSSElJYWCAlEC88iRI0yfPp1t27Zx/Phx5s+fz4QJE7jiiito376UlYsuVsLDRUe1uDgxqFx7rT0BcMkS6NyZwB27yQjWMewmmNVRVLMb8uxMUAdhq9UupNxU7HNkSEshpAw6A/d0dd/TSyfpGN9Kmbxb/c5bb0HxkNTz52FvmhgMm0Q2L1X/CY+odXZTU7lxZANev0GUnfVlstHpJNa8excthiqTsDpYu8KxYp8DiXUTmd0adsVKkJtLcJGF1Q3h1Ip5FV82VH3vC1yxT8VWQjb6P2gletdc2bJsTVv99f4Eo0wCitdmeHPf93VT+5uY1ncaZ1R7KCODnzd/RUe1eFn9+k5eIq9Iki05nRMnxCpdfr543F1lKxeV+zhzRlQ/0utp3k9URIsNiaVBRAPfj6WyiIkh73V7w9fl4XuxylYub3A5X4/+mqRHk/hwxIduF1CqC5ENxFhmTj6D5bSoTuZftwK/X0VIXVXbnp/63OXP0SamTfn2m5UFW7aIv/v3L9++HFGF1OLFTtVeR16m9AFrvoiXXoJ//gGpjRBS49TxG0RlurMir9CVkBrcZDBRDVuQ7Q86ZHoHCu9Udq0wEsISSmxfaagGbnS005io08Hy926k4M19XD1ENKlumKvHWvsA1r7TwFDIkKZDuCpEKUfu2KTOG2oZdF+ElLKAsi0eQtsJwfytaE1I86vE996/cX+igqJKvlYxlCWlIqFNSPlQ5MIJX6r2FRbaqzxWctU+lf5N+3AsUvzdOHQDAH0a9LEvqDmSk2MXem18u+daNBMLVX5WSD17zNaIt2V0S/t4Nnas+H6UEunhAeE0j7TP78FK0ZoLWf4cwL+xWOjtkAIBFuVBd/0FVCF1/jxdErrwWK/HCPF3EIKOQqplS+GJkmX4+GP7Npdg2XOVai2kPv30U7KysujXrx/x8fG2n19++QUAf39/li1bxpAhQ2jZsiWPPfYY48eP588/fetNdMnQoIEQU8HBQjzdfz9Mnw7Dhgml0qULH34ykcXN4e4r4d+Wkehy82DkSNHn4/Rp0a/Ez8/9qroD41qNo1V0K57u/bTXCdI2EbeYz8bNJkJCoG5d6NsXbr8dHn0Uh7C+ChQC6oCiTvyqgivNqp06CXsSUm48Us2imhEZXIvJI2Ry6tbh9d5w1R1BNGtdCYPQdddBQgLccEPF77sM1A2rS7ghCnQWaCESeYc3K5uQAgjX2VfZagXWIrFu6bw2L/R9gVHdbyJXcdb9t2sFXdQKwg49XHzGsTeUumKXkCBKwnrbXuVfJRajdWumDX+dltEtub/7/RVaSrw81Ln7UdYPbMHeuv50v/ohDkw+wJrb1jCx40TnCbgaE6+UOfdLz8CQKkr7hjfy7nH3GUVIdTTUo3+j/gxqMohnLn+m/Pv95x+xuNWsmV20VwTt2omws6IicEjMt92bDf8h35wLoSnI9cTq89iWY+2vVxcCIiJcrkhLksQDiQ9ySLHZrjAJwRDUsFHFfQZf6NEDmjQR3UZd9AyUJImmHYQXblRQBz6/8nMS6ybSonYLPr/yc1vD4VIJqdI05lXu/e0JErmB/0GYKHwSHg7HgxQB23Kc69eqHilF4JRZSKkeqfPnS65uqqjtOUJCnEq0VyZ9GvSxeaQaG0Rvp/GtXDThBfsiZny8z8cXFFEbtQ3U/kPrSzbizcyEP/4QNtHGjbbXOfb2CzYqquVCeqQAXQMhbhOVujmmyHD3c5D6/eTkiJryxVHtmNaKV07t1DtzplhAAfu13bsKvcvVhGqdIyUXc6EWp379+qxevbqKjqaG06WLiHMdO1ZtjS646y54/30ST6+G47MwGWDTu0/Q+ZHvRCO/UaNgiuhaTdOmHptQqsSFxrFv8j6fDqtPgz7UCa5DGmmEtFlN3u5BJCUJ/aakAcBoIVRsfZkqAgePFFDlQkqSJLrX7c7iwsUMfbEZG06n0atup/KH/LhiyBDh4agmSJJE57odWHViJeisRARE0LN+zzLvr1lcPClJIsfN1ui2lMfz5VUzSa39O6EpBSRkyQzNqgOkla0XhqOHKVDxoHpagHAV2qf2yuvShaZRTdk/eX/pj6MykSR6LROGzHQvm1ZXmrQQnqKwrAKildu+drN2FfcGipDSn89gxQMrKm6/lRHWB8JrOn68aHY1Zw5cI3orXVb7MhpHNuZY5jFovAIpPBlZkuletzv1IxzEhLcQVmBChwksiX6ILslm6hWJEuK1m1bgd+4L9erBkSPetwH0p5O4q8td3NXlLvtzqpfGS762E716id8HDkB6uu3acIly72e1aQocJr7XSpIX38yYCaf5NnkTEhJjWo5x/VrVI1VeIRUZKYRAXp4QDa4iURx7SFXRAk/9iPoU1IsFUmmsGPBjW411vXEpw/oAkCQKQgLwzy7i6LF/2Rkk5k2bkFI9wSDm9mGiAXBi3URm7ZgFMgSpET8X2COlLrJ0V6Z+XbyHRe2ICHEOZVmIZ8cQQLPZ3otLtWOGDxf24JEj8P33cMst9jlL80hpXNSMHg1vvy3+DgyEr76Czz+HwED6NupLVFAUfpIfw7rdILq+16kjVsfuuEO8xoewvtKi1+ltk8KEV3/n3DnYtElovpdegokTodZlwkiuUCGl5EhViJA6etT1qt25cyK0C4Q7vBiq52TDaRGi0DmuDN6PGkqn+I62v4c0HYJBV/Y1nUbR9kG/rJ6tAEMA8S1EfHzdbEhMVY6nPELK0SPli5ByDO1TJ6WyeMQ0fKJRC3H/RRVAo0zxmKFuKbwM3lCN5fT0itsniBxKqNiwPhU1vO+vv0ToFmKhQb2v2oxZRMuxIoeqhFfEh2s91D+U0DbO13TDVmVfRKk0VG9TamrJFfqyeKRq17av5itl612SlyfEFlC7j0jk7zRuBTfcAM2vEmHQver3cp9PqhrAqanojEZ7w+zSCilfmvJWUTPe4sR3vBwrEJ8LI4Lauw93LouQAizhQgCdPrHbFtpnC0ffvNm+4X774lb3ut0BCDKBTl3/v8AeKVVIqU3l9Ql13W+r19u9yOfOOT935IhIBwkOtl8TOp09V+qDD4TRZjaLcKIqvh6qA5qQutR4+GHRnn33brjtNtvDgYZAVty8gteav0a98Hoi9OGPP4QrWGm66UuhibKghvfNPTCXyFpWuncXUWhTpsBXs6wYIytBSDmG9sly2YRUnTpCkMmy06BqQ32sQQOXg2piPecQtC4Jl04ncNsKH+UL6wOID7UbFUObDi3zfvwaNAJgSuw1RB5XYv8rKrTPk5DyFNp3CXWHr2r00XUwKzNgM2WIoyKLFFWGkDp3DpSCSpUipLp1E8ZQbi4sW2Z7WM07PFdnHofMQsg55UeBb9c60KG38+vqKQsY1YroaFFxU5ZFeIQjqrAojZAC38L7du4UYZvx8XTvIopJ7CtYyY8/wvIkF3lpxVGuXyk5mbBTp5AsFoiKEue0tHgrOKEu/FRRoQmVzq0GsEH56h9K9mC0l1FI6aJEmNupk7s5dE54Ymzz1aZN9g0d5vx2se0IMgTZKvYBpbMlKoPi16e7QhMqDgUnnFA/Z8uWzqGwt90m7Jp9+0SqCIhrvJqEn1clmpC61JAkERLiojFg6zqtaRrsUJq1Z09QGsMClSakBjQeQHhAOCm5KbT9pC3vbHiHtDzhyTmReYI8Ux7+en+aRXlvZugzqkeqqAiys8smpMCexOoqvE+N0VZXIouhrmKpdIm/dIxm2wofMKzZsHLtSxVSXeK7uE469hXFcGix6TCSLIu8Jm+TjyscPUyqseGLRyo9XaxIJyeLH52u9CvJGr6j05EXIe5320SYUIFFD1TDpKKEVEEBPCKKINCmjfvE8fKg09m9Ug5N7fs36k+APoCU3BTMVjPtYtrRvHaxyq4+CqnYTs6hPzpPK+UXCp3OLiSKe2TKEtoHvgkph5DePg36YNAZOJ55nM1nNrPmhIh1d8pLK46DRypCHXs6dCibcVtNPVJ9GvThdyXCrM/Ws+43LKOQCowW84kx/SwyMnGhcWJekWVnIbVvn62Cn0FnoEtCF0JV52VQEC57uVQlxfMnfRVS6sK5ipuCWUREwK23ir9VL/klGNYHmpDS8MZ114mcqqFDRX5VJeCv9+edIe8QZAhif/p+HlvyGHXfqcs1v13DF/9+AYiqOeUJ/ypBcLA9hjk1texCylOelJv8KJXo4Gia1hLCNdAQSKs6rre7GGkf2557u97LtL7TylT23JEb2t3A8GbDeXXgq+U7KHXVdvt28bus3iDVsDh1ytZHzKNxGRkpMslBGCeqMdWy5YUPD7nIsdSxJ6Fb/P3EuagoKtIjdfSoSOL+7jth5D/9dPn36Y6rlLLaixYJ7wiiDHpfpQw6uPGK+LJoACUX8cqyWFEVqEKiuEemLKF9YDcyt24VotgVDp7oEP8QetQTeXwP//0wVtlK5/jONK7l4ftVPVLZ2dRSQgTp2LF0x6lSTYVU25i2hF0v+pwFr9/i+v5KT7dXFHSzkOmOgDrieowUka12b9SJEyKCxWAQwvT8eXvoPvB076cZFKNU6LzQ+VFQ8vr05m335pFyZceo4X0qmpDS0HDDpEnw998iRKCSuKPzHSQ/lsxnIz+jW0I3TFYTs/fN5tW1wjiu0LA+FceCExdASIE9vK9DbIeKFYrVHJ2k45ORnzC139Ry7ysuNI6FNy1kcNPB5dtR8RXmsuYnJSSIydZs9nmV3im8TwvrqzL8EuzGhjm2TsWGpVSUkFq4UFwL27eLfS5ZAjffXP7jc8fllwsBn5IiQs0UHENwXQopX6/1mBh7/6U6dapFbzuXuPJIZWWJymaOz/tKo0bCmDWZnIsWOFIsN3JAI1FQRM2jdVutTyUsTHhDgBg1BLSsXu1qGtonSRLTbvsGOnUSQv+PP0putFekA9C4calFjaQsptQqLqRUb1SHDvbP7BDeN/KykXwx4F3xT3UQUsWrKXpbsFDtu9IIKbUUOohrr10VF46pJmhCSqPaEBEYwd1d72bzpM3svGcnD3Z/kFqBIgFycJNyGsmucMyTukBCSm0kOLL5yNK9r0bFUzyPoKxCRq93Xg308/Oeo+BYoMIhvEejcgmpZzf6DfUquEeXKqSKGya+YrHACy+INhSZmZCYKET2wIEVdoguCQiwVwR0KIM+puUYgv2C6ZrQlXYxxQymjAx7s3dvhrUkiQaxULE5aRWNK4+U+netWqX3FkuS5/C+ggJ7GJVy7/dv7JwH5zE/Sn0PxWAOVr0lZRVSnjxSJpO9EuyFKi7gIgTVRhnD+gBb0QWbRyqumJBKTLR7uYrnRqv916pLJIFjeF9ZPFJWq634iVvP3pNPCi/5qFEXPpzxAqEJKY1qSfvY9rw//H2SHkti/+T9TOwwseLfpCI8UmqO1OnTtgZ9gBhQ1T4bHoTUtW2u5dADhyqmv4xG+agoIQXOxkWDBt4nGNX4PH5cq9hXhehi7au0HqtalQVVSBUUuO/F447z54WAUpO477sPVq8ufThZWRmueJ8chFSjyEYcvP8gy25ZVrKfmeqNio31bQxVw/sqMietonElJMpaaEJFFVIrV9rya2zs2iXEc2ys7XvpUa+HrQl9y+iWvoV/OxjMssHgcf7xiLscMRDzndUqCnJURq6eL6hCatkyu4hXUYWUj414nVA8UiVC+xyFlPqdFhdSak+l6uCRAmchVZZiE6dPi89kMIhy564YOFCEsH/xRfmOtQajCSmNak2gIZCW0S0rpxGpYwn0sgqpiAj7pKqGE4C97GydOl6bATaLanZJhfVVW2Ji7H3SYmPLt1ruuCrvQxNrm/DaskWs9EpS2XMbNHzH0QisaO9IaKgwNKF04X0FBTBiBCxeLMK0vvsOPv7YfTPNykAVUuvXC2+TQt3wukQERpTcXhVSvoZ5qYWLShseV5W4EhJlLTShogqpZcuEV+7VV+1VAR0XUJT5LtAQSJ8G4jVew/pUHA3mli3Lft2o81pmpt3TAkK0qEVPGjd22dS4SmjdWnw+o1GEvzpSAR6pWgUQZAiiRXQL4YFTQ667d3cvpKqzR6osQkr9fM2bew7Bbdz4wlcpvIBoQkrj0qUiQvvAdXifl4p9GtUQvd5uTHfpUr58GUePlC/Gpbq92mD8ssvseSQalYejkKpo74gklT5PymqFCRPE6netWrBhQ+XmQ7mjUSNhLFqtsHSp9+19zY9SuftuuPdeu0FeHfEU2ldWj1SnTvD44+LePnIEnn1WGLujRsEvv4htinnC3x7yNg8lPsQTvZ/w7T0cPVLt25ftOEEsEqpjkPq5d+8WJfL/+EMsErz8ctn3XxGoXqk5c+yPyXL5hJTikeoS3JRvxnwjFjl37RJ91SIixNhc0zxS/v72PlHucFW1z13FPg0nNCGlcelSEaF94FpI+ZAfpVENUVeay5ufVFqPlLq9xVIx76/hG6pXGionX6e0Quq552D2bLH6O3fuhS1/7yK8zy2lFVJ168Inn1TvhSbHprxFReLv8ob2SRK8+aZobzBrlvBQWSywYAGsEeXNi9/77WPb896w94gMjPTtPRw8D+USUuDslfvhB+jRAw4dEp9/7Vq4+ury7b+8qEJq0SL7HJ6UJLxoen3ZWrYogqO+JYRr2lwjHlMb8XbvLjxw6rx++rRon6JSXT1ScXHeFwZdFZvQ7Bif0ISUxqVLRQspx9A+bQCqmQwZIozYUaPKtx9Hj1RpQvtUNCFVNVRmaB+UTkh9+SW89pr4e+ZM6NvX8/aVjSqk/v7bVgbdLaUVUjWB2rUhUOQn2QorlDe0TyUkRPTg+ecfMVc88YQQ9fHxcMUV5dt3RQopVTA+8YTwjObnw+DBIsytW7fy7bsi6NxZjJ35+aKaJdgXNJs3t5+/0qC2QHDMeXbMj1K3Ub9ntRgDVD+PVM+ewqs4aJD3bT2F9ml2jEc0IaVx6VIROVLguimvNgDVTKZNEyuM5TUSSiukoqNtZYsBrdBEVVGZoX3gu5BatkyEuoGo1HfLLRV/LKXFTRl0l1yMQkqSSpYAL29onytatoQ33hBeqjNnyt9mpKJC+8D+OdXz//zzwvujXtcXGkkqWb1PXdAsS1gf2EPgHHIDSwgpcB3eV908UvXri15XX37pfVtHIaUWQtHsGJ/QhJTGpUtF5Ui1aiUG9LQ0sS+j0d6IVRuAah5lWcUsTv369u72xRuQukKSnMMBO3Uq/zFoeKdOHft5qoyKeL4IqX37RIiU2Qw33ijEfHXATRn0Epw4YR/vfLnWaxKOoW2yXP7QPk/odBXTx0wRs/nR0c6hq2VBPZ+RkfDnn6KKZHUrcT1+vPg9f76Ye8uTHwV2j1ROjrgnMzPtXidHIeWqBLoqpKqLRwrEfezLdaUKKZNJfI60NCGqJEmIfQ23aKXCNC5dVCGlNliEsgmp4GBRGvTwYTGIx8WJuPewMO/9gzQuTvz8YN48Eerh6+ptw4ZiUm7eXCQ1a1Q+6nnKz7cbUBWJNyGVmirKnGdliXyZr76q2KbA5WXECGFAL1woCiO4Yvp0YXAOGFDlzVkrHceCE1lZ9tCt6jyut26N+csv2XruHD3Lu6977hELDVddVX29jT17ijk3JUWUlS+vkHIce7Oy7NX6GjcWCy8qrjxS6vVRXTxSpSE4WIiuoiJRcOLECfF4w4aXdEU+X9A8UhqXLuHh9vLEKmUdMBwLTqiVblq2rF5GkUbVMmQIjB3r+/ZqOKAW1le1DBkCY8ZUzr69CalbbxW9w5o1E8UlqrLEuS+oeVIbNjiHOqkcPgxffy3+VnteXUw49pJSvVG1a1d7w1KeMIGMshRaKE5kJDz0UPUVUSA8eer9O3t2+UP7/PzsHqXMTNdhfeA5tK86eaR8RZKcC05oYX0+owkpjUsXSXLOkdDpSgorX3EUUuoAVJ0rUmlUP669Vng2b7vtQh+JRkXhSUgdOyYKOUiSCEuqLnknjjRs6LkM+osvCu/7iBHQq1fVH19l4xjaV1GFJjQqHjW87/vvhXc5IMB9A1lfUL3TGRl2IdW9u/M2qsA4ckSURofqV2yitDjmSWmlz31GE1IalzaOQio4uOweJFdCShuANErDgAFihX/o0At9JBoVhSch9f334vfAgdV7rHBXBn3fPlESG+Cll6r2mKoKx9C+yig0oVEx9O0rikSogqZVK3tz9bLgWHDCnUcqLk6EAVqtoiQ8VL9iE6XFUUhpdozPaEJK49KmuJAqK65C+7QBSEPj0sadkJJl+PZb8feECVV7TKXFXRn0adPE5xg79uIt1+8qtE8TUtUPPz+Rx6VS1rA+FdUjtWOHKLpgMJQsACRJJcP7LiaPlCakfEYTUhqXNo5VjcojpJo3F4N5To7ogg7aAKShcanjKKTUksIAGzcK72NISOny6C4EjmXQd+wQj+3YAb/9JozJF1+8kEdXuahhfGlpdq+DFtpXPVHD+8DekqSsqB6pxYvF7w4dnNtTqBQXUheLR+rECbsHVrNjvKIJKY1Lm4rySPn727uoy7L4vzon6GpoaFQ+jiWFHauDqt6o8eOr/+p1QIAIPwR7eN8LL4jf118P7dpdmOOqCqKi7Aa0GuKleaSqJ4MH2wVMRXmk/vlH/C4e1qdSvAR6TfdIqcUm1q8Xv2Njy9/X7BJAE1IalzYVJaTAefC+7LLyxWhraGjUfIKD7eOKGt5XVAQ//yz+ru5hfSqOeVKbN4uS6DodTJ16YY+rspEku3A6flz81oRU9SQwED78UFTCHDKkfPtSPVJGo/jtTkhdrB6pLVvEb80b5ROakNK4tKmo0D5wDifQKvZpaGhAyTypBQtEWeV69aBfvwt1VKXDsQz6I4+IvydMsHvhL2aKh/JpoX3Vl9tug1mzyl59V6V4TzlvQuq//0T1yppc/hzsQkoVkJqQ8glNSGlc2lSWR0obgDQ0NKCkkFLD+m6+GfT6C3NMpcWxDPr69cLbrob3XewU90BpQuriR/VIgRBVzZu73q5hQ+EJKyoSXim1GEtN90ipaHaMT2hCSuPSRhNSGhoalYmjkEpLg4ULxf+33HLhjqksqF4pgDvuuHRyQB2FU3S0MJw1Lm4cPVLduokwVlfo9Xav7Nat9sc1IXVJoQkpjUubihRSjRvbXfrlrRqkoaFxceAopH7+Gcxm6Nq15oX/jhghfgcEwPPPX9hjqUocPVJaftSlgaNHyl1Yn4oqNtS8osDAmpsfrQmpMlFDz7aGRgURFSVWm6zW8gspvR6++gqOHtWElIaGhsBRSC1dKv6uKUUmHBkwAF5/XazAX0rhbZqQuvRw9Eh5E1Lqgojqkaqp3ihwrtAXHg4JCRfuWGoQmpDSuLTR66FOHUhNLb+QArjmmvLvQ0ND4+JBFVJr1ghjy2AQZcNrGpIETz55oY+i6nEUjZeSgLyUKY2QUr02ao+1mlpoApyFVKtW4p7X8IoW2qehoYb3VYSQ0tDQ0HBEFVLr1onfI0aIxRuNmoHmkbr0aN5cFJIYOtT7vaoKKbXSXU32SPn5CU8UaGF9pUDzSGloqCXQNSGloaFR0RTPO6iJYX2XMpGRwjjOy9OE1KVCUBAcPuxbVc3mzcV2Fov4vyZ7pECMV9nZmpAqBZpHSkOjSRPx27GnlIaGhkZFoHqkQBjlV155wQ5FowxIkmiwDvbfGhc/BoNvoW3+/tC0qf3/muyRAmjQQPzu1OnCHkcNQvNIaWhMmyZKnN5444U+Eg0NjYsNRyF1/fWi6p1GzeLbb2HnTlFtUUOjOK1awcGD4u+a7pH6/HPReHvQoAt9JDUGTUhpaMTHw513Xuij0NDQuBhxFFJaWF/NpG1b5z6BGhqOtGoFf/wh/q7pHqkWLey9sTR8QhNSGhoaGhoalUV8vGi+q9dDjx4X+mg0NDQqGseecDXdI6VRajQhpaGhoaGhUVlIkggN09DQuDhxLMygCalLDq3YhIaGhoaGhoaGhkZZaNnS/ndND+3TKDWakNLQ0NDQ0NDQ0NAoC6Gh9tL4mkfqkkMTUhoaGhoaGhoaGhplRc2TCgu7sMehUeVoOVIaGhoaGhoaGhoaZeXppyEiAsaOvdBHolHFaEJKQ0NDQ0NDQ0NDo6z06yd+NC45tNA+DQ0NDQ0NDQ0NDQ2NUqIJKQ0NDQ0NDQ0NDQ0NjVKiCSkNDQ0NDQ0NDQ0NDY1SogkpDQ0NDQ0NDQ0NDQ2NUqIJKQ0NDQ0NDQ0NDQ0NjVKiCSkNDQ0NDQ0NDQ0NDY1SogkpDQ0NDQ0NDQ0NDQ2NUqIJKQ0NDQ0NDQ0NDQ0NjVKiCSkNDQ0NDQ0NDQ0NDY1SogkpDQ0NDQ0NDQ0NDQ2NUqIJKQ0NDQ0NDQ0NDQ0NjVJy0Qipjz/+mEaNGhEYGEhiYiKbN2++0IekoaGhoaGhoaGhoXGRclEIqV9++YVHH32UqVOn8u+//9KhQweGDh3K2bNnL/ShaWhoaGhoaGhoaGhchFwUQuqdd95h0qRJ3HbbbbRu3ZrPPvuM4OBgvvrqqwt9aBoaGhoaGhoaGhoaFyGGC30A5cVoNLJt2zaeeeYZ22M6nY5BgwaxYcMGl68pKiqiqKjI9n92djYAJpMJk8lUuQdcjVE/+6X8HVxqaOf80kI735ce2jm/9NDO+aWHds4rHl+/yxovpNLT07FYLMTGxjo9Hhsby4EDB1y+5tVXX+XFF18s8fiSJUsIDg6ulOOsSSxduvRCH4JGFaOd80sL7Xxfemjn/NJDO+eXHto5rzjy8/N92q7GC6my8Mwzz/Doo4/a/s/OzqZ+/foMGTKE8PDwC3hkFxaTycTSpUsZPHgwfn5+F/pwNKoA7ZxfWmjn+9JDO+eXHto5v/TQznnFo0areaPGC6no6Gj0ej2pqalOj6emphIXF+fyNQEBAQQEBJR43M/PT7sA0b6HSxHtnF9aaOf70kM755ce2jm/9NDOecXh6/dY44WUv78/Xbp0Yfny5YwZMwYAq9XK8uXLuf/++33ahyzLgO/q82LFZDKRn59Pdna2diNeImjn/NJCO9+XHto5v/TQzvmlh3bOKx5VE6gawR01XkgBPProo0ycOJGuXbvSvXt33nvvPfLy8rjtttt8en1OTg4A9evXr8zD1NDQ0NDQ0NDQ0NCoIeTk5BAREeH2+YtCSF133XWkpaXxwgsvkJKSQseOHfn7779LFKBwR0JCAqdOnSIsLAxJkir5aKsvaq7YqVOnLulcsUsJ7ZxfWmjn+9JDO+eXHto5v/TQznnFI8syOTk5JCQkeNxOkr35rDQuGbKzs4mIiCArK0u7ES8RtHN+aaGd70sP7Zxfemjn/NJDO+cXjouiIa+GhoaGhoaGhoaGhkZVogkpDQ0NDQ0NDQ0NDQ2NUqIJKQ0bAQEBTJ061WVpeI2LE+2cX1po5/vSQzvnlx7aOb/00M75hUPLkdLQ0NDQ0NDQ0NDQ0CglmkdKQ0NDQ0NDQ0NDQ0OjlGhCSkNDQ0NDQ0NDQ0NDo5RoQkpDQ0NDQ0NDQ0NDQ6OUaEJKQ0NDQ0NDQ0NDQ0OjlGhCqgaxZs0aRo0aRUJCApIkMW/ePKfnZVnmhRdeID4+nqCgIAYNGsShQ4dc7quoqIiOHTsiSRI7duxwuc3hw4cJCwsjMjKyxHPvvfceLVq0ICgoiPr16/PII49QWFjotM3HH39Mo0aNCAwMJDExkc2bN5flY1/SVMU5P378OJIklfjZuHGj0+t/++03WrZsSWBgIO3atWPhwoVlPhYN91SXc/7FF19w+eWXU6tWLWrVqsWgQYNK3MPaOS8/1eV8O/Lzzz8jSRJjxowp87FouKc6nfPMzEwmT55MfHw8AQEBXHbZZSXGdm0uLz/V6Zxr9lvFogmpGkReXh4dOnTg448/dvn8G2+8wQcffMBnn33Gpk2bCAkJYejQoSVuEIAnn3yShIQEt+9lMpm44YYbuPzyy0s89+OPP/L0008zdepU9u/fz8yZM/nll1949tlnbdv88ssvPProo0ydOpV///2XDh06MHToUM6ePVuGT37pUpXnfNmyZSQnJ9t+unTpYntu/fr13HDDDdxxxx1s376dMWPGMGbMGPbs2VOmY9FwT3U556tWreKGG25g5cqVbNiwgfr16zNkyBDOnDlTpmPRcE11Od8qx48f5/HHH3c59mvnu2KoLufcaDQyePBgjh8/zuzZs/nvv//44osvqFu3rm0bbS6vGKrLOdfst0pA1qiRAPLcuXNt/1utVjkuLk5+8803bY9lZmbKAQEB8k8//eT02oULF8otW7aU9+7dKwPy9u3bS+z/ySeflG+++WZ51qxZckREhNNzkydPlgcMGOD02KOPPir37t3b9n/37t3lyZMn2/63WCxyQkKC/Oqrr5bh02rIcuWd82PHjrm9DlSuvfZaeeTIkU6PJSYmynfffXepj0XDdy7kOS+O2WyWw8LC5G+++abUx6LhGxf6fJvNZrlXr17yl19+KU+cOFEePXp0mY5Fw3cu5Dn/9NNP5SZNmshGo9HtNtpcXvFcyHOu2W8Vj+aRukg4duwYKSkpDBo0yPZYREQEiYmJbNiwwfZYamoqkyZN4rvvviM4ONjlvlasWMFvv/3mduWkV69ebNu2zebqPXr0KAsXLmTEiBGAWOXatm2b07HodDoGDRrkdCwa5aMizznAVVddRUxMDH369GH+/PlOz23YsMHpfQCGDh1qex9fj0WjfFTlOS9Ofn4+JpOJqKioUh2LRtmp6vP90ksvERMTwx133FHmY9EoH1V5zufPn0/Pnj2ZPHkysbGxtG3blldeeQWLxQJoc3lVUZXnXLPfKh7DhT4AjYohJSUFgNjYWKfHY2Njbc/Jssytt97KPffcQ9euXTl+/HiJ/Zw7d45bb72V77//nvDwcJfvdeONN5Kenk6fPn2QZRmz2cw999xjcw2np6djsVhcHsuBAwfK+1E1FCrqnIeGhvL222/Tu3dvdDodc+bMYcyYMcybN4+rrrrK9l6e3seXY9EoP1V5zovz1FNPkZCQYJtgtXNe+VTl+V67di0zZ850mzOrne+qoSrP+dGjR1mxYgU33XQTCxcu5PDhw9x3332YTCamTp2qzeVVRFWec81+q3g0IXUJ8eGHH5KTk8MzzzzjdptJkyZx4403csUVV7jdZtWqVbzyyit88sknJCYmcvjwYR566CGmT5/OlClTKuPQNcqIL+c8OjqaRx991PZ/t27dSEpK4s0333RrVGtUXyrjnL/22mv8/PPPrFq1isDAwEo5bo2yURHnOycnh1tuuYUvvviC6OjoqjhsjXJQUfe41WolJiaGGTNmoNfr6dKlC2fOnOHNN99k6tSplf45NHynos65Zr9VPFpo30VCXFwcIFy/jqSmptqeW7FiBRs2bCAgIACDwUCzZs0A6Nq1KxMnTrRt89Zbb2EwGDAYDNxxxx1kZWVhMBj46quvAJgyZQq33HILd955J+3atWPs2LG88sorvPrqq1itVqKjo9Hr9R6PRaP8VNQ5d4U6wDq+l6f38eVYNMpPVZ5zlbfeeovXXnuNJUuW0L59+1Idi0b5qKrzfeTIEY4fP86oUaNsY/+3337L/PnzMRgMHDlyRDvfVURV3uPx8fFcdtll6PV622OtWrUiJSUFo9GozeVVRFWec81+q3g0IXWR0LhxY+Li4li+fLntsezsbDZt2kTPnj0B+OCDD9i58//t3EtIVP8bx/GPk463RgYpa4IxShA1aJExIUHYhS4UaS7cuFAoQiRcRS0sFHPAQty0CQKxMksLNOgiShSJoZsck5wsu3gBo0VFFormfP+L+A1M+YdOmlq+XzDgmfOcM8/x0Zn5MMy3Rz6fTz6fL7jEaUNDg7xer6Tv34X5b7/P51N5ebkcDod8Pp8OHTok6ft3JWy20D+d/56IjTGy2+1KT08P6SUQCOj+/fvBXjB7czXzmfh8PrlcruB2RkZGyONIUltbW/BxfqUXzN58zlz6vpLUmTNn1NLSos2bN1vuBbMzX/NOSUlRb29vyHP/wYMHtX37dvl8PrndbuY9T+bzf3zr1q0aGBhQIBAI3vfixQu5XC7Z7XZey+fJfM6c929/wEKtcgHrxsbGTHd3t+nu7jaSTHV1tenu7jaDg4PGGGMqKyuN0+k0t27dMk+fPjVZWVlm3bp1Znx8fMbz/coKLzOt2ldaWmocDoe5du2aef36tWltbTVJSUkmNzc3WHP9+nUTGRlpamtrTV9fnzl69KhxOp3m3bt3s/49LCXzMfPa2lpTX19v/H6/8fv9xuv1GpvNZmpqaoI1HR0dJjw83FRVVRm/329KS0tNRESE6e3tDdZY7QUzWywzr6ysNHa73dy8edOMjo4Gb2NjYyE1zHx2Fsu8f/Tjqn2/0wtmtlhmPjQ0ZBwOhzl27Jjp7+83t2/fNgkJCaaioiJYw2v53FgsM+f929wjSP1FHjx4YCT9dMvPzzfGfF9C8/Tp02bVqlUmMjLS7Ny50/T39//f8/1ukJqamjJlZWUmKSnJREVFGbfbbYqKiszHjx9D6s6fP28SExON3W43Ho/HdHZ2/uaVL13zMfPa2lqTmppqYmJiTFxcnPF4PObGjRs/HdvY2GiSk5ON3W43GzZsMHfu3AnZb7UXzGyxzHzt2rUz9lFaWhqsYeazt1jm/aOZghTznhuLaeaPHz82W7ZsMZGRkWb9+vXG6/Wab9++hdTwWj57i2XmvH+be2HGGDOnH3EBAAAAwD+O70gBAAAAgEUEKQAAAACwiCAFAAAAABYRpAAAAADAIoIUAAAAAFhEkAIAAAAAiwhSAAAAAGARQQoAAAAALCJIAQAgKSwsTM3NzQvdBgDgL0GQAgD89QoKCpSdnb3QbQAAlhCCFAAAAABYRJACAPxTMjMzVVxcrBMnTig+Pl6rV69WWVlZSM3Lly+1bds2RUVFKS0tTW1tbT+dZ3h4WLm5uXI6nYqPj1dWVpbevn0rSXr+/LliYmJUX18frG9sbFR0dLT6+vr+5OUBABYJghQA4J9z6dIlxcbGqqurS+fOnVN5eXkwLAUCAeXk5Mhut6urq0sXLlzQyZMnQ46fmprSnj175HA41N7ero6ODi1fvlx79+7V5OSkUlJSVFVVpaKiIg0NDWlkZESFhYU6e/as0tLSFuKSAQDzLMwYYxa6CQAAZqOgoECfPn1Sc3OzMjMzNT09rfb29uB+j8ejHTt2qLKyUq2trdq/f78GBwe1Zs0aSVJLS4v27dunpqYmZWdnq66uThUVFfL7/QoLC5MkTU5Oyul0qrm5Wbt375YkHThwQJ8/f5bdbteyZcvU0tISrAcA/NvCF7oBAADm2saNG0O2XS6X3r9/L0ny+/1yu93BECVJGRkZIfU9PT0aGBiQw+EIuX9iYkKvXr0KbtfU1Cg5OVk2m03Pnj0jRAHAEkKQAgD8cyIiIkK2w8LCFAgEfvn4L1++KD09XVevXv1p38qVK4M/9/T06OvXr7LZbBodHZXL5fr9pgEAfxWCFABgSUlNTdXw8HBI8Ons7Ayp2bRpkxoaGpSQkKC4uLgZz/PhwwcVFBSopKREo6OjysvL05MnTxQdHf3HrwEAsPBYbAIAsKTs2rVLycnJys/PV09Pj9rb21VSUhJSk5eXpxUrVigrK0vt7e168+aNHj58qOLiYo2MjEiSCgsL5Xa7derUKVVXV2t6elrHjx9fiEsCACwAghQAYEmx2WxqamrS+Pi4PB6Pjhw5Iq/XG1ITExOjR48eKTExUTk5OUpNTdXhw4c1MTGhuLg4Xb58WXfv3tWVK1cUHh6u2NhY1dXV6eLFi7p3794CXRkAYD6xah8AAAAAWMQnUgAAAABgEUEKAAAAACwiSAEAAACARQQpAAAAALCIIAUAAAAAFhGkAAAAAMAighQAAAAAWESQAgAAAACLCFIAAAAAYBFBCgAAAAAsIkgBAAAAgEX/AxREAcxqLZp/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Vẽ đường cho cột \"item_cnt_month_forecast\"\n",
    "plt.plot(df_campaign_1.index, df_campaign_1['item_cnt_month_forecast'], label='Item Forecast', linestyle='-', color='blue')\n",
    "\n",
    "# Vẽ đường cho cột \"item_cnt_month\"\n",
    "plt.plot(df_campaign_1.index, df_campaign_1['item_cnt_month'], label='Item Count Month', linestyle='-', color='green')\n",
    "\n",
    "# Vẽ đường cho cột \"target\"\n",
    "plt.plot(df_campaign_1.index, df_campaign_1['target'], label='Target', linestyle='-', color='red')\n",
    "\n",
    "# Đặt tên trục và tiêu đề\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Line Chart of Item Forecast, Item Count Month, and Target')\n",
    "\n",
    "# Thêm chú thích và lưới\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6d9fea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_ctn_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001819</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525605</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2598757</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2731793</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3714775</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274070205</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274567901</th>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274780231</th>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274917979</th>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274918975</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1393 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            order_ctn_month\n",
       "product_id                 \n",
       "2001819                  86\n",
       "2525605                  70\n",
       "2598757                  78\n",
       "2731793                  86\n",
       "3714775                  95\n",
       "...                     ...\n",
       "274070205                85\n",
       "274567901               173\n",
       "274780231               165\n",
       "274917979               177\n",
       "274918975               185\n",
       "\n",
       "[1393 rows x 1 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2=df.groupby(['product_id']).agg({'item_cnt_month': ['sum']})\n",
    "df2.columns = [ 'order_ctn_month']\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87b98816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tạo biểu đồ dạng đường\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df2.index, df2['order_ctn_month'], color='skyblue')\n",
    "\n",
    "# Đặt tiêu đề và nhãn cho biểu đồ\n",
    "plt.title('Monthly Orders by Product')\n",
    "plt.xlabel('Product ID')\n",
    "plt.ylabel('Order Count')\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum_order_ctn_month = df2['order_ctn_month'].sum()\n",
    "# print(\"Tổng số đơn hàng trong tháng 2:\", sum_order_ctn_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a23f0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# product_data_1 = df2.loc[df2['order_ctn_month'] < 100]\n",
    "# product_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac4377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_data_2 = df2.loc[(df2['order_ctn_month'] >= 100) & (df2['order_ctn_month'] < 500)]\n",
    "\n",
    "# product_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78999840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_data_2 = df2.loc[(df2['order_ctn_month'] >= 500)]\n",
    "\n",
    "# product_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d0c749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.groupby(['campaign_id']).agg({'profit_month': ['sum'], 'salary': ['sum']})\n",
    "\n",
    "# # Đặt lại tên cột kết quả\n",
    "# df1.columns = [ 'profit_month_sum','salary_month_sum']\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcab526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e56333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marketing  = pd.read_csv('E:/DOWLOAD/LightGBM_XGBoost/data/marketing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f81ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c786f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thực hiện phép nối giữa DataFrame 'matrix' và DataFrame 'items' dựa trên cột 'product_id' của 'matrix' và cột 'id' của 'items'\n",
    "# marketing = pd.merge(marketing, df1, left_on=['ID'], right_on=['campaign_id'], how='left')\n",
    "\n",
    "# # Xóa cột 'id' trong DataFrame 'matrix' nếu bạn không cần nó sau khi đã thực hiện phép nối\n",
    "# marketing.drop('ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dec9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4eea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marketing['profit_month']=marketing['profit_month_sum']-29*marketing['Marketing_Cost_Day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# marketing.drop(['profit_month_sum'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a241a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2965040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Lọc ra các dòng có giá trị âm trong cột profit_month\n",
    "# negative_profit_data = marketing.loc[marketing['profit_month'] < 0]\n",
    "\n",
    "# # Lưu DataFrame chứa dữ liệu có profit_month âm vào một tệp tin CSV mới\n",
    "# negative_profit_data.to_csv(\"marketing_fail.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0723143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative_profit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97e28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expensive  = pd.read_csv('E:/DOWLOAD/LightGBM_XGBoost/data/expensive.csv')\n",
    "# salary= pd.read_csv('E:/DOWLOAD/LightGBM_XGBoost/data/salary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tính tổng của cột profit_month\n",
    "# total_profit = marketing['profit_month'].sum() -expensive['Chi phí (VNĐ)'].sum() -sum_order_ctn_month*1000- marketing['salary_month_sum'].sum()\n",
    "\n",
    "# # In ra tổng lợi nhuận\n",
    "# print(\"Tổng lợi nhuận trước thuế: \", total_profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842d2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2809b0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
